---
title: "Interactive Linear Algebra"
subtitle: "Visualizing the Foundations of Statistical Modeling"
sidebar: weeks
filters:
  - marimo-team/marimo
toc: true
toc-depth: 3
---

::: {.callout-tip title="How to Use This Guide"}
This guide teaches linear algebra through **interactive visualizations**. Each concept includes:

- **Visual intuition** — see what the math means geometrically
- **Interactive widgets** — manipulate matrices and see results in real-time
- **Statistical connections** — understand why this matters for regression

**Core insight:** Matrices aren't just tables of numbers—they're *transformations* of space.
:::

---

## The Big Picture

In statistics, we constantly work with matrices:

- **Design matrices** ($\mathbf{X}$) encode our predictors
- **Coefficient vectors** ($\boldsymbol{\beta}$) are what we estimate
- **The OLS formula** $\hat{\boldsymbol{\beta}} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}$ involves transpose, multiplication, and inversion

But what do these operations *mean*? This guide builds your intuition from the ground up.

---

## 1. Vectors: Arrows in Space {#vectors}

A **vector** is an arrow with a direction and magnitude. In 2D, we describe it with two numbers: how far right ($x$) and how far up ($y$).

```{.marimo}
#| echo: false
import marimo as mo
import numpy as np
import altair as alt
```

```{.marimo}
#| echo: false
# Vector visualization controls
vec_x = mo.ui.slider(-3, 3, value=2, step=0.25, label="x component")
vec_y = mo.ui.slider(-3, 3, value=1, step=0.25, label="y component")
mo.hstack([vec_x, vec_y])
```

```{.marimo}
#| echo: false
import polars as pl

# Create vector visualization
def draw_vector(x, y, color="steelblue", label="v"):
    # Grid for background
    grid_x = np.linspace(-4, 4, 9)
    grid_y = np.linspace(-4, 4, 9)

    # Create grid lines data
    h_lines = pl.DataFrame({
        "x": [-4] * 9,
        "x2": [4] * 9,
        "y": grid_y.tolist(),
        "y2": grid_y.tolist()
    })
    v_lines = pl.DataFrame({
        "x": grid_x.tolist(),
        "x2": grid_x.tolist(),
        "y": [-4] * 9,
        "y2": [4] * 9
    })

    # Grid chart
    h_grid = alt.Chart(h_lines).mark_rule(color="lightgray", strokeWidth=0.5).encode(
        x=alt.X("x:Q", scale=alt.Scale(domain=[-4, 4])),
        x2="x2:Q",
        y=alt.Y("y:Q", scale=alt.Scale(domain=[-4, 4])),
        y2="y2:Q"
    )
    v_grid = alt.Chart(v_lines).mark_rule(color="lightgray", strokeWidth=0.5).encode(
        x="x:Q", x2="x2:Q", y="y:Q", y2="y2:Q"
    )

    # Axes
    axes_data = pl.DataFrame({
        "x": [-4, 0],
        "x2": [4, 0],
        "y": [0, -4],
        "y2": [0, 4]
    })
    axes = alt.Chart(axes_data).mark_rule(color="black", strokeWidth=1).encode(
        x="x:Q", x2="x2:Q", y="y:Q", y2="y2:Q"
    )

    # Vector arrow
    arrow_data = pl.DataFrame({"x": [0], "y": [0], "x2": [x], "y2": [y]})
    arrow = alt.Chart(arrow_data).mark_rule(color=color, strokeWidth=3).encode(
        x="x:Q", x2="x2:Q", y="y:Q", y2="y2:Q"
    )

    # Arrowhead (triangle at tip)
    angle_rad = np.arctan2(y, x)
    angle_deg = (np.degrees(angle_rad) - 90) % 360  # Normalize to 0-360
    head_data = pl.DataFrame({
        "x": [x],
        "y": [y]
    })
    arrowhead = alt.Chart(head_data).mark_point(
        shape="triangle",
        size=150,
        color=color,
        angle=angle_deg
    ).encode(x="x:Q", y="y:Q")

    # Label
    label_data = pl.DataFrame({"x": [x + 0.2], "y": [y + 0.2], "text": [label]})
    text = alt.Chart(label_data).mark_text(fontSize=14, fontWeight="bold", color=color).encode(
        x="x:Q", y="y:Q", text="text:N"
    )

    return (h_grid + v_grid + axes + arrow + arrowhead + text).properties(
        width=350, height=350
    ).configure_axis(grid=False)

chart = draw_vector(vec_x.value, vec_y.value)

mo.md(f"""
### Your Vector

$$\\mathbf{{v}} = \\begin{{bmatrix}} {vec_x.value} \\\\ {vec_y.value} \\end{{bmatrix}}$$

**Magnitude:** $|\\mathbf{{v}}| = \\sqrt{{{vec_x.value}^2 + {vec_y.value}^2}} = {np.sqrt(vec_x.value**2 + vec_y.value**2):.2f}$

{mo.as_html(chart)}

*Drag the sliders to move the vector!*
""")
```

::: {.callout-note title="Key Insight"}
A vector is just an arrow from the origin. The numbers tell you where the tip lands.

- First number = horizontal position
- Second number = vertical position
:::

### Vector Operations

**Scalar multiplication** stretches or shrinks a vector:

```{.marimo}
#| echo: false
scalar = mo.ui.slider(-2, 2, value=1.5, step=0.25, label="Scalar k")
scalar
```

```{.marimo}
#| echo: false
k = scalar.value
orig_x, orig_y = 2, 1

def draw_two_vectors(x1, y1, x2, y2, label1="v", label2="kv", color1="steelblue", color2="coral"):
    grid_x = np.linspace(-4, 4, 9)
    grid_y = np.linspace(-4, 4, 9)

    h_lines = pl.DataFrame({"x": [-4]*9, "x2": [4]*9, "y": grid_y.tolist(), "y2": grid_y.tolist()})
    v_lines = pl.DataFrame({"x": grid_x.tolist(), "x2": grid_x.tolist(), "y": [-4]*9, "y2": [4]*9})

    h_grid = alt.Chart(h_lines).mark_rule(color="lightgray", strokeWidth=0.5).encode(
        x=alt.X("x:Q", scale=alt.Scale(domain=[-4, 4])), x2="x2:Q",
        y=alt.Y("y:Q", scale=alt.Scale(domain=[-4, 4])), y2="y2:Q"
    )
    v_grid = alt.Chart(v_lines).mark_rule(color="lightgray", strokeWidth=0.5).encode(
        x="x:Q", x2="x2:Q", y="y:Q", y2="y2:Q"
    )

    axes_data = pl.DataFrame({"x": [-4, 0], "x2": [4, 0], "y": [0, -4], "y2": [0, 4]})
    axes = alt.Chart(axes_data).mark_rule(color="black", strokeWidth=1).encode(
        x="x:Q", x2="x2:Q", y="y:Q", y2="y2:Q"
    )

    # First vector
    arrow1_data = pl.DataFrame({"x": [0], "y": [0], "x2": [x1], "y2": [y1]})
    arrow1 = alt.Chart(arrow1_data).mark_rule(color=color1, strokeWidth=2.5).encode(
        x="x:Q", x2="x2:Q", y="y:Q", y2="y2:Q"
    )
    label1_data = pl.DataFrame({"x": [x1 + 0.2], "y": [y1 + 0.2], "text": [label1]})
    text1 = alt.Chart(label1_data).mark_text(fontSize=14, fontWeight="bold", color=color1).encode(
        x="x:Q", y="y:Q", text="text:N"
    )

    # Second vector
    arrow2_data = pl.DataFrame({"x": [0], "y": [0], "x2": [x2], "y2": [y2]})
    arrow2 = alt.Chart(arrow2_data).mark_rule(color=color2, strokeWidth=2.5).encode(
        x="x:Q", x2="x2:Q", y="y:Q", y2="y2:Q"
    )
    label2_data = pl.DataFrame({"x": [x2 + 0.2], "y": [y2 + 0.2], "text": [label2]})
    text2 = alt.Chart(label2_data).mark_text(fontSize=14, fontWeight="bold", color=color2).encode(
        x="x:Q", y="y:Q", text="text:N"
    )

    return (h_grid + v_grid + axes + arrow1 + text1 + arrow2 + text2).properties(
        width=350, height=350
    ).configure_axis(grid=False)

scaled_x = k * orig_x
scaled_y = k * orig_y
chart2 = draw_two_vectors(orig_x, orig_y, scaled_x, scaled_y)

mo.md(f"""
$$k \\cdot \\mathbf{{v}} = {k} \\cdot \\begin{{bmatrix}} 2 \\\\ 1 \\end{{bmatrix}} = \\begin{{bmatrix}} {scaled_x} \\\\ {scaled_y} \\end{{bmatrix}}$$

{mo.as_html(chart2)}

- $k > 1$: vector stretches
- $0 < k < 1$: vector shrinks
- $k < 0$: vector flips direction
""")
```

---

## 2. Linear Transformations: Matrices as Functions {#transformations}

Here's the key insight: **a matrix is a transformation of space**.

When you multiply a vector by a matrix, you're transforming that vector—stretching, rotating, shearing, or reflecting it.

```{.marimo}
#| echo: false
from wigglystuff import Matrix

# Create an editable 2x2 transformation matrix
tfm_matrix = mo.ui.anywidget(
    Matrix(np.array([[1.0, 0.0], [0.0, 1.0]]), step=0.1)
)
```

```{.marimo}
#| echo: false
def draw_transformation(mat):
    """Draw a grid before and after transformation"""
    # Create grid points
    t = np.linspace(-2, 2, 5)
    grid_pts = np.array([[x, y] for x in t for y in t])

    # Transform the grid
    transformed = (mat @ grid_pts.T).T

    # Original grid data
    orig_data = pl.DataFrame({
        "x": grid_pts[:, 0].tolist(),
        "y": grid_pts[:, 1].tolist(),
        "type": ["original"] * len(grid_pts)
    })

    # Transformed grid data
    trans_data = pl.DataFrame({
        "x": transformed[:, 0].tolist(),
        "y": transformed[:, 1].tolist(),
        "type": ["transformed"] * len(transformed)
    })

    all_data = pl.concat([orig_data, trans_data])

    # Basis vectors - original
    basis_orig = pl.DataFrame({
        "x": [0, 0],
        "y": [0, 0],
        "x2": [1, 0],
        "y2": [0, 1],
        "label": ["i", "j"],
        "color": ["red", "green"]
    })

    # Basis vectors - transformed
    i_hat = mat @ np.array([1, 0])
    j_hat = mat @ np.array([0, 1])
    basis_trans = pl.DataFrame({
        "x": [0, 0],
        "y": [0, 0],
        "x2": [i_hat[0], j_hat[0]],
        "y2": [i_hat[1], j_hat[1]],
        "label": ["i'", "j'"],
        "color": ["darkred", "darkgreen"]
    })

    # Grid points
    points = alt.Chart(all_data).mark_circle(size=30).encode(
        x=alt.X("x:Q", scale=alt.Scale(domain=[-4, 4])),
        y=alt.Y("y:Q", scale=alt.Scale(domain=[-4, 4])),
        color=alt.Color("type:N", scale=alt.Scale(
            domain=["original", "transformed"],
            range=["lightblue", "coral"]
        )),
        opacity=alt.value(0.7)
    )

    # Original basis vectors
    basis1 = alt.Chart(basis_orig).mark_rule(strokeWidth=3).encode(
        x="x:Q", y="y:Q", x2="x2:Q", y2="y2:Q",
        color=alt.Color("color:N", scale=None)
    )

    # Transformed basis vectors
    basis2 = alt.Chart(basis_trans).mark_rule(strokeWidth=3, strokeDash=[5,3]).encode(
        x="x:Q", y="y:Q", x2="x2:Q", y2="y2:Q",
        color=alt.Color("color:N", scale=None)
    )

    # Axes
    axes_data = pl.DataFrame({"x": [-4, 0], "x2": [4, 0], "y": [0, -4], "y2": [0, 4]})
    axes = alt.Chart(axes_data).mark_rule(color="gray", strokeWidth=0.5).encode(
        x="x:Q", x2="x2:Q", y="y:Q", y2="y2:Q"
    )

    return (axes + points + basis1 + basis2).properties(
        width=400, height=400
    ).configure_axis(grid=False)

mat = np.array(tfm_matrix.matrix)
i_new = mat @ np.array([1, 0])
j_new = mat @ np.array([0, 1])
det = np.linalg.det(mat)

chart3 = draw_transformation(mat)

mo.md(f"""
### Edit the Transformation Matrix

{mo.as_html(tfm_matrix)}

**What the columns mean:**
- Column 1: where $\\hat{{i}} = (1, 0)$ lands → $({i_new[0]:.1f}, {i_new[1]:.1f})$
- Column 2: where $\\hat{{j}} = (0, 1)$ lands → $({j_new[0]:.1f}, {j_new[1]:.1f})$

**Determinant:** ${det:.2f}$ (how much area scales)

{mo.as_html(chart3)}

**Legend:** Light blue = original grid, Coral = transformed grid
Solid arrows = original basis, Dashed = transformed basis
""")
```

::: {.callout-important title="The Fundamental Insight"}
**The columns of a matrix tell you where the basis vectors land.**

$$\mathbf{A} = \begin{bmatrix} | & | \\ \mathbf{a}_1 & \mathbf{a}_2 \\ | & | \end{bmatrix}$$

- First column = where $\hat{i} = (1, 0)$ goes
- Second column = where $\hat{j} = (0, 1)$ goes

Every other vector is a combination of basis vectors, so once you know where the basis lands, you know the whole transformation!
:::

### Try These Transformations

Edit the matrix above to create:

| Transformation | Matrix |
|----------------|--------|
| Horizontal stretch by 2 | $\begin{bmatrix} 2 & 0 \\ 0 & 1 \end{bmatrix}$ |
| 90° rotation | $\begin{bmatrix} 0 & -1 \\ 1 & 0 \end{bmatrix}$ |
| Horizontal shear | $\begin{bmatrix} 1 & 1 \\ 0 & 1 \end{bmatrix}$ |
| Reflection over x-axis | $\begin{bmatrix} 1 & 0 \\ 0 & -1 \end{bmatrix}$ |

---

## 3. Matrix Multiplication: Composing Transformations {#multiplication}

When you multiply two matrices $\mathbf{BA}$, you're applying transformation $\mathbf{A}$ first, then transformation $\mathbf{B}$.

```{.marimo}
#| echo: false
# Two matrices to compose
mat_A = mo.ui.anywidget(Matrix(np.array([[1.0, 0.5], [0.0, 1.0]]), step=0.1))
mat_B = mo.ui.anywidget(Matrix(np.array([[0.0, -1.0], [1.0, 0.0]]), step=0.1))
```

```{.marimo}
#| echo: false
A = np.array(mat_A.matrix)
B = np.array(mat_B.matrix)
BA = B @ A

mo.md(f"""
### Matrix A (applied first)

{mo.as_html(mat_A)}

### Matrix B (applied second)

{mo.as_html(mat_B)}

### Composition: BA = B @ A

$$\\mathbf{{BA}} = \\begin{{bmatrix}} {BA[0,0]:.2f} & {BA[0,1]:.2f} \\\\ {BA[1,0]:.2f} & {BA[1,1]:.2f} \\end{{bmatrix}}$$

**Why BA and not AB?** Because we read right-to-left: $\\mathbf{{BA}}\\mathbf{{v}}$ means "apply $\\mathbf{{A}}$ to $\\mathbf{{v}}$, then apply $\\mathbf{{B}}$ to the result."
""")
```

```{.marimo}
#| echo: false
# Show the composed transformation
chart_composed = draw_transformation(BA)

mo.md(f"""
### The Composed Transformation

{mo.as_html(chart_composed)}

This single matrix does both transformations at once!
""")
```

::: {.callout-note title="Matrix Multiplication Rule"}
To multiply matrices, each entry in the result is a **dot product**:

$$(\\mathbf{BA})_{ij} = \\text{row } i \\text{ of } \\mathbf{B} \\cdot \\text{column } j \\text{ of } \\mathbf{A}$$

But geometrically, you're just composing transformations!
:::

---

## 4. The Determinant: How Area Scales {#determinant}

The **determinant** measures how much a transformation scales area.

```{.marimo}
#| echo: false
det_matrix = mo.ui.anywidget(Matrix(np.array([[2.0, 0.0], [0.0, 1.0]]), step=0.1))
```

```{.marimo}
#| echo: false
def draw_area_scaling(mat):
    """Show unit square transforming to parallelogram"""
    # Unit square corners
    square = np.array([[0, 0], [1, 0], [1, 1], [0, 1], [0, 0]])

    # Transform the square
    parallelogram = (mat @ square.T).T

    square_data = pl.DataFrame({
        "x": square[:, 0].tolist(),
        "y": square[:, 1].tolist(),
        "order": list(range(5))
    })

    para_data = pl.DataFrame({
        "x": parallelogram[:, 0].tolist(),
        "y": parallelogram[:, 1].tolist(),
        "order": list(range(5))
    })

    # Background axes
    axes_data = pl.DataFrame({"x": [-3, 0], "x2": [3, 0], "y": [0, -3], "y2": [0, 3]})
    axes = alt.Chart(axes_data).mark_rule(color="lightgray", strokeWidth=0.5).encode(
        x=alt.X("x:Q", scale=alt.Scale(domain=[-3, 3])),
        x2="x2:Q",
        y=alt.Y("y:Q", scale=alt.Scale(domain=[-3, 3])),
        y2="y2:Q"
    )

    # Original square (filled)
    orig_square = alt.Chart(square_data).mark_area(
        color="steelblue", opacity=0.3, line={"color": "steelblue", "strokeWidth": 2}
    ).encode(x="x:Q", y="y:Q", order="order:O")

    # Transformed parallelogram (filled)
    trans_para = alt.Chart(para_data).mark_area(
        color="coral", opacity=0.4, line={"color": "coral", "strokeWidth": 2}
    ).encode(x="x:Q", y="y:Q", order="order:O")

    return (axes + orig_square + trans_para).properties(width=350, height=350)

M = np.array(det_matrix.matrix)
det_val = np.linalg.det(M)
area_chart = draw_area_scaling(M)

sign_text = "positive (preserves orientation)" if det_val > 0 else "negative (flips orientation)" if det_val < 0 else "zero (collapses to line)"

mo.md(f"""
### Edit the Matrix

{mo.as_html(det_matrix)}

### Area Scaling

**Determinant:** $\\det(\\mathbf{{A}}) = {det_val:.2f}$

{mo.as_html(area_chart)}

**Blue square** (area = 1) → **Coral parallelogram** (area = $|{det_val:.2f}|$ = {abs(det_val):.2f})

The determinant is **{sign_text}**.
""")
```

::: {.callout-important title="What Determinant Tells You"}
| Determinant | Meaning |
|-------------|---------|
| $\det > 1$ | Transformation expands area |
| $0 < \det < 1$ | Transformation shrinks area |
| $\det < 0$ | Area scaled AND orientation flipped |
| $\det = 0$ | Transformation collapses to lower dimension (no inverse!) |
:::

---

## 5. The Matrix Inverse: Undoing Transformations {#inverse}

If $\mathbf{A}$ transforms space, then $\mathbf{A}^{-1}$ **undoes** that transformation.

$$\mathbf{A}^{-1}\mathbf{A} = \mathbf{I}$$

```{.marimo}
#| echo: false
inv_matrix = mo.ui.anywidget(Matrix(np.array([[2.0, 1.0], [0.0, 1.0]]), step=0.1))
```

```{.marimo}
#| echo: false
A_inv = np.array(inv_matrix.matrix)
det_A = np.linalg.det(A_inv)

if abs(det_A) > 1e-10:
    A_inverse = np.linalg.inv(A_inv)
    product = A_inverse @ A_inv

    mo.md(f"""
### Matrix A

{mo.as_html(inv_matrix)}

**Determinant:** {det_A:.2f}

### Inverse A⁻¹

$$\\mathbf{{A}}^{{-1}} = \\begin{{bmatrix}} {A_inverse[0,0]:.3f} & {A_inverse[0,1]:.3f} \\\\ {A_inverse[1,0]:.3f} & {A_inverse[1,1]:.3f} \\end{{bmatrix}}$$

### Verification: A⁻¹A = I

$$\\mathbf{{A}}^{{-1}}\\mathbf{{A}} = \\begin{{bmatrix}} {product[0,0]:.1f} & {product[0,1]:.1f} \\\\ {product[1,0]:.1f} & {product[1,1]:.1f} \\end{{bmatrix}} = \\mathbf{{I}}$$

*The inverse "undoes" the transformation, returning to the identity.*
""")
else:
    mo.md(f"""
### Matrix A

{mo.as_html(inv_matrix)}

**Determinant:** {det_A:.2f}

### No Inverse Exists!

When $\\det(\\mathbf{{A}}) = 0$, the matrix squishes space onto a lower dimension.

You can't "undo" squishing—information is lost!

*Try adjusting the matrix so the determinant is non-zero.*
""")
```

::: {.callout-caution title="When Does an Inverse Exist?"}
A matrix has an inverse **only if** its determinant is non-zero.

If $\det(\mathbf{A}) = 0$:
- The transformation collapses space
- Multiple inputs map to the same output
- You can't uniquely reverse the transformation
:::

---

## 6. Design Matrices: Connecting to Statistics {#design-matrices}

In regression, the **design matrix** $\mathbf{X}$ encodes your predictors.

Each **row** is an observation. Each **column** is a predictor.

```{.marimo}
#| echo: false
# Interactive design matrix
n_obs = mo.ui.slider(3, 8, value=5, step=1, label="Number of observations")
n_obs
```

```{.marimo}
#| echo: false
n = n_obs.value
np.random.seed(42)

# Generate example data
x1_vals = np.round(np.random.uniform(1, 5, n), 1)
x2_vals = np.round(np.random.uniform(0, 10, n), 1)

# Design matrix with intercept
X = np.column_stack([np.ones(n), x1_vals, x2_vals])

mo.md(f"""
### Design Matrix Structure

For {n} observations with 2 predictors ($x_1$ and $x_2$):

$$\\mathbf{{X}} = \\begin{{bmatrix}}
1 & x_{{11}} & x_{{21}} \\\\
1 & x_{{12}} & x_{{22}} \\\\
\\vdots & \\vdots & \\vdots \\\\
1 & x_{{1n}} & x_{{2n}}
\\end{{bmatrix}}$$

**Your data:**

| Obs | Intercept | $x_1$ | $x_2$ |
|-----|-----------|-------|-------|
""" + "\n".join([f"| {i+1} | 1 | {x1_vals[i]} | {x2_vals[i]} |" for i in range(n)]) + f"""

**Matrix dimensions:** {X.shape[0]} rows × {X.shape[1]} columns = $({n} \\times 3)$

The column of 1s lets us estimate an **intercept** ($\\beta_0$).
""")
```

---

## 7. X'X: The Gram Matrix {#gram-matrix}

The matrix $\mathbf{X}^T\mathbf{X}$ is called the **Gram matrix**. It captures:

- **Diagonal:** How much each predictor varies (related to variance)
- **Off-diagonal:** How predictors relate to each other (related to covariance)

```{.marimo}
#| echo: false
# Use the X from previous cell
XtX = X.T @ X

mo.md(f"""
### Computing X'X

**Transpose:** $\\mathbf{{X}}^T$ is $({X.shape[1]} \\times {X.shape[0]})$

**Product:** $\\mathbf{{X}}^T\\mathbf{{X}}$ is $({X.shape[1]} \\times {X.shape[1]})$

$$\\mathbf{{X}}^T\\mathbf{{X}} = \\begin{{bmatrix}}
{XtX[0,0]:.1f} & {XtX[0,1]:.1f} & {XtX[0,2]:.1f} \\\\
{XtX[1,0]:.1f} & {XtX[1,1]:.1f} & {XtX[1,2]:.1f} \\\\
{XtX[2,0]:.1f} & {XtX[2,1]:.1f} & {XtX[2,2]:.1f}
\\end{{bmatrix}}$$

**Interpretation:**
- $({XtX[0,0]:.1f})$ = sum of squared intercepts = $n$ = {n}
- Diagonal = sum of squares for each predictor
- Off-diagonal = sum of cross-products (related to covariance)
""")
```

::: {.callout-note title="Why X'X Matters"}
The Gram matrix tells you about **multicollinearity**:

- If predictors are uncorrelated, off-diagonals are small
- If predictors are highly correlated, off-diagonals are large
- When $\mathbf{X}^T\mathbf{X}$ is nearly singular (det ≈ 0), coefficients become unstable
:::

---

## 8. The OLS Formula: Putting It Together {#ols}

Finally, the formula for estimating regression coefficients:

$$\hat{\boldsymbol{\beta}} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}$$

```{.marimo}
#| echo: false
# Generate y values with known coefficients
true_beta = np.array([2.0, 1.5, 0.3])  # intercept, b1, b2
noise = np.random.normal(0, 0.5, n)
y = X @ true_beta + noise
```

```{.marimo}
#| echo: false
# Compute OLS step by step (XtX already defined above)
XtX_inv = np.linalg.inv(XtX)
Xty = X.T @ y
beta_hat = XtX_inv @ Xty

mo.md(f"""
### Step-by-Step OLS

**Given:** Design matrix $\\mathbf{{X}}$ ({X.shape[0]}×{X.shape[1]}) and outcome $\\mathbf{{y}}$ ({len(y)}×1)

**Step 1:** Compute $\\mathbf{{X}}^T\\mathbf{{X}}$
$$\\mathbf{{X}}^T\\mathbf{{X}} = \\begin{{bmatrix}}
{XtX[0,0]:.2f} & {XtX[0,1]:.2f} & {XtX[0,2]:.2f} \\\\
{XtX[1,0]:.2f} & {XtX[1,1]:.2f} & {XtX[1,2]:.2f} \\\\
{XtX[2,0]:.2f} & {XtX[2,1]:.2f} & {XtX[2,2]:.2f}
\\end{{bmatrix}}$$

**Step 2:** Compute $(\\mathbf{{X}}^T\\mathbf{{X}})^{{-1}}$
$$({XtX_inv[0,0]:.4f}, {XtX_inv[0,1]:.4f}, {XtX_inv[0,2]:.4f}, ...)$$

**Step 3:** Compute $\\mathbf{{X}}^T\\mathbf{{y}}$
$$\\mathbf{{X}}^T\\mathbf{{y}} = \\begin{{bmatrix}} {Xty[0]:.2f} \\\\ {Xty[1]:.2f} \\\\ {Xty[2]:.2f} \\end{{bmatrix}}$$

**Step 4:** Compute $\\hat{{\\boldsymbol{{\\beta}}}} = (\\mathbf{{X}}^T\\mathbf{{X}})^{{-1}}\\mathbf{{X}}^T\\mathbf{{y}}$

$$\\hat{{\\boldsymbol{{\\beta}}}} = \\begin{{bmatrix}} {beta_hat[0]:.3f} \\\\ {beta_hat[1]:.3f} \\\\ {beta_hat[2]:.3f} \\end{{bmatrix}}$$

**Estimated model:** $\\hat{{y}} = {beta_hat[0]:.2f} + {beta_hat[1]:.2f}x_1 + {beta_hat[2]:.2f}x_2$

*(True values were: $\\beta_0 = 2.0$, $\\beta_1 = 1.5$, $\\beta_2 = 0.3$)*
""")
```

::: {.callout-important title="What Each Piece Does"}
| Term | Meaning |
|------|---------|
| $\mathbf{X}^T\mathbf{X}$ | How predictors relate to each other |
| $(\mathbf{X}^T\mathbf{X})^{-1}$ | Removes redundancy between predictors |
| $\mathbf{X}^T\mathbf{y}$ | How each predictor relates to the outcome |
| $(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}$ | Unique contribution of each predictor |
:::

---

## Summary: The Linear Algebra of Regression

You now understand the **geometric meaning** behind every piece of OLS:

1. **Vectors** are arrows in space
2. **Matrices** are transformations of space
3. **Multiplication** composes transformations
4. **Determinant** measures area scaling
5. **Inverse** undoes a transformation
6. **Design matrix** encodes predictors
7. **Gram matrix** captures predictor relationships
8. **OLS formula** finds the best-fitting coefficients

::: {.callout-tip title="Going Further"}
- See [Formula Reference](/guides/formulas.qmd) for all the equations
- Week 8-9 lectures cover these topics in more depth
- [3blue1brown's Essence of Linear Algebra](https://www.3blue1brown.com/topics/linear-algebra) is an excellent visual resource
:::
