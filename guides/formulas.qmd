---
sidebar: weeks
title: "Statistical Formula Reference"
subtitle: "Formulas, Python Code, and Intuitions for PSYC 201B"
toc: true
toc-depth: 3
toc-expand: 2
code-fold: show
code-overflow: wrap
execute:
  eval: false
  warning: false
---

::: {.callout-tip title="How to Use This Reference"}
This guide serves as both a **quick lookup** and a **learning resource**. Each formula includes:

- **The formula itself** — always visible
- **Intuition** — what it means conceptually (click to expand)
- **Python code** — how to compute it (click to expand)
- **Common pitfalls** — mistakes to avoid

**Quick Jump:** [Notation](#notation-key) | [GLM](#the-general-linear-model) | [Model Fit](#model-fit-and-comparison) | [Inference](#inference-and-uncertainty) | [Logistic](#different-outcome-types) | [Resampling](#resampling-methods) | [Mixed Models](#mixed-models) | [Linear Algebra](#linear-algebra-essentials) | [PCA](#pca-and-dimensionality-reduction) | [Index](#alphabetical-index)
:::

---

## Quick Reference

### Notation Key {#notation-key}

Throughout this guide, we use consistent notation:

| Symbol | Meaning |
|--------|---------|
| $y$ | Outcome/dependent variable (single observation) |
| $\mathbf{y}$ | Vector of all outcomes ($n \times 1$) |
| $x$ | Predictor/independent variable (single value) |
| $\mathbf{X}$ | Design matrix ($n \times p$) |
| $\beta$ | Population parameter (true coefficient) |
| $\hat{\beta}$ | Estimated coefficient (from data) |
| $\boldsymbol{\beta}$ | Vector of all coefficients |
| $\epsilon$ | Error term (population) |
| $e$ or $\hat{\epsilon}$ | Residual (sample estimate of error) |
| $n$ | Sample size (number of observations) |
| $p$ | Number of predictors (not counting intercept) |
| $\sigma^2$ | Population variance |
| $\hat{\sigma}^2$ | Estimated variance |
| $\mathbf{I}$ | Identity matrix |
| $\mathbf{X}^T$ | Transpose of matrix $\mathbf{X}$ |
| $\mathbf{X}^{-1}$ | Inverse of matrix $\mathbf{X}$ |

### Greek Letters Quick Reference

| Letter | Name | Common Use |
|--------|------|------------|
| $\alpha$ | alpha | Significance level (e.g., 0.05) |
| $\beta$ | beta | Regression coefficients |
| $\gamma$ | gamma | Group-level coefficients (mixed models) |
| $\epsilon$ | epsilon | Error/residual term |
| $\lambda$ | lambda | Regularization parameter |
| $\mu$ | mu | Population mean |
| $\sigma$ | sigma | Standard deviation |
| $\sigma^2$ | sigma-squared | Variance |
| $\tau$ | tau | Random effect variance |
| $\chi^2$ | chi-squared | Chi-squared distribution/statistic |

---

## Model-Based Thinking Foundations

Before diving into specific formulas, let's establish the core idea that underlies everything in this course.

### The Statistical Model {#statistical-model}

::: {.callout-note icon=false title="Formula"}
$$y = f(x) + \epsilon$$
:::

**Where:**

- $y$ = observed outcome (what we measure)
- $f(x)$ = systematic component (what we're trying to model)
- $\epsilon$ = random error (what we can't explain)

::: {.callout-tip title="Intuition" collapse="true"}
Every statistical model has the same basic structure: **Data = Model + Error**

The model ($f(x)$) captures the *predictable* part — the pattern or relationship we believe exists. The error ($\epsilon$) captures everything else — measurement noise, individual differences, factors we didn't measure.

**The goal of modeling** is to find an $f(x)$ that explains as much of $y$ as possible, leaving only random noise in $\epsilon$.

**Where it comes from:** This isn't derived — it's a *framework* for thinking about data. We *choose* to decompose observations this way because it's useful.
:::

::: {.callout-note title="Python Conceptual Example" collapse="true"}
```{python}
import numpy as np

# Simulating this idea:
# True relationship (unknown in practice)
def true_f(x):
    return 2 + 3 * x  # intercept=2, slope=3

# Generate data
np.random.seed(42)
x = np.array([1, 2, 3, 4, 5])
epsilon = np.random.normal(0, 1, size=5)  # random error
y = true_f(x) + epsilon  # observed data

print(f"x: {x}")
print(f"true f(x): {true_f(x)}")
print(f"error: {epsilon.round(2)}")
print(f"observed y: {y.round(2)}")
```
:::

### Residuals {#residuals}

::: {.callout-note icon=false title="Formula"}
$$e_i = y_i - \hat{y}_i$$
:::

**Where:**

- $e_i$ = residual for observation $i$
- $y_i$ = observed value
- $\hat{y}_i$ = predicted/fitted value

::: {.callout-tip title="Intuition" collapse="true"}
Residuals are the **leftover** after your model makes its prediction. They're your best estimate of the error term $\epsilon$.

- **Positive residual** ($e_i > 0$): Model under-predicted (actual > predicted)
- **Negative residual** ($e_i < 0$): Model over-predicted (actual < predicted)
- **Zero residual** ($e_i = 0$): Perfect prediction (rare!)

**Why they matter:** If your model is good, residuals should look like random noise with no pattern. If you see patterns in residuals, your model is missing something systematic.
:::

::: {.callout-note title="Python Implementation" collapse="true"}
```{python}
import numpy as np

# Example data
y_observed = np.array([5.2, 7.8, 10.1, 12.5, 15.3])
y_predicted = np.array([5.0, 8.0, 11.0, 14.0, 17.0])

# Calculate residuals
residuals = y_observed - y_predicted

print(f"Observed:  {y_observed}")
print(f"Predicted: {y_predicted}")
print(f"Residuals: {residuals}")
print(f"Sum of residuals: {residuals.sum():.2f}")  # Should be ~0 for good models
```
:::

::: {.callout-caution title="Common Pitfalls"}
- **Confusing residuals with errors**: Residuals ($e$) are *estimates* of errors ($\epsilon$). We never see true errors.
- **Forgetting the sign**: $e_i = y_i - \hat{y}_i$, not the other way around!
:::

---

## The General Linear Model {#the-general-linear-model}

The GLM is the foundation for most statistical models you'll encounter.

### The GLM Equation {#glm-equation}

::: {.callout-note icon=false title="Formula"}
$$\mathbf{y} = \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon}$$

where $\boldsymbol{\epsilon} \sim N(\mathbf{0}, \sigma^2\mathbf{I})$
:::

**Where:**

- $\mathbf{y}$ = outcome vector ($n \times 1$)
- $\mathbf{X}$ = design matrix ($n \times (p+1)$, includes intercept column)
- $\boldsymbol{\beta}$ = coefficient vector ($(p+1) \times 1$)
- $\boldsymbol{\epsilon}$ = error vector ($n \times 1$)
- $\sigma^2\mathbf{I}$ = errors are independent with constant variance

::: {.callout-tip title="Intuition" collapse="true"}
This single equation encompasses:

- Simple regression ($p = 1$ predictor)
- Multiple regression ($p > 1$ predictors)
- ANOVA (predictors are group indicators)
- ANCOVA (mix of continuous and categorical)

**Matrix multiplication** $\mathbf{X}\boldsymbol{\beta}$ gives you predicted values for all observations at once. Each row of $\mathbf{X}$ represents one observation's predictor values; multiplying by $\boldsymbol{\beta}$ gives that observation's predicted outcome.

**The assumption** $\boldsymbol{\epsilon} \sim N(\mathbf{0}, \sigma^2\mathbf{I})$ means:

1. Errors average to zero (no systematic bias)
2. All errors have the same variance (homoscedasticity)
3. Errors are uncorrelated with each other (independence)
4. Errors are normally distributed
:::

::: {.callout-note title="Python Implementation" collapse="true"}
```{python}
import numpy as np

# Example: y = 2 + 3*x with 5 observations
y = np.array([5, 8, 11, 14, 17])  # outcomes

# Design matrix: first column = 1s (intercept), second = x values
X = np.array([
    [1, 1],
    [1, 2],
    [1, 3],
    [1, 4],
    [1, 5]
])

# Coefficients (if we knew them)
beta = np.array([2, 3])  # intercept=2, slope=3

# Matrix multiplication gives predictions
y_predicted = X @ beta  # @ is matrix multiplication
print(f"Predicted: {y_predicted}")
print(f"Observed:  {y}")
```
:::

### Design Matrix Structure {#design-matrix}

::: {.callout-note icon=false title="Formula"}
$$\mathbf{X} = \begin{bmatrix} 1 & x_{11} & x_{12} & \cdots & x_{1p} \\ 1 & x_{21} & x_{22} & \cdots & x_{2p} \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ 1 & x_{n1} & x_{n2} & \cdots & x_{np} \end{bmatrix}$$
:::

::: {.callout-tip title="Intuition" collapse="true"}
The design matrix is how we encode our predictors for the model:

- **Rows** = observations (one per participant/trial)
- **Columns** = predictors (first column is usually 1s for intercept)
- **First column of 1s** = allows us to estimate the intercept

For **categorical predictors**, we use dummy coding (0s and 1s) or other contrast coding schemes instead of raw category labels.
:::

::: {.callout-note title="Python Implementation" collapse="true"}
```{python}
import numpy as np
import polars as pl

# From a DataFrame
df = pl.DataFrame({
    'y': [5, 8, 11, 14, 17],
    'x1': [1, 2, 3, 4, 5],
    'x2': [0.5, 1.0, 1.5, 2.0, 2.5]
})

# Extract as numpy arrays
y = df['y'].to_numpy()

# Create design matrix (add intercept column)
X = np.column_stack([
    np.ones(len(df)),           # intercept
    df['x1'].to_numpy(),        # predictor 1
    df['x2'].to_numpy()         # predictor 2
])

print("Design matrix X:")
print(X)
print(f"\nShape: {X.shape}")  # (n_observations, n_predictors+1)
```
:::

### OLS Estimator {#ols-estimator}

::: {.callout-note icon=false title="Formula"}
$$\hat{\boldsymbol{\beta}} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}$$
:::

**Where:**

- $\hat{\boldsymbol{\beta}}$ = estimated coefficients
- $\mathbf{X}^T$ = transpose of design matrix
- $(\mathbf{X}^T\mathbf{X})^{-1}$ = inverse of $\mathbf{X}^T\mathbf{X}$

::: {.callout-tip title="Intuition" collapse="true"}
OLS = **Ordinary Least Squares**. This formula finds the coefficients that minimize the sum of squared residuals.

**Where it comes from:**

1. We want to minimize: $\sum_i (y_i - \hat{y}_i)^2 = (\mathbf{y} - \mathbf{X}\boldsymbol{\beta})^T(\mathbf{y} - \mathbf{X}\boldsymbol{\beta})$
2. Take the derivative with respect to $\boldsymbol{\beta}$ and set to zero
3. Solve for $\boldsymbol{\beta}$... and you get this formula!

**Reading the formula:**

- $\mathbf{X}^T\mathbf{y}$ = how much each predictor correlates with the outcome
- $(\mathbf{X}^T\mathbf{X})^{-1}$ = adjusts for correlations among predictors
:::

::: {.callout-note title="Python Implementation" collapse="true"}
```{python}
import numpy as np

# Example data
y = np.array([5.2, 7.8, 10.1, 13.2, 15.9])
X = np.array([
    [1, 1],
    [1, 2],
    [1, 3],
    [1, 4],
    [1, 5]
])

# OLS formula step by step
XtX = X.T @ X                    # X'X
XtX_inv = np.linalg.inv(XtX)     # (X'X)^(-1)
Xty = X.T @ y                    # X'y
beta_hat = XtX_inv @ Xty         # Final estimate

print(f"Estimated coefficients: {beta_hat}")
print(f"  Intercept: {beta_hat[0]:.3f}")
print(f"  Slope: {beta_hat[1]:.3f}")

# Verify with numpy's built-in (uses same math internally)
beta_numpy, residuals, rank, s = np.linalg.lstsq(X, y, rcond=None)
print(f"\nNumPy lstsq result: {beta_numpy}")
```
:::

::: {.callout-caution title="Common Pitfalls"}
- **Forgetting the intercept column**: Your design matrix needs a column of 1s unless you're centering predictors
- **Singular matrix**: If predictors are perfectly correlated, $\mathbf{X}^T\mathbf{X}$ can't be inverted
- **Order matters for matrix multiplication**: $\mathbf{X}^T\mathbf{X}$ is different from $\mathbf{X}\mathbf{X}^T$
:::

**See also:** [Design Matrix](#design-matrix), [Variance of Coefficients](#variance-of-coefficients)

### Variance of Coefficients {#variance-of-coefficients}

::: {.callout-note icon=false title="Formula"}
$$\text{Var}(\hat{\boldsymbol{\beta}}) = \sigma^2(\mathbf{X}^T\mathbf{X})^{-1}$$
:::

::: {.callout-tip title="Intuition" collapse="true"}
This formula tells us how uncertain we are about our coefficient estimates.

**Key insights:**

- Larger $\sigma^2$ (more noise) → more uncertainty in coefficients
- $(\mathbf{X}^T\mathbf{X})^{-1}$ depends on your predictor values:
  - More spread in $X$ → smaller variance (more precise estimates)
  - Correlated predictors → larger variance (less precise)
  - More observations → smaller variance

**The diagonal elements** give variances for each coefficient; off-diagonal elements give covariances between coefficients.
:::

::: {.callout-note title="Python Implementation" collapse="true"}
```{python}
import numpy as np

# Continuing from OLS example
# We need to estimate sigma^2 first (from residuals)
y = np.array([5.2, 7.8, 10.1, 13.2, 15.9])
X = np.array([[1, 1], [1, 2], [1, 3], [1, 4], [1, 5]])

# Get beta_hat (from before)
beta_hat = np.linalg.inv(X.T @ X) @ X.T @ y

# Calculate residuals and estimate sigma^2
y_hat = X @ beta_hat
residuals = y - y_hat
n, p_plus_1 = X.shape
sigma2_hat = np.sum(residuals**2) / (n - p_plus_1)  # n - p - 1

# Variance-covariance matrix of coefficients
var_beta = sigma2_hat * np.linalg.inv(X.T @ X)

print("Variance-covariance matrix:")
print(var_beta)
print(f"\nVariance of intercept: {var_beta[0,0]:.4f}")
print(f"Variance of slope: {var_beta[1,1]:.4f}")
```
:::

### Standard Errors {#standard-errors}

::: {.callout-note icon=false title="Formula"}
$$SE(\hat{\beta}_j) = \hat{\sigma}\sqrt{(\mathbf{X}^T\mathbf{X})^{-1}_{jj}}$$
:::

**Where:**

- $SE(\hat{\beta}_j)$ = standard error of the $j$th coefficient
- $\hat{\sigma}$ = estimated residual standard deviation
- $(\mathbf{X}^T\mathbf{X})^{-1}_{jj}$ = the $j$th diagonal element

::: {.callout-tip title="Intuition" collapse="true"}
Standard errors are the **square roots of the variances**. They're in the same units as the coefficients, making them easier to interpret.

**SE tells you:** How much would $\hat{\beta}_j$ vary if you repeated the study many times?

A coefficient is typically "significant" if it's more than ~2 SEs away from zero (this is the t-test!).
:::

::: {.callout-note title="Python Implementation" collapse="true"}
```{python}
import numpy as np

# Continuing from variance example
# var_beta already computed above

# Standard errors are square roots of diagonal
se_intercept = np.sqrt(var_beta[0, 0])
se_slope = np.sqrt(var_beta[1, 1])

print(f"SE of intercept: {se_intercept:.4f}")
print(f"SE of slope: {se_slope:.4f}")

# Or get all SEs at once
se_all = np.sqrt(np.diag(var_beta))
print(f"All SEs: {se_all}")
```
:::

### Residual Variance Estimate {#residual-variance}

::: {.callout-note icon=false title="Formula"}
$$\hat{\sigma}^2 = \frac{\sum_i(y_i - \hat{y}_i)^2}{n - p - 1} = \frac{SSE}{n - p - 1}$$
:::

**Where:**

- $\hat{\sigma}^2$ = estimated error variance
- $SSE$ = sum of squared errors (residuals)
- $n - p - 1$ = degrees of freedom (residual df)

::: {.callout-tip title="Intuition" collapse="true"}
We divide by $n - p - 1$ (not $n$) because we "used up" $p + 1$ degrees of freedom estimating the coefficients.

**Why $n - p - 1$?**

- $n$ = total observations
- $p$ = number of predictors
- $1$ = the intercept
- Each coefficient we estimate "costs" one degree of freedom

This correction makes $\hat{\sigma}^2$ an **unbiased** estimator of the true $\sigma^2$.
:::

::: {.callout-note title="Python Implementation" collapse="true"}
```{python}
import numpy as np

# Example
y = np.array([5.2, 7.8, 10.1, 13.2, 15.9])
X = np.array([[1, 1], [1, 2], [1, 3], [1, 4], [1, 5]])

beta_hat = np.linalg.inv(X.T @ X) @ X.T @ y
y_hat = X @ beta_hat
residuals = y - y_hat

n = len(y)
p = X.shape[1] - 1  # number of predictors (not counting intercept)

SSE = np.sum(residuals**2)
df_residual = n - p - 1
sigma2_hat = SSE / df_residual
sigma_hat = np.sqrt(sigma2_hat)

print(f"SSE: {SSE:.4f}")
print(f"Residual df: {df_residual}")
print(f"Estimated variance (sigma^2): {sigma2_hat:.4f}")
print(f"Estimated SD (sigma): {sigma_hat:.4f}")
```
:::

::: {.callout-tip title="Your Turn: OLS by Hand"}
Given the following data, calculate the OLS estimates by hand (well, with numpy):

```{python}
import numpy as np

y = np.array([3, 5, 7, 9, 11])
x = np.array([1, 2, 3, 4, 5])

# 1. Create design matrix X (with intercept column)
# 2. Calculate beta_hat using OLS formula
# 3. Calculate residuals
# 4. Calculate estimated sigma^2
# 5. Calculate standard errors
```
:::

::: {.callout-note title="Solution" collapse="true"}
```{python}
import numpy as np

y = np.array([3, 5, 7, 9, 11])
x = np.array([1, 2, 3, 4, 5])

# 1. Design matrix
X = np.column_stack([np.ones(5), x])
print("Design matrix:\n", X)

# 2. OLS estimates
beta_hat = np.linalg.inv(X.T @ X) @ X.T @ y
print(f"\nCoefficients: intercept={beta_hat[0]:.2f}, slope={beta_hat[1]:.2f}")

# 3. Residuals
y_hat = X @ beta_hat
residuals = y - y_hat
print(f"Residuals: {residuals}")

# 4. Sigma^2
n, p_plus_1 = X.shape
sigma2_hat = np.sum(residuals**2) / (n - p_plus_1)
print(f"Sigma^2: {sigma2_hat:.6f}")

# 5. Standard errors
var_beta = sigma2_hat * np.linalg.inv(X.T @ X)
se = np.sqrt(np.diag(var_beta))
print(f"SE intercept: {se[0]:.6f}")
print(f"SE slope: {se[1]:.6f}")
```
:::

---

## Model Fit and Comparison {#model-fit-and-comparison}

How do we know if our model is any good? These metrics help us evaluate and compare models.

### R-squared {#r-squared}

::: {.callout-note icon=false title="Formula"}
$$R^2 = 1 - \frac{SS_{res}}{SS_{tot}} = 1 - \frac{\sum_i(y_i - \hat{y}_i)^2}{\sum_i(y_i - \bar{y})^2}$$
:::

**Where:**

- $SS_{res}$ = residual sum of squares (unexplained variance)
- $SS_{tot}$ = total sum of squares (total variance)
- $\bar{y}$ = mean of $y$

::: {.callout-tip title="Intuition" collapse="true"}
$R^2$ answers: **What proportion of variance in $y$ does our model explain?**

- $R^2 = 0$: Model explains nothing (predictions = $\bar{y}$)
- $R^2 = 1$: Model explains everything (perfect prediction)
- $R^2 = 0.5$: Model explains 50% of variance

**Where it comes from:** We partition total variance into explained + unexplained:

$$SS_{tot} = SS_{model} + SS_{res}$$

Then $R^2 = SS_{model} / SS_{tot} = 1 - SS_{res} / SS_{tot}$

**Important:** $R^2$ always increases when you add predictors, even useless ones!
:::

::: {.callout-note title="Python Implementation" collapse="true"}
```{python}
import numpy as np

y = np.array([5.2, 7.8, 10.1, 13.2, 15.9])
y_hat = np.array([5.3, 7.9, 10.5, 13.1, 15.7])

# Calculate R-squared
y_bar = np.mean(y)
SS_tot = np.sum((y - y_bar)**2)
SS_res = np.sum((y - y_hat)**2)
R2 = 1 - SS_res / SS_tot

print(f"SS_total: {SS_tot:.4f}")
print(f"SS_residual: {SS_res:.4f}")
print(f"R-squared: {R2:.4f}")
print(f"Interpretation: Model explains {R2*100:.1f}% of variance")
```
:::

::: {.callout-caution title="Common Pitfalls"}
- **Thinking higher is always better**: You can inflate $R^2$ by adding noise predictors
- **Comparing across different outcomes**: $R^2$ depends on variance in $y$
- **Using with small samples**: $R^2$ is biased upward in small samples
:::

### Adjusted R-squared {#adjusted-r-squared}

::: {.callout-note icon=false title="Formula"}
$$R^2_{adj} = 1 - (1 - R^2)\frac{n - 1}{n - p - 1}$$
:::

::: {.callout-tip title="Intuition" collapse="true"}
Adjusted $R^2$ **penalizes** for adding predictors. It can actually *decrease* if you add a useless predictor.

**The penalty term** $\frac{n-1}{n-p-1}$ is always $\geq 1$ and gets larger as you add more predictors ($p$ increases).

**Use adjusted $R^2$ when:** Comparing models with different numbers of predictors.
:::

::: {.callout-note title="Python Implementation" collapse="true"}
```{python}
import numpy as np

# Continuing from R-squared example
R2 = 0.95  # from previous calculation
n = 5      # observations
p = 1      # predictors (not counting intercept)

R2_adj = 1 - (1 - R2) * (n - 1) / (n - p - 1)

print(f"R-squared: {R2:.4f}")
print(f"Adjusted R-squared: {R2_adj:.4f}")

# Compare with more predictors (same R2)
p2 = 3
R2_adj_2 = 1 - (1 - R2) * (n - 1) / (n - p2 - 1)
print(f"\nWith {p2} predictors (same R2): {R2_adj_2:.4f}")
print("Note: More predictors → lower adjusted R2 (if R2 doesn't improve much)")
```
:::

### F-statistic {#f-statistic}

::: {.callout-note icon=false title="Formula"}
$$F = \frac{(SS_{tot} - SS_{res})/p}{SS_{res}/(n-p-1)} = \frac{MS_{model}}{MS_{error}}$$
:::

**Where:**

- $MS_{model}$ = mean square for model (explained variance / df)
- $MS_{error}$ = mean square for error (unexplained variance / df)
- $p$ = number of predictors
- $n - p - 1$ = residual degrees of freedom

::: {.callout-tip title="Intuition" collapse="true"}
The F-statistic tests: **Does the model explain more variance than expected by chance?**

It's a ratio of two variances:

- **Numerator**: How much variance does the model explain (per predictor)?
- **Denominator**: How much noise is left over (per residual df)?

If the model is useful, the numerator should be much larger than the denominator, giving $F >> 1$.

**Under the null hypothesis** (model explains nothing), $F \sim F(p, n-p-1)$.
:::

::: {.callout-note title="Python Implementation" collapse="true"}
```{python}
import numpy as np
from scipy import stats

y = np.array([5.2, 7.8, 10.1, 13.2, 15.9])
y_hat = np.array([5.3, 7.9, 10.5, 13.1, 15.7])
y_bar = np.mean(y)

n = len(y)
p = 1  # predictors

SS_tot = np.sum((y - y_bar)**2)
SS_res = np.sum((y - y_hat)**2)
SS_model = SS_tot - SS_res

# Mean squares
MS_model = SS_model / p
MS_error = SS_res / (n - p - 1)

# F-statistic
F = MS_model / MS_error

# p-value
p_value = 1 - stats.f.cdf(F, p, n - p - 1)

print(f"MS_model: {MS_model:.4f}")
print(f"MS_error: {MS_error:.4f}")
print(f"F-statistic: {F:.4f}")
print(f"p-value: {p_value:.6f}")
```
:::

### Likelihood Ratio Test {#likelihood-ratio-test}

::: {.callout-note icon=false title="Formula"}
$$\chi^2 = -2(\log L_{reduced} - \log L_{full}) = -2 \log\left(\frac{L_{reduced}}{L_{full}}\right)$$

with $df$ = difference in number of parameters
:::

::: {.callout-tip title="Intuition" collapse="true"}
The LRT compares two **nested models** (the reduced model is a special case of the full model).

**Key insight:** If the full model is much better, the likelihood ratio $L_{reduced}/L_{full}$ will be very small, making $-2 \log(\cdot)$ very large.

**When to use:** Comparing models fit with maximum likelihood (logistic regression, mixed models, etc.) where F-tests don't apply.

Under $H_0$ (reduced model is adequate), $\chi^2 \sim \chi^2(df)$.
:::

::: {.callout-note title="Python Implementation" collapse="true"}
```{python}
from scipy import stats

# Example log-likelihoods (you'd get these from your fitted models)
logL_reduced = -45.2  # simpler model
logL_full = -42.8     # more complex model

# LRT statistic
chi2 = -2 * (logL_reduced - logL_full)

# Degrees of freedom = difference in parameters
df = 2  # full model has 2 more parameters

# p-value
p_value = 1 - stats.chi2.cdf(chi2, df)

print(f"Log-likelihood (reduced): {logL_reduced}")
print(f"Log-likelihood (full): {logL_full}")
print(f"Chi-squared statistic: {chi2:.4f}")
print(f"Degrees of freedom: {df}")
print(f"p-value: {p_value:.6f}")
```
:::

---

## Inference and Uncertainty {#inference-and-uncertainty}

How do we quantify uncertainty and test hypotheses about our estimates?

### t-statistic for Coefficients {#t-statistic}

::: {.callout-note icon=false title="Formula"}
$$t = \frac{\hat{\beta}_j - \beta_{0}}{SE(\hat{\beta}_j)}$$

Typically testing $\beta_0 = 0$:

$$t = \frac{\hat{\beta}_j}{SE(\hat{\beta}_j)}$$
:::

::: {.callout-tip title="Intuition" collapse="true"}
The t-statistic measures **how many standard errors the estimate is from the null value**.

- $|t| > 2$ roughly corresponds to $p < 0.05$ (for reasonable $df$)
- Larger $|t|$ = stronger evidence against the null

**Why it works:** If $H_0$ is true and assumptions hold, $t \sim t(n - p - 1)$.

**Relationship to confidence intervals:** If the 95% CI doesn't include 0, then $|t| > t_{crit}$ and $p < 0.05$.
:::

::: {.callout-note title="Python Implementation" collapse="true"}
```{python}
import numpy as np
from scipy import stats

# Example
beta_hat = 2.5
se_beta = 0.8
n = 30
p = 2  # predictors

# t-statistic (testing beta = 0)
t_stat = beta_hat / se_beta
df = n - p - 1

# Two-tailed p-value
p_value = 2 * (1 - stats.t.cdf(abs(t_stat), df))

print(f"Coefficient: {beta_hat}")
print(f"SE: {se_beta}")
print(f"t-statistic: {t_stat:.3f}")
print(f"df: {df}")
print(f"p-value: {p_value:.4f}")
```
:::

### Confidence Intervals {#confidence-intervals}

::: {.callout-note icon=false title="Formula"}
$$\hat{\beta}_j \pm t_{\alpha/2, n-p-1} \cdot SE(\hat{\beta}_j)$$
:::

**Where:**

- $t_{\alpha/2, n-p-1}$ = critical t-value for desired confidence level
- For 95% CI: $\alpha = 0.05$, so we use $t_{0.025, df}$

::: {.callout-tip title="Intuition" collapse="true"}
A 95% CI means: **If we repeated this study many times, 95% of the intervals would contain the true $\beta$.**

It does NOT mean: "There's a 95% probability the true value is in this interval." (The true value either is or isn't in the interval—we just don't know which.)

**Interpretation tip:** The width of the CI reflects uncertainty. Wider CI = more uncertain.
:::

::: {.callout-note title="Python Implementation" collapse="true"}
```{python}
import numpy as np
from scipy import stats

beta_hat = 2.5
se_beta = 0.8
n = 30
p = 2
df = n - p - 1
confidence = 0.95

# Critical t-value
t_crit = stats.t.ppf((1 + confidence) / 2, df)

# Confidence interval
ci_lower = beta_hat - t_crit * se_beta
ci_upper = beta_hat + t_crit * se_beta

print(f"Coefficient: {beta_hat}")
print(f"SE: {se_beta}")
print(f"t critical (95%): {t_crit:.3f}")
print(f"95% CI: [{ci_lower:.3f}, {ci_upper:.3f}]")

# Check if CI excludes 0
if ci_lower > 0 or ci_upper < 0:
    print("CI excludes 0 → significant at alpha = 0.05")
else:
    print("CI includes 0 → not significant at alpha = 0.05")
```
:::

::: {.callout-caution title="Common Pitfalls"}
- **Using z instead of t**: For small samples, you need the t-distribution
- **Forgetting it's two-tailed**: For 95% CI, use $t_{0.025}$, not $t_{0.05}$
- **Misinterpreting**: It's about the procedure, not probability of the interval
:::

### Standard Error of the Mean {#standard-error-mean}

::: {.callout-note icon=false title="Formula"}
$$SE(\bar{x}) = \frac{s}{\sqrt{n}}$$
:::

**Where:**

- $s$ = sample standard deviation
- $n$ = sample size

::: {.callout-tip title="Intuition" collapse="true"}
The SE of the mean tells you: **How much would the sample mean vary across repeated samples?**

Key insights:

- Larger $n$ → smaller SE (more precise estimate)
- SE decreases with $\sqrt{n}$, so you need 4x the sample size to halve the SE
- This is different from $s$ (which describes spread of individual observations)
:::

::: {.callout-note title="Python Implementation" collapse="true"}
```{python}
import numpy as np
from scipy import stats

data = np.array([245, 312, 289, 267, 301, 278, 295])

# Manual calculation
n = len(data)
sample_mean = np.mean(data)
sample_sd = np.std(data, ddof=1)  # ddof=1 for sample SD
se_mean = sample_sd / np.sqrt(n)

print(f"Sample mean: {sample_mean:.2f}")
print(f"Sample SD: {sample_sd:.2f}")
print(f"SE of mean: {se_mean:.2f}")

# Using scipy (same result)
se_scipy = stats.sem(data)
print(f"scipy.stats.sem: {se_scipy:.2f}")
```
:::

::: {.callout-tip title="Your Turn: Confidence Interval"}
Given reaction time data from 10 participants with mean = 285ms and SD = 45ms:

1. Calculate the SE of the mean
2. Calculate the 95% CI for the mean
3. Calculate the 99% CI for the mean

Which interval is wider? Why?
:::

::: {.callout-note title="Solution" collapse="true"}
```{python}
import numpy as np
from scipy import stats

mean_rt = 285
sd_rt = 45
n = 10

# 1. SE of mean
se = sd_rt / np.sqrt(n)
print(f"SE of mean: {se:.2f}")

# 2. 95% CI
df = n - 1
t_95 = stats.t.ppf(0.975, df)
ci_95 = (mean_rt - t_95 * se, mean_rt + t_95 * se)
print(f"95% CI: [{ci_95[0]:.2f}, {ci_95[1]:.2f}]")
print(f"  Width: {ci_95[1] - ci_95[0]:.2f}")

# 3. 99% CI
t_99 = stats.t.ppf(0.995, df)
ci_99 = (mean_rt - t_99 * se, mean_rt + t_99 * se)
print(f"99% CI: [{ci_99[0]:.2f}, {ci_99[1]:.2f}]")
print(f"  Width: {ci_99[1] - ci_99[0]:.2f}")

print("\n99% CI is wider because higher confidence requires a larger interval!")
```
:::

---

## Different Outcome Types {#different-outcome-types}

When your outcome isn't continuous, you need different models.

### Logistic Regression Model {#logistic-regression}

::: {.callout-note icon=false title="Formula"}
**Logit form:**
$$\log\left(\frac{p}{1-p}\right) = \mathbf{x}^T\boldsymbol{\beta}$$

**Probability form:**
$$p = \frac{1}{1 + e^{-\mathbf{x}^T\boldsymbol{\beta}}}$$
:::

**Where:**

- $p$ = probability of the outcome (e.g., success, "yes")
- $\frac{p}{1-p}$ = odds
- $\log\left(\frac{p}{1-p}\right)$ = log-odds (logit)

::: {.callout-tip title="Intuition" collapse="true"}
Logistic regression models **binary outcomes** (0/1, yes/no, success/failure).

**Why log-odds?**

- Probabilities are bounded (0 to 1)
- The linear predictor ($\mathbf{x}^T\boldsymbol{\beta}$) can be any real number
- Log-odds transforms probabilities to an unbounded scale

**The sigmoid function** $\frac{1}{1 + e^{-x}}$ squashes any real number back to (0, 1).

**Coefficients mean:** A 1-unit increase in $x_j$ changes the log-odds by $\beta_j$.
:::

::: {.callout-note title="Python Implementation" collapse="true"}
```{python}
import numpy as np

def logistic(x):
    """Sigmoid/logistic function: converts log-odds to probability"""
    return 1 / (1 + np.exp(-x))

def logit(p):
    """Logit function: converts probability to log-odds"""
    return np.log(p / (1 - p))

# Example: predict probability from coefficients
intercept = -2
beta_x = 0.5
x_values = np.array([0, 2, 4, 6, 8])

# Calculate log-odds and probabilities
log_odds = intercept + beta_x * x_values
probabilities = logistic(log_odds)

print("x\tlog-odds\tprobability")
for x, lo, p in zip(x_values, log_odds, probabilities):
    print(f"{x}\t{lo:.2f}\t\t{p:.3f}")
```
:::

### Odds Ratio {#odds-ratio}

::: {.callout-note icon=false title="Formula"}
$$OR = e^{\beta_j}$$
:::

::: {.callout-tip title="Intuition" collapse="true"}
The odds ratio is the **multiplicative change in odds** for a 1-unit increase in $x_j$.

**Interpretation:**

- $OR = 1$: No effect (odds unchanged)
- $OR > 1$: Increased odds (e.g., $OR = 2$ means odds double)
- $OR < 1$: Decreased odds (e.g., $OR = 0.5$ means odds halve)

**Example:** If $\beta = 0.693$ and $OR = e^{0.693} = 2$, then a 1-unit increase in $x$ doubles the odds of the outcome.

**Why exponentiate?** Because $\beta$ is on the log-odds scale; $e^\beta$ converts to the odds scale.
:::

::: {.callout-note title="Python Implementation" collapse="true"}
```{python}
import numpy as np

# Example logistic regression coefficient
beta = 0.693

# Convert to odds ratio
OR = np.exp(beta)
print(f"Beta coefficient: {beta}")
print(f"Odds Ratio: {OR:.2f}")

# Interpretation
if OR > 1:
    print(f"A 1-unit increase in x multiplies odds by {OR:.2f}")
elif OR < 1:
    print(f"A 1-unit increase in x multiplies odds by {OR:.2f} (reduces odds)")
else:
    print("No effect on odds")

# For a range of betas
betas = np.array([-1, -0.5, 0, 0.5, 1])
ORs = np.exp(betas)
print("\nBeta\tOdds Ratio")
for b, o in zip(betas, ORs):
    print(f"{b:.1f}\t{o:.2f}")
```
:::

::: {.callout-caution title="Common Pitfalls"}
- **Confusing OR with probability**: $OR = 2$ does NOT mean probability doubles
- **Forgetting to exponentiate**: Raw coefficients are log-odds, not odds ratios
- **Assuming symmetry**: $OR = 2$ (odds double) is not the same magnitude as $OR = 0.5$ (odds halve)
:::

---

## Resampling Methods {#resampling-methods}

When parametric assumptions are questionable, resampling provides distribution-free inference.

### Bootstrap Percentile CI {#bootstrap-ci}

::: {.callout-note icon=false title="Formula"}
$$CI_{1-\alpha} = [Q_{\alpha/2}(\hat{\theta}^*), Q_{1-\alpha/2}(\hat{\theta}^*)]$$
:::

**Where:**

- $\hat{\theta}^*$ = bootstrap distribution of the statistic
- $Q_p(\cdot)$ = the $p$th percentile

::: {.callout-tip title="Intuition" collapse="true"}
The bootstrap creates a **sampling distribution by resampling your data**.

**The procedure:**

1. Resample $n$ observations **with replacement** from your data
2. Calculate your statistic on the resample
3. Repeat many times (e.g., 10,000)
4. Use percentiles of the bootstrap distribution as CI bounds

**Why it works:** The variability in bootstrap samples mimics the variability you'd see if you could actually repeat the study.

**When to use:** When you're unsure about parametric assumptions, or for statistics without easy formulas (e.g., median, correlation).
:::

::: {.callout-note title="Python Implementation" collapse="true"}
```{python}
import numpy as np
from scipy import stats

np.random.seed(42)

# Original data
data = np.array([12, 15, 14, 10, 12, 11, 14, 13, 15, 16])

# Bootstrap parameters
n_bootstrap = 10000
n = len(data)

# Bootstrap resampling
bootstrap_means = np.zeros(n_bootstrap)
for i in range(n_bootstrap):
    # Resample with replacement
    resample = np.random.choice(data, size=n, replace=True)
    bootstrap_means[i] = np.mean(resample)

# Percentile CI
ci_lower = np.percentile(bootstrap_means, 2.5)
ci_upper = np.percentile(bootstrap_means, 97.5)

print(f"Original mean: {np.mean(data):.2f}")
print(f"Bootstrap SE: {np.std(bootstrap_means):.3f}")
print(f"95% Bootstrap CI: [{ci_lower:.2f}, {ci_upper:.2f}]")

# Compare to parametric CI
se = stats.sem(data)
t_crit = stats.t.ppf(0.975, n-1)
ci_param = (np.mean(data) - t_crit*se, np.mean(data) + t_crit*se)
print(f"95% Parametric CI: [{ci_param[0]:.2f}, {ci_param[1]:.2f}]")
```
:::

::: {.callout-caution title="Common Pitfalls"}
- **Too few bootstrap samples**: Use at least 1,000; 10,000 is better
- **Forgetting "with replacement"**: This is essential for bootstrap
- **Small original sample**: Bootstrap can't create information that isn't there
:::

### Power {#power}

::: {.callout-note icon=false title="Formula"}
$$\text{Power} = P(\text{reject } H_0 \mid H_1 \text{ is true}) = 1 - \beta$$
:::

**Where:**

- $\beta$ = probability of Type II error (false negative)
- Power = probability of detecting a true effect

::: {.callout-tip title="Intuition" collapse="true"}
Power answers: **If there's a real effect, what's the chance we'll find it?**

**Factors that increase power:**

- Larger sample size ($n$)
- Larger effect size (bigger true difference)
- Smaller variability ($\sigma$)
- Higher $\alpha$ level (but increases false positives)

**Convention:** We typically want power $\geq 0.80$ (80% chance of detecting a true effect).
:::

::: {.callout-note title="Python Implementation" collapse="true"}
```{python}
import numpy as np
from scipy import stats

# Power analysis for one-sample t-test
# Testing if mean differs from 0

def power_one_sample_t(n, effect_size, alpha=0.05):
    """
    Calculate power for one-sample t-test.
    effect_size = (true_mean - null_mean) / sd (Cohen's d)
    """
    df = n - 1
    t_crit = stats.t.ppf(1 - alpha/2, df)  # two-tailed

    # Non-central t-distribution parameter
    ncp = effect_size * np.sqrt(n)

    # Power = P(|t| > t_crit | H1 true)
    power = 1 - stats.nct.cdf(t_crit, df, ncp) + stats.nct.cdf(-t_crit, df, ncp)
    return power

# Example: medium effect size (d = 0.5)
effect_size = 0.5

print("Sample Size\tPower")
for n in [10, 20, 30, 50, 100]:
    pwr = power_one_sample_t(n, effect_size)
    print(f"{n}\t\t{pwr:.3f}")

# Find sample size for 80% power
print("\nSearching for n to achieve 80% power...")
for n in range(10, 200):
    if power_one_sample_t(n, effect_size) >= 0.80:
        print(f"Need n = {n} for 80% power with d = {effect_size}")
        break
```
:::

::: {.callout-tip title="Your Turn: Bootstrap"}
Use the bootstrap to estimate the 95% CI for the **median** reaction time:

```{python}
import numpy as np
np.random.seed(123)

rt_data = np.array([245, 312, 289, 267, 301, 278, 295, 310, 256, 288])

# 1. Calculate the sample median
# 2. Generate 10,000 bootstrap samples
# 3. Calculate median for each bootstrap sample
# 4. Find the 2.5th and 97.5th percentiles
```
:::

::: {.callout-note title="Solution" collapse="true"}
```{python}
import numpy as np
np.random.seed(123)

rt_data = np.array([245, 312, 289, 267, 301, 278, 295, 310, 256, 288])

# 1. Sample median
sample_median = np.median(rt_data)
print(f"Sample median: {sample_median}")

# 2-3. Bootstrap
n_bootstrap = 10000
n = len(rt_data)
bootstrap_medians = np.zeros(n_bootstrap)

for i in range(n_bootstrap):
    resample = np.random.choice(rt_data, size=n, replace=True)
    bootstrap_medians[i] = np.median(resample)

# 4. Percentile CI
ci_lower = np.percentile(bootstrap_medians, 2.5)
ci_upper = np.percentile(bootstrap_medians, 97.5)

print(f"Bootstrap SE: {np.std(bootstrap_medians):.2f}")
print(f"95% CI for median: [{ci_lower}, {ci_upper}]")
```
:::

---

## Mixed Models {#mixed-models}

When observations are clustered (repeated measures, hierarchical data), mixed models account for non-independence.

### Random Intercept Model {#random-intercept}

::: {.callout-note icon=false title="Formula"}
$$y_{ij} = \beta_0 + u_{0j} + \beta_1 x_{ij} + \epsilon_{ij}$$

where:
$$u_{0j} \sim N(0, \tau^2)$$
$$\epsilon_{ij} \sim N(0, \sigma^2)$$
:::

**Where:**

- $y_{ij}$ = outcome for observation $i$ in group $j$
- $\beta_0$ = fixed intercept (grand mean)
- $u_{0j}$ = random intercept for group $j$ (deviation from grand mean)
- $\beta_1$ = fixed effect of $x$
- $\epsilon_{ij}$ = residual error
- $\tau^2$ = between-group variance
- $\sigma^2$ = within-group variance

::: {.callout-tip title="Intuition" collapse="true"}
A random intercept model allows **each group to have its own baseline** while sharing a common slope.

**Example:** Participants in a study each have different average reaction times, but the effect of caffeine is the same for everyone.

**The variance decomposition:**

- $\tau^2$ = how much groups differ from each other
- $\sigma^2$ = how much observations vary within groups

**Why "random"?** We treat groups as a random sample from a larger population of possible groups, not as fixed categories we specifically chose.
:::

::: {.callout-note title="Python Conceptual Example" collapse="true"}
```{python}
import numpy as np

# Simulating random intercept data
np.random.seed(42)

n_groups = 5
n_per_group = 10
n_total = n_groups * n_per_group

# Fixed effects
beta_0 = 50  # grand intercept
beta_1 = 3   # slope

# Variance components
tau = 10     # between-group SD
sigma = 5    # within-group SD

# Generate data
group_ids = np.repeat(range(n_groups), n_per_group)
x = np.random.normal(0, 1, n_total)

# Random intercepts (one per group)
u_0 = np.random.normal(0, tau, n_groups)

# Generate y
y = beta_0 + u_0[group_ids] + beta_1 * x + np.random.normal(0, sigma, n_total)

print("Group random intercepts (u_0j):")
for j, u in enumerate(u_0):
    print(f"  Group {j}: {u:+.2f} (effective intercept: {beta_0 + u:.2f})")

print(f"\nTotal variance: {np.var(y):.2f}")
print(f"Expected: tau^2 + sigma^2 = {tau**2 + sigma**2}")
```
:::

### ICC (Intraclass Correlation) {#icc}

::: {.callout-note icon=false title="Formula"}
$$ICC = \frac{\tau^2}{\tau^2 + \sigma^2}$$
:::

**Where:**

- $\tau^2$ = between-group variance
- $\sigma^2$ = within-group variance

::: {.callout-tip title="Intuition" collapse="true"}
ICC answers: **What proportion of total variance is due to group membership?**

**Interpretation:**

- $ICC = 0$: Groups don't matter (all variance is within-group)
- $ICC = 1$: All variance is between groups (observations in same group are identical)
- $ICC = 0.5$: Half the variance is between groups, half within

**Rule of thumb:** If ICC > 0.05-0.10, you probably need to account for clustering.

**Also means:** ICC = correlation between two randomly selected observations from the same group.
:::

::: {.callout-note title="Python Implementation" collapse="true"}
```{python}
import numpy as np

# From model output (you'd get these from your mixed model)
tau_squared = 100   # between-group variance
sigma_squared = 25  # within-group variance

# Calculate ICC
icc = tau_squared / (tau_squared + sigma_squared)

print(f"Between-group variance (tau^2): {tau_squared}")
print(f"Within-group variance (sigma^2): {sigma_squared}")
print(f"Total variance: {tau_squared + sigma_squared}")
print(f"ICC: {icc:.3f}")
print(f"\nInterpretation: {icc*100:.1f}% of variance is between groups")
```
:::

::: {.callout-caution title="Common Pitfalls"}
- **Ignoring clustering**: Standard errors will be too small, p-values too low
- **ICC = 0 doesn't mean no groups**: It means groups don't explain variance
- **Sample size**: You need enough groups AND enough observations per group
:::

---

## Linear Algebra Essentials {#linear-algebra-essentials}

These operations are the building blocks for understanding matrix-based formulas.

### Vectors and Matrices {#vectors-matrices}

::: {.callout-note icon=false title="Notation"}
**Column vector:**
$$\mathbf{y} = \begin{bmatrix} y_1 \\ y_2 \\ \vdots \\ y_n \end{bmatrix}$$

**Matrix:**
$$\mathbf{X} = \begin{bmatrix} 1 & x_{11} & \cdots & x_{1p} \\ 1 & x_{21} & \cdots & x_{2p} \\ \vdots & \vdots & \ddots & \vdots \\ 1 & x_{n1} & \cdots & x_{np} \end{bmatrix}$$
:::

::: {.callout-tip title="Intuition" collapse="true"}
**Vectors** are ordered lists of numbers. In statistics:

- Columns represent variables (e.g., all values of $y$)
- Rows represent observations (e.g., one participant's data)

**Matrices** are 2D arrays. In regression:

- Each row = one observation
- Each column = one predictor (first column usually 1s for intercept)
- The design matrix $\mathbf{X}$ encodes your predictors
:::

::: {.callout-note title="Python Implementation" collapse="true"}
```{python}
import numpy as np

# Column vector (note the double brackets for 2D)
y = np.array([[5], [8], [11], [14], [17]])
print("Column vector y:")
print(y)
print(f"Shape: {y.shape}")  # (5, 1)

# Or simply as 1D (more common in practice)
y_1d = np.array([5, 8, 11, 14, 17])
print(f"\n1D array shape: {y_1d.shape}")  # (5,)

# Matrix
X = np.array([
    [1, 1, 0.5],
    [1, 2, 1.0],
    [1, 3, 1.5],
    [1, 4, 2.0],
    [1, 5, 2.5]
])
print("\nDesign matrix X:")
print(X)
print(f"Shape: {X.shape}")  # (5, 3) = 5 observations, 3 columns
```
:::

### Matrix Operations {#matrix-operations}

::: {.callout-note icon=false title="Key Operations"}
- **Transpose:** $\mathbf{X}^T$ — flip rows and columns
- **Inverse:** $\mathbf{X}^{-1}$ — the matrix such that $\mathbf{X}\mathbf{X}^{-1} = \mathbf{I}$
- **Multiplication:** $\mathbf{AB}$ — columns of $\mathbf{A}$ must equal rows of $\mathbf{B}$
:::

::: {.callout-tip title="Intuition" collapse="true"}
**Transpose ($\mathbf{X}^T$):**

- Swaps rows and columns
- If $\mathbf{X}$ is $n \times p$, then $\mathbf{X}^T$ is $p \times n$
- Useful for aligning dimensions for multiplication

**Inverse ($\mathbf{X}^{-1}$):**

- Only exists for square, non-singular matrices
- Like division: $\mathbf{X}^{-1}\mathbf{X} = \mathbf{I}$ (identity matrix)
- In regression, we need $(\mathbf{X}^T\mathbf{X})^{-1}$

**Multiplication ($\mathbf{AB}$):**

- Result dimensions: $(m \times n) \times (n \times p) = (m \times p)$
- Not commutative: $\mathbf{AB} \neq \mathbf{BA}$ in general
:::

::: {.callout-note title="Python Implementation" collapse="true"}
```{python}
import numpy as np

A = np.array([[1, 2], [3, 4], [5, 6]])  # 3x2
B = np.array([[1, 2, 3], [4, 5, 6]])    # 2x3

print("A (3x2):")
print(A)

print("\nA transpose (2x3):")
print(A.T)

print("\nA @ B (3x2 @ 2x3 = 3x3):")
print(A @ B)

# Inverse (need square matrix)
C = np.array([[4, 7], [2, 6]])
C_inv = np.linalg.inv(C)
print("\nC:")
print(C)
print("\nC inverse:")
print(C_inv)
print("\nC @ C_inv (should be identity):")
print((C @ C_inv).round(10))  # round to avoid floating point noise
```
:::

### Eigenvalue Decomposition {#eigendecomposition}

::: {.callout-note icon=false title="Formula"}
$$\mathbf{A} = \mathbf{V}\boldsymbol{\Lambda}\mathbf{V}^T$$

Or equivalently, for each eigenvector-eigenvalue pair:
$$\mathbf{A}\mathbf{v}_i = \lambda_i\mathbf{v}_i$$
:::

**Where:**

- $\mathbf{A}$ = symmetric matrix (e.g., covariance matrix)
- $\mathbf{V}$ = matrix of eigenvectors (columns)
- $\boldsymbol{\Lambda}$ = diagonal matrix of eigenvalues
- $\lambda_i$ = the $i$th eigenvalue
- $\mathbf{v}_i$ = the $i$th eigenvector

::: {.callout-tip title="Intuition" collapse="true"}
Eigendecomposition finds the **principal directions** of a matrix.

**What it means:**

- Eigenvectors = directions that only get scaled (not rotated) by the matrix
- Eigenvalues = how much scaling happens in each direction

**Why it matters for PCA:**

- Eigenvectors of the covariance matrix = principal components
- Eigenvalues = variance explained by each component
- Larger eigenvalue = more important direction
:::

::: {.callout-note title="Python Implementation" collapse="true"}
```{python}
import numpy as np

# Covariance matrix (symmetric)
cov_matrix = np.array([
    [2.0, 1.2],
    [1.2, 1.5]
])

# Eigendecomposition
eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)

print("Covariance matrix:")
print(cov_matrix)

print("\nEigenvalues:")
print(eigenvalues)

print("\nEigenvectors (as columns):")
print(eigenvectors)

# Verify: A @ v = lambda * v
for i in range(len(eigenvalues)):
    v = eigenvectors[:, i]
    lam = eigenvalues[i]
    Av = cov_matrix @ v
    lam_v = lam * v
    print(f"\nEigenvector {i+1}: A @ v = {Av.round(4)}, lambda * v = {lam_v.round(4)}")
```
:::

---

## PCA and Dimensionality Reduction {#pca-and-dimensionality-reduction}

PCA finds the directions of maximum variance in your data.

### PCA via Covariance Matrix {#pca-covariance}

::: {.callout-note icon=false title="Formula"}
Given centered data $\mathbf{X}$ (mean-subtracted), the covariance matrix is:
$$\mathbf{S} = \frac{1}{n-1}\mathbf{X}^T\mathbf{X}$$

Eigendecomposition:
$$\mathbf{S} = \mathbf{V}\boldsymbol{\Lambda}\mathbf{V}^T$$
:::

::: {.callout-tip title="Intuition" collapse="true"}
**PCA finds:**

1. The direction of maximum variance (PC1)
2. The direction of maximum remaining variance, orthogonal to PC1 (PC2)
3. And so on...

**The eigenvectors** of the covariance matrix give these directions.
**The eigenvalues** tell you how much variance each direction captures.

**Dimensionality reduction:** Project data onto the first $k$ principal components to reduce dimensions while preserving most variance.
:::

::: {.callout-note title="Python Implementation" collapse="true"}
```{python}
import numpy as np
from sklearn.decomposition import PCA

# Example data (5 observations, 3 variables)
np.random.seed(42)
X = np.random.randn(100, 3)
X[:, 1] = X[:, 0] * 0.8 + np.random.randn(100) * 0.5  # correlated
X[:, 2] = X[:, 0] * 0.3 + np.random.randn(100) * 0.8

# Manual PCA
X_centered = X - X.mean(axis=0)
cov_matrix = np.cov(X_centered, rowvar=False)
eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)

# Sort by eigenvalue (descending)
idx = np.argsort(eigenvalues)[::-1]
eigenvalues = eigenvalues[idx]
eigenvectors = eigenvectors[:, idx]

print("Eigenvalues (variance per PC):")
print(eigenvalues)

print("\nProportion of variance:")
print(eigenvalues / eigenvalues.sum())

# Using sklearn (easier!)
pca = PCA()
pca.fit(X)
print("\nsklearn explained variance ratio:")
print(pca.explained_variance_ratio_)
```
:::

### Proportion of Variance Explained {#variance-explained}

::: {.callout-note icon=false title="Formula"}
$$\text{Proportion}_k = \frac{\lambda_k}{\sum_j \lambda_j}$$
:::

::: {.callout-tip title="Intuition" collapse="true"}
This tells you **how important each principal component is**.

**Cumulative variance:** Sum up proportions to see how many PCs you need to capture a target amount of variance (e.g., 90%).

**Scree plot:** Plot eigenvalues to find the "elbow" where adding more PCs doesn't help much.
:::

::: {.callout-note title="Python Implementation" collapse="true"}
```{python}
import numpy as np

eigenvalues = np.array([2.5, 0.8, 0.3])  # example eigenvalues

# Proportion for each PC
proportions = eigenvalues / eigenvalues.sum()

# Cumulative proportion
cumulative = np.cumsum(proportions)

print("PC\tEigenvalue\tProportion\tCumulative")
for i, (eig, prop, cum) in enumerate(zip(eigenvalues, proportions, cumulative)):
    print(f"PC{i+1}\t{eig:.2f}\t\t{prop:.3f}\t\t{cum:.3f}")

print(f"\nTo explain 90% of variance, need {np.argmax(cumulative >= 0.9) + 1} PCs")
```
:::

---

## Regularization {#regularization}

When you have many predictors or multicollinearity, regularization prevents overfitting.

### Ridge Regression {#ridge-regression}

::: {.callout-note icon=false title="Formula"}
**Optimization:**
$$\hat{\boldsymbol{\beta}}_{ridge} = \arg\min_\beta \left[\sum_i(y_i - \mathbf{x}_i^T\boldsymbol{\beta})^2 + \lambda\sum_j\beta_j^2\right]$$

**Closed form:**
$$\hat{\boldsymbol{\beta}}_{ridge} = (\mathbf{X}^T\mathbf{X} + \lambda\mathbf{I})^{-1}\mathbf{X}^T\mathbf{y}$$
:::

**Where:**

- $\lambda$ = regularization parameter (penalty strength)
- $\sum_j\beta_j^2$ = L2 penalty (squared coefficients)

::: {.callout-tip title="Intuition" collapse="true"}
Ridge regression **shrinks coefficients toward zero** by adding a penalty for large coefficients.

**Why it helps:**

- Prevents overfitting when $p$ is large relative to $n$
- Handles multicollinearity (when $\mathbf{X}^T\mathbf{X}$ is nearly singular)
- Produces more stable estimates

**The bias-variance tradeoff:**

- Ridge introduces bias (coefficients shrunk toward 0)
- But reduces variance (more stable estimates)
- Often improves prediction on new data

**Note:** Ridge never sets coefficients exactly to zero — all predictors stay in the model.
:::

::: {.callout-note title="Python Implementation" collapse="true"}
```{python}
import numpy as np
from sklearn.linear_model import Ridge

# Example data
np.random.seed(42)
n, p = 50, 10
X = np.random.randn(n, p)
y = X[:, 0] * 3 + X[:, 1] * 2 + np.random.randn(n)

# Compare OLS and Ridge
ols_beta = np.linalg.lstsq(X, y, rcond=None)[0]

# Ridge with different lambda values
for lam in [0.1, 1.0, 10.0]:
    ridge = Ridge(alpha=lam, fit_intercept=False)
    ridge.fit(X, y)

    print(f"\nlambda = {lam}")
    print(f"  Sum of squared coefficients: {np.sum(ridge.coef_**2):.3f}")
    print(f"  First 3 coefficients: {ridge.coef_[:3].round(3)}")

print("\nOLS (no penalty):")
print(f"  Sum of squared coefficients: {np.sum(ols_beta**2):.3f}")
print(f"  First 3 coefficients: {ols_beta[:3].round(3)}")
```
:::

### Lasso Regression {#lasso-regression}

::: {.callout-note icon=false title="Formula"}
$$\hat{\boldsymbol{\beta}}_{lasso} = \arg\min_\beta \left[\sum_i(y_i - \mathbf{x}_i^T\boldsymbol{\beta})^2 + \lambda\sum_j|\beta_j|\right]$$
:::

**Where:**

- $\lambda$ = regularization parameter
- $\sum_j|\beta_j|$ = L1 penalty (absolute values)

::: {.callout-tip title="Intuition" collapse="true"}
Lasso is like ridge, but uses **absolute values** instead of squares.

**Key difference:** Lasso can set coefficients **exactly to zero**, effectively doing variable selection.

**When to use:**

- You suspect only a subset of predictors matter
- You want an interpretable model with fewer variables
- Feature selection is part of your goal

**No closed form:** Unlike ridge, lasso requires iterative optimization.
:::

::: {.callout-note title="Python Implementation" collapse="true"}
```{python}
import numpy as np
from sklearn.linear_model import Lasso

# Same data as ridge example
np.random.seed(42)
n, p = 50, 10
X = np.random.randn(n, p)
y = X[:, 0] * 3 + X[:, 1] * 2 + np.random.randn(n)  # only first 2 predictors matter

# Lasso with different lambda values
for lam in [0.1, 0.5, 1.0]:
    lasso = Lasso(alpha=lam, fit_intercept=False)
    lasso.fit(X, y)

    n_nonzero = np.sum(lasso.coef_ != 0)
    print(f"\nlambda = {lam}")
    print(f"  Non-zero coefficients: {n_nonzero} of {p}")
    print(f"  Coefficients: {lasso.coef_.round(3)}")
```
:::

::: {.callout-caution title="Common Pitfalls"}
- **Forgetting to standardize**: Regularization penalizes magnitude, so predictors should be on same scale
- **Choosing lambda**: Use cross-validation to select the optimal $\lambda$
- **Ridge vs. Lasso**: Ridge when all predictors might matter; Lasso for sparse solutions
:::

---

## Alphabetical Index {#alphabetical-index}

Quick links to all formulas:

| Formula | Section |
|---------|---------|
| [Adjusted R-squared](#adjusted-r-squared) | Model Fit |
| [Bootstrap CI](#bootstrap-ci) | Resampling |
| [Confidence Intervals](#confidence-intervals) | Inference |
| [Design Matrix](#design-matrix) | GLM |
| [Eigendecomposition](#eigendecomposition) | Linear Algebra |
| [F-statistic](#f-statistic) | Model Fit |
| [GLM Equation](#glm-equation) | GLM |
| [ICC](#icc) | Mixed Models |
| [Lasso Regression](#lasso-regression) | Regularization |
| [Likelihood Ratio Test](#likelihood-ratio-test) | Model Fit |
| [Logistic Regression](#logistic-regression) | Outcome Types |
| [Matrix Operations](#matrix-operations) | Linear Algebra |
| [Odds Ratio](#odds-ratio) | Outcome Types |
| [OLS Estimator](#ols-estimator) | GLM |
| [PCA](#pca-covariance) | PCA |
| [Power](#power) | Resampling |
| [R-squared](#r-squared) | Model Fit |
| [Random Intercept Model](#random-intercept) | Mixed Models |
| [Residual Variance](#residual-variance) | GLM |
| [Residuals](#residuals) | Foundations |
| [Ridge Regression](#ridge-regression) | Regularization |
| [SE of the Mean](#standard-error-mean) | Inference |
| [Standard Errors](#standard-errors) | GLM |
| [Statistical Model](#statistical-model) | Foundations |
| [t-statistic](#t-statistic) | Inference |
| [Variance of Coefficients](#variance-of-coefficients) | GLM |
| [Variance Explained](#variance-explained) | PCA |
| [Vectors and Matrices](#vectors-matrices) | Linear Algebra |

---

::: {.callout-note}
**Last updated:** This reference is a living document and will be updated throughout the course.

Found an error or have a suggestion? Let us know!
:::
