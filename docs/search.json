[
  {
    "objectID": "weeks/final/project.html",
    "href": "weeks/final/project.html",
    "title": "Final Project",
    "section": "",
    "text": "The final project is your opportunity to apply the statistical modeling skills you‚Äôve learned throughout the course to a research question of your choice."
  },
  {
    "objectID": "weeks/final/project.html#overview",
    "href": "weeks/final/project.html#overview",
    "title": "Final Project",
    "section": "",
    "text": "The final project is your opportunity to apply the statistical modeling skills you‚Äôve learned throughout the course to a research question of your choice."
  },
  {
    "objectID": "weeks/final/project.html#timeline",
    "href": "weeks/final/project.html#timeline",
    "title": "Final Project",
    "section": "Timeline",
    "text": "Timeline\n\n\n\nDate\nMilestone\n\n\n\n\nWeek 8 (Feb 27)\nProject proposal due\n\n\nWeek 10 (Mar 13)\nDraft analysis due (optional feedback)\n\n\nFinal Exam Week (Mar 21)\nFinal project due"
  },
  {
    "objectID": "weeks/final/project.html#requirements",
    "href": "weeks/final/project.html#requirements",
    "title": "Final Project",
    "section": "Requirements",
    "text": "Requirements\n\n1. Research Question\nFormulate a clear, answerable research question that requires statistical analysis.\n\n\n2. Data\n\nUse a publicly available dataset OR\nUse data from your own research (with appropriate permissions)\nMinimum 100 observations recommended\n\n\n\n3. Analysis\nYour analysis should include:\n\nExploratory Data Analysis\n\nSummary statistics\nVisualizations\nData quality assessment\n\nStatistical Modeling\n\nAt least one model from the course (GLM, LMM, etc.)\nModel diagnostics\nModel comparison (if appropriate)\n\nInference or Prediction\n\nParameter estimates with uncertainty\nHypothesis tests or confidence intervals\nOR cross-validated prediction performance\n\nInterpretation\n\nWhat do the results mean?\nLimitations\nFuture directions\n\n\n\n\n4. Deliverables\n\nRendered .qmd file (HTML or PDF)\n\nIntroduction and research question\nMethods\nResults (with code)\nDiscussion\n\nData and code\n\nAll code should be reproducible\nInclude data or instructions for obtaining it"
  },
  {
    "objectID": "weeks/final/project.html#grading-rubric",
    "href": "weeks/final/project.html#grading-rubric",
    "title": "Final Project",
    "section": "Grading Rubric",
    "text": "Grading Rubric\n\n\n\nComponent\nPoints\n\n\n\n\nResearch question clarity\n10\n\n\nData preparation and EDA\n15\n\n\nAppropriate model choice\n20\n\n\nModel fitting and diagnostics\n20\n\n\nInterpretation and conclusions\n20\n\n\nCode quality and reproducibility\n10\n\n\nWriting and presentation\n5\n\n\nTotal\n100"
  },
  {
    "objectID": "weeks/final/project.html#proposal-template",
    "href": "weeks/final/project.html#proposal-template",
    "title": "Final Project",
    "section": "Proposal Template",
    "text": "Proposal Template\nSubmit a 1-page proposal including:\n\nResearch question (1-2 sentences)\nData source (what data, where from, sample size)\nProposed analysis (what models/methods)\nExpected challenges (what might be tricky?)"
  },
  {
    "objectID": "weeks/final/project.html#example-projects",
    "href": "weeks/final/project.html#example-projects",
    "title": "Final Project",
    "section": "Example Projects",
    "text": "Example Projects\nPast successful projects have included:\n\nPredicting movie ratings with mixed models (repeated measures per user)\nAnalyzing experimental psychology data with factorial ANOVA/LMM\nLogistic regression for predicting customer churn\nPower analysis for a planned study\nFactor analysis of survey responses"
  },
  {
    "objectID": "weeks/final/project.html#resources",
    "href": "weeks/final/project.html#resources",
    "title": "Final Project",
    "section": "Resources",
    "text": "Resources\n\nOffice hours: TBD\nCourse materials: All previous weeks\nStatistical consulting: TBD"
  },
  {
    "objectID": "weeks/final/project.html#submission",
    "href": "weeks/final/project.html#submission",
    "title": "Final Project",
    "section": "Submission",
    "text": "Submission\nSubmit via GitHub Classroom by Saturday, March 21, 2026.\nGood luck!"
  },
  {
    "objectID": "weeks/02/index.html",
    "href": "weeks/02/index.html",
    "title": "Week 2",
    "section": "",
    "text": "The two cultures of statistics\nEssence of sampling theory\nModel-based thinking",
    "crumbs": [
      "Schedule",
      "Week 2",
      "Overview"
    ]
  },
  {
    "objectID": "weeks/02/index.html#overview",
    "href": "weeks/02/index.html#overview",
    "title": "Week 2",
    "section": "",
    "text": "The two cultures of statistics\nEssence of sampling theory\nModel-based thinking",
    "crumbs": [
      "Schedule",
      "Week 2",
      "Overview"
    ]
  },
  {
    "objectID": "weeks/02/index.html#slides",
    "href": "weeks/02/index.html#slides",
    "title": "Week 2",
    "section": "Slides",
    "text": "Slides\n\n\n\n\n\n\nTipSlides Mon Jan 12th (coming soon)\n\n\n\n\n\n\n\n\n\n\n\n\nTipSlides Wed Jan 14th (coming soon)",
    "crumbs": [
      "Schedule",
      "Week 2",
      "Overview"
    ]
  },
  {
    "objectID": "weeks/02/index.html#materials",
    "href": "weeks/02/index.html#materials",
    "title": "Week 2",
    "section": "Materials",
    "text": "Materials\n\n\n\n\n\n\nCautionüìö Lab 02 - Data visualization & seaborn (plotting) intro (coming soon)",
    "crumbs": [
      "Schedule",
      "Week 2",
      "Overview"
    ]
  },
  {
    "objectID": "weeks/02/index.html#mentioned-references",
    "href": "weeks/02/index.html#mentioned-references",
    "title": "Week 2",
    "section": "Mentioned References",
    "text": "Mentioned References\ncoming soon",
    "crumbs": [
      "Schedule",
      "Week 2",
      "Overview"
    ]
  },
  {
    "objectID": "weeks/01/lab/python-fundamentals.html",
    "href": "weeks/01/lab/python-fundamentals.html",
    "title": "Python Core Concepts",
    "section": "",
    "text": "This notebook is designed to teach you the essence of Python building upon your understanding of R. We‚Äôll skip some programming basics (see the quickstart in the assignment repo for that) but add references resources to the course website. We‚Äôve structured this notebook to focus on the key bits of Python that might give you trouble coming from R and how to handle them gracefully.",
    "crumbs": [
      "Schedule",
      "Week 1",
      "Python Fundamentals"
    ]
  },
  {
    "objectID": "weeks/01/lab/python-fundamentals.html#variables-and-types",
    "href": "weeks/01/lab/python-fundamentals.html#variables-and-types",
    "title": "Python Core Concepts",
    "section": "Variables and Types",
    "text": "Variables and Types\n\n\nCode\n# We assign variables using `=`\nfirst_name = 'Eshin'\nfirst_name\n\n\n'Eshin'\n\n\n\n\nCode\n# Strings can use single or double quotes\nlast_name = \"Jolly\"\nlast_name\n\n\n'Jolly'\n\n\n\n\nCode\n# Integers\nmy_number = 3\nmy_number\n\n\n3\n\n\n\n\nCode\n# Floats contain decimal points\nmy_decimal = 3.1\nmy_decimal\n\n\n3.1\n\n\nWhat happens if you do this?\n# What happens if you do this?\nmy_variable\n= 3\n\n\n\n\n\n\nImportantSyntaxError\n\n\n\nThe most common error message you‚Äôll encounter early on. It just means you mistyped something and Python doesn‚Äôt understand it.",
    "crumbs": [
      "Schedule",
      "Week 1",
      "Python Fundamentals"
    ]
  },
  {
    "objectID": "weeks/01/lab/python-fundamentals.html#comparisons",
    "href": "weeks/01/lab/python-fundamentals.html#comparisons",
    "title": "Python Core Concepts",
    "section": "Comparisons",
    "text": "Comparisons\n\n\nCode\n# We can make comparisons\nmy_decimal &gt; my_number\n\n\nTrue\n\n\n\n\nCode\n# Not equal\nmy_decimal != my_number\n\n\nTrue\n\n\n\n\nCode\n# We can intuitively combine comparisons with `and`\nmy_number &gt; 2 and my_number &lt; 10\n\n\nTrue\n\n\n\n\nCode\n# Using `or`\nmy_number &gt; 0 or my_number &lt; 1000\n\n\nTrue\n\n\n\n\nCode\n# What happens here?\nmy_number &gt; first_name\n\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[9], line 2\n      1 # What happens here?\n----&gt; 2 my_number &gt; first_name\n\nTypeError: '&gt;' not supported between instances of 'int' and 'str'\n\n\n\n\n\n\n\n\n\nImportantTypeError\n\n\n\nAnother common message telling you that you‚Äôre not providing the expected input to the operation you‚Äôre trying. In this case Python has no way to check a number is greater than a string!",
    "crumbs": [
      "Schedule",
      "Week 1",
      "Python Fundamentals"
    ]
  },
  {
    "objectID": "weeks/01/lab/python-fundamentals.html#types-and-functions",
    "href": "weeks/01/lab/python-fundamentals.html#types-and-functions",
    "title": "Python Core Concepts",
    "section": "Types and Functions",
    "text": "Types and Functions\n\n\nCode\n# We call functions using `function(inputs)`\ntype(first_name)\n\n\nstr\n\n\n\n\nCode\n# the type function tells us what kind of object something is\n# the variable we defined above\ntype(my_number)\n\n\nint\n\n\n\n\nCode\n# Float\ntype(1.2)\n\n\nfloat\n\n\n\n\n\n\n\n\nNoteIntegers vs Floats\n\n\n\nPython like many programming languages distinguishes between numerical values that do or do not require decimal-point precision. Python will always convert to the highest precision it can for you.\n\n\n\n\nCode\n# Integer + Float = Float\ntype(my_number + my_decimal)\n\n\nfloat\n\n\n\n\nCode\n# You can always get help on variables and functions using the `help()` function\n\n# What is print?\nhelp(print)\n\n\nHelp on built-in function print in module builtins:\n\nprint(*args, sep=' ', end='\\n', file=None, flush=False)\n    Prints the values to a stream, or to sys.stdout by default.\n    \n    sep\n      string inserted between values, default a space.\n    end\n      string appended after the last value, default a newline.\n    file\n      a file-like object (stream); defaults to the current sys.stdout.\n    flush\n      whether to forcibly flush the stream.",
    "crumbs": [
      "Schedule",
      "Week 1",
      "Python Fundamentals"
    ]
  },
  {
    "objectID": "weeks/01/lab/python-fundamentals.html#lists",
    "href": "weeks/01/lab/python-fundamentals.html#lists",
    "title": "Python Core Concepts",
    "section": "Lists",
    "text": "Lists\n\n\nCode\n# We can put multiple variables in a list using square brackets `[]`\nmy_list = [first_name, last_name, 'third_name', 'fourth_name']\nmy_list\n\n\n['Eshin', 'Jolly', 'third_name', 'fourth_name']\n\n\n\n\n\n\n\n\nNoteNote: 0-based indexing\n\n\n\nNotice how the variables in the list start at 0? That‚Äôs because unlike R, Python counts starting from 0 not from 1! This is usually the first major difference to get used to and applies to all Python libraries and tools. For example, the first row of a dataframe is row 0 not row 1.\n\n\n\n\nCode\n# We can index into the list to get a single item using `[]`\nmy_list[0]\n\n\n'Eshin'\n\n\n\n\nCode\n# 2nd item\nmy_list[1]\n\n\n'Jolly'\n\n\n\n\nCode\n# We can use negative position to index items backwards\n# last item\nmy_list[-1]\n\n\n'fourth_name'\n\n\n\n\nCode\n# 2nd-to-last item\nmy_list[-2]\n\n\n'third_name'\n\n\n\n\nCode\n# What happens if we try this?\nmy_list[4]\n\n\n\n---------------------------------------------------------------------------\nIndexError                                Traceback (most recent call last)\nCell In[20], line 2\n      1 # What happens if we try this?\n----&gt; 2 my_list[4]\n\nIndexError: list index out of range\n\n\n\n\n\n\n\n\n\nImportantIndexError\n\n\n\nOne the most common error messages is just telling you‚Äôre trying to retrieve an item in a position that doesn‚Äôt exist. In other words the list has too few items and Python doesn‚Äôt know what to do.\n\n\n\n\nCode\n# We can use the `len` function to get the size of the list\nlen(my_list)\n\n\n4",
    "crumbs": [
      "Schedule",
      "Week 1",
      "Python Fundamentals"
    ]
  },
  {
    "objectID": "weeks/01/lab/python-fundamentals.html#slicing",
    "href": "weeks/01/lab/python-fundamentals.html#slicing",
    "title": "Python Core Concepts",
    "section": "Slicing",
    "text": "Slicing\n\n\nCode\n# To retrieve multiple items we can use `[start:stop]` to index the list\nmy_list[0:3]\n\n\n['Eshin', 'Jolly', 'third_name']\n\n\n\n\n\n\n\n\nNoteNote: Slicing doesn‚Äôt include stop\n\n\n\nBy default Python slices up-to but not-including the stop value. Notice how the 3rd index (\"fourth name\") was not included even though we used 3.\n\n\n\n\nCode\n# Leaving off the `start` or `stop` will get all items from-the-start or until-the-end\n# from-the-start\nmy_list[:3]\n\n\n['Eshin', 'Jolly', 'third_name']\n\n\n\n\nCode\n# until-the-end\nmy_list[1:]\n\n\n['Jolly', 'third_name', 'fourth_name']\n\n\n\n\nCode\n# We can optionally control `step` size using a third value\n# from-the-start -&gt; 3rd index -&gt; by two (every other)\nmy_list[0:3:2]\n\n\n['Eshin', 'third_name']\n\n\n\n\nCode\n# If we use a negative `step` we can slice backwards\n# from-the-start -&gt; until-the-end -&gt; backwards\nmy_list[::-1]\n\n\n['fourth_name', 'third_name', 'Jolly', 'Eshin']\n\n\n\n\n\n\n\n\nTip\n\n\n\nUsing list[::-1] is a very common pattern for quickly reversing a list in Python",
    "crumbs": [
      "Schedule",
      "Week 1",
      "Python Fundamentals"
    ]
  },
  {
    "objectID": "weeks/01/lab/python-fundamentals.html#control-flow",
    "href": "weeks/01/lab/python-fundamentals.html#control-flow",
    "title": "Python Core Concepts",
    "section": "Control Flow",
    "text": "Control Flow\n\n\nCode\n# We use indentation and `:` to create blocks of logic (control flow)\nif my_number &gt; 0:\n    print(\"Greater than 0\")\n\n\nGreater than 0\n\n\n\n\n\n\n\n\nNoteNote: Indentation\n\n\n\nPython is often loved for being very readable in part because it doesn‚Äôt use {} to surround code-block like R, Javascript and other languages. However, that means you need to carefully indent or de-indent to accomplish the same thing.\n\n\n\n\nCode\n# We can create branches of logic using indentation with `if/else` and `elif`\nif my_number &gt; 0:\n    print(\"Greater than 0\")\nelse:\n    print(\"Less than 0\")\n\n\nGreater than 0\n\n\nWhat happens here?\n# What happens here?\nif my_number &gt; 0:\nprint(\"Greater than 0\")\n\n\n\n\n\n\nImportantIndentationError\n\n\n\nPython will let you know if your spacing is off and where it‚Äôs happening. You‚Äôll mostly encounter this when you‚Äôre editing code, because VSCode will try to be helpful and automatically indent correctly as you‚Äôre writing code.\n\n\n\n\nCode\n# We can keep branching with `elif`\nif my_number &lt; 0:\n    print(\"Less that 0\")\nelif 4 &gt; my_number &gt; 0: # notice how we can express this like in English\n    print(\"Between 4 and 0\")\nelse:\n    print(\"Very large\")\n\n\nBetween 4 and 0",
    "crumbs": [
      "Schedule",
      "Week 1",
      "Python Fundamentals"
    ]
  },
  {
    "objectID": "weeks/01/lab/python-fundamentals.html#loops",
    "href": "weeks/01/lab/python-fundamentals.html#loops",
    "title": "Python Core Concepts",
    "section": "Loops",
    "text": "Loops\n\n\nCode\n# We can loop in the same way using `for`, indentation and `:`\nfor elem in my_list:\n    # Everything indented at this level happens for EACH item\n    print(elem)\n\n\nEshin\nJolly\nthird_name\nfourth_name\n\n\n\n\nCode\n# The name of the looping variable is arbitrary. Using `elem` is just a convention\nfor boogity_bop in my_list:\n    print(boogity_bop)\n\n\nEshin\nJolly\nthird_name\nfourth_name\n\n\n\n\nCode\n# To operate on each item AND its position/index we use the `enumerate()` function\nhelp(enumerate)\n\n\nHelp on class enumerate in module builtins:\n\nclass enumerate(object)\n |  enumerate(iterable, start=0)\n |  \n |  Return an enumerate object.\n |  \n |    iterable\n |      an object supporting iteration\n |  \n |  The enumerate object yields pairs containing a count (from start, which\n |  defaults to zero) and a value yielded by the iterable argument.\n |  \n |  enumerate is useful for obtaining an indexed list:\n |      (0, seq[0]), (1, seq[1]), (2, seq[2]), ...\n |  \n |  Methods defined here:\n |  \n |  __getattribute__(self, name, /)\n |      Return getattr(self, name).\n |  \n |  __iter__(self, /)\n |      Implement iter(self).\n |  \n |  __next__(self, /)\n |      Implement next(self).\n |  \n |  __reduce__(...)\n |      Return state information for pickling.\n |  \n |  ----------------------------------------------------------------------\n |  Class methods defined here:\n |  \n |  __class_getitem__(...)\n |      See PEP 585\n |  \n |  ----------------------------------------------------------------------\n |  Static methods defined here:\n |  \n |  __new__(*args, **kwargs)\n |      Create and return a new object.  See help(type) for accurate signature.\n\n\n\n\n\n\n\n\n\nTipYour Turn\n\n\n\nTry using help() and the examples above to figure out how to use the enumerate() function to print out each item and its position. If the notebook gives you an error about reusing a variable name (e.g.¬†elem) just call your looping variable something else.\n\n\n\n\nCode\n# Your code below\nfor ...\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\n\nCode\nfor idx, item in enumerate(my_list):\n    print(f\"Position {idx}: {item}\")\n\n\nPosition 0: Eshin\nPosition 1: Jolly\nPosition 2: third_name\nPosition 3: fourth_name",
    "crumbs": [
      "Schedule",
      "Week 1",
      "Python Fundamentals"
    ]
  },
  {
    "objectID": "weeks/01/lab/python-fundamentals.html#functions",
    "href": "weeks/01/lab/python-fundamentals.html#functions",
    "title": "Python Core Concepts",
    "section": "Functions",
    "text": "Functions\n\n\n\n\n\n\nNoteCreating functions\n\n\n\nPython makes it easy to write your own functions to create usable blocks of code using the def keyword (not function like in R). Then we just use indentation like before:\ndef myfunction(first_argument, second_argument...):\n  # Everything indented is inside the function\n  print(\"I'm calculating...\")\n  output = first_argument + second_argument\n  # Optionally return something\n  return output\n\n\n\n\nCode\n# Running this code cell defines the function for use anywhere in the notebook\n\ndef myfunction(first_argument, second_argument):\n    \"\"\"This is optional documentation string for function help\"\"\"\n\n    print(\"I'm calculating...\")\n    output = first_argument + second_argument\n    return output\n\n\n\n\nCode\n# Now lets use it like any other function\nmyfunction(1, 2)\n\n\nI'm calculating...\n\n\n3\n\n\n\n\nCode\nmyfunction(4, 5)\n\n\nI'm calculating...\n\n\n9\n\n\n\n\nCode\n# We can even get help on our function\nhelp(myfunction)\n\n\nHelp on function myfunction in module __main__:\n\nmyfunction(first_argument, second_argument)\n    This is optional documentation string for function help",
    "crumbs": [
      "Schedule",
      "Week 1",
      "Python Fundamentals"
    ]
  },
  {
    "objectID": "weeks/01/lab/python-fundamentals.html#methods",
    "href": "weeks/01/lab/python-fundamentals.html#methods",
    "title": "Python Core Concepts",
    "section": "Methods",
    "text": "Methods\n\n\nCode\n# Unlike R sometimes we use \"functions\" attached to objects with the `.` syntax\nfirst_name.upper()\n\n\n'ESHIN'\n\n\n\n\n\n\n\n\nNoteMethods are functions attached to objects called with .\n\n\n\nUnlike R, Python is an object-oriented-language which means functions can be attached to objects.\nWe call these methods but you can intuitively treat them the same.\nIn the example above, Python doesn‚Äôt have an upper() function, but strings have a .upper() method. In your head when you see first_name.upper() just think upper(first_name).\nThis allows for method-chaining which is Python‚Äôs alternative to R‚Äôs %&gt;% syntax.\nIn R we might do: function() %&gt;% function() %&gt;% function()\nIn Python we‚Äôll often do: object.method().method().method() to achieve the same effect.\n\n\n\n\nCode\n# This is a method-chain\nfirst_name.upper().lower()\n\n\n'eshin'\n\n\n\n\nCode\n# We can use the `dir()` function to see all the methods that belong to an object\n# Since our variable is a list this show all list methods\ndir(my_list)\n\n\n['__add__',\n '__class__',\n '__class_getitem__',\n '__contains__',\n '__delattr__',\n '__delitem__',\n '__dir__',\n '__doc__',\n '__eq__',\n '__format__',\n '__ge__',\n '__getattribute__',\n '__getitem__',\n '__getstate__',\n '__gt__',\n '__hash__',\n '__iadd__',\n '__imul__',\n '__init__',\n '__init_subclass__',\n '__iter__',\n '__le__',\n '__len__',\n '__lt__',\n '__mul__',\n '__ne__',\n '__new__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__reversed__',\n '__rmul__',\n '__setattr__',\n '__setitem__',\n '__sizeof__',\n '__str__',\n '__subclasshook__',\n 'append',\n 'clear',\n 'copy',\n 'count',\n 'extend',\n 'index',\n 'insert',\n 'pop',\n 'remove',\n 'reverse',\n 'sort']\n\n\n\n\nCode\n# Using the `.append()` method\nmy_list.append(\"another_item\")\n\n\n\n\n\n\n\n\nImportantNot all methods are chainable\n\n\n\nNotice how .append() didn‚Äôt return anything? Some methods cannot be chained because they modify the object in-place\nRun the cell below to see how the value of the variable my_list has changed Then run the cell below that to .append() a second time and see what happens\n\n\n\n\nCode\n# my_list was updated in place\nprint(f\"There are {len(my_list)} items:\\n{my_list}\")\n\n\nThere are 5 items:\n['Eshin', 'Jolly', 'third_name', 'fourth_name', 'another_item']\n\n\n\n\nCode\n# Let's append again\nmy_list.append(\"add_another\")\n\n\n\n\nCode\n# Now what does it show?\nprint(f\"There are {len(my_list)} items:\\n{my_list}\")\n\n\nThere are 6 items:\n['Eshin', 'Jolly', 'third_name', 'fourth_name', 'another_item', 'add_another']",
    "crumbs": [
      "Schedule",
      "Week 1",
      "Python Fundamentals"
    ]
  },
  {
    "objectID": "weeks/01/lab/python-fundamentals.html#importing-libraries",
    "href": "weeks/01/lab/python-fundamentals.html#importing-libraries",
    "title": "Python Core Concepts",
    "section": "Importing Libraries",
    "text": "Importing Libraries\n\n\nCode\n# We use the `import` keyword to bring in functionality from other libraries\nimport polars\n\n# Use something from the module with `.`\nmy_empty_dataframe = polars.DataFrame()\nmy_empty_dataframe\n\n\n\nshape: (0, 0)\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteImporting libraries with import and as\n\n\n\nWhereas in R you might use library(lme4) to import a library and automatically get all it‚Äôs functions (e.g.¬†lmer), in Python you have to be more explicit. This is because in Python everything is an object including other libraries, which means you can do accidental things like overwrite a library you imported with a variable:\n# Import the library\nimport mylibrary\n\n# Use it\nmylibrary.myfunction()\n\n# Oops Python will let you do this but DONT\nmylibrary = \"Eshin\"\n\n# This doesn't work anymore!\nmylibrary.myfunction()\n\n\n\n\nCode\n# We use typically using `as` to shorten common library names by convention\nimport polars as pl\n\n# Less typing, fewer mistakes!\nnew_df = pl.DataFrame()\n\n# Show it\nnew_df\n\n\n\nshape: (0, 0)\n\n\n\n\n\n\n\n\n\nCode\n# Or to just import specific functionality\nfrom polars import DataFrame\n\n# Use it\nanother_df = DataFrame()\n\n# Show it\nanother_df\n\n\n\nshape: (0, 0)\n\n\n\n\n\n\n\n\n\nCode\n# Here's a convention Eshin likes, but make sure to never create a variable called `c`\n# (you shouldn't be doing that anyway)\nfrom polars import col as c\n\nhelp(c)\n\n\nHelp on Col in module polars.functions.col:\n\n&lt;Expr ['col(\"__origin__\")']&gt;",
    "crumbs": [
      "Schedule",
      "Week 1",
      "Python Fundamentals"
    ]
  },
  {
    "objectID": "weeks/01/lab/python-fundamentals.html#pro-tips",
    "href": "weeks/01/lab/python-fundamentals.html#pro-tips",
    "title": "Python Core Concepts",
    "section": "Pro-tips",
    "text": "Pro-tips\n\nReference help docs often\nChange-and-rerun often\nDon‚Äôt reuse variable names (the notebook won‚Äôt let you!)",
    "crumbs": [
      "Schedule",
      "Week 1",
      "Python Fundamentals"
    ]
  },
  {
    "objectID": "weeks/01/lab/index.html",
    "href": "weeks/01/lab/index.html",
    "title": "Lab ‚ÄúPre-flight‚Äù Setup",
    "section": "",
    "text": "NoteGoals\n\n\n\n\nSetup your coding tools\nLearn the GitHub Classroom assignment workflow\nAccept & pull Lab 01 to your laptop\n\n\n\nFirst we‚Äôll focus on getting your personal computer ready for the rest of the course. We‚Äôll be making using of the macOS An application for controlling your computer via commands that you type in (e.g.¬†cd, ls) instead of point-and-clickterminal for the first section. Don‚Äôt worry if you‚Äôve never used it before or have limited experience. We‚Äôve written the instructions below so you can follow along step-by-step and just copy and paste the commands into your terminal to avoid typos.\n\n\n\n\n\n\nTip\n\n\n\nYou can click and hold any linked words in the text on this page to get a definition. All definitions are available on the terminology page.\n\n\n\n\nFirst we‚Äôll install the A command-line package manager for macOS that lets you install packages and applications using the brew commandHomebrew package manager for macOS. You can think of this as an ‚ÄúApp store‚Äù for programs we‚Äôll run from our macOS An application for controlling your computer via commands that you type in (e.g.¬†cd, ls) instead of point-and-clickTerminal.\nStart by copying and paste the following command into a new macOS An application for controlling your computer via commands that you type in (e.g.¬†cd, ls) instead of point-and-clickTerminal\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n\n\n\n\n\n\nNote\n\n\n\nThe ‚Äúprogram‚Äù that‚Äôs running when you first launch your terminal is called a A program that runs automatically when your Terminal starts and interprets the commands you type to control your computer instead of pointing-and-clicking. FYI: the default shell on macOS is zsh not bash.shell. You can think of it like console in RStudio, but for running system commands like cd, pwd, ls, etc.\n\n\nAfter some long messages and setup you should have the brew A program that you interact with exclusively from your terminal; often abbreviated as CLIcommand-line-program (CLI) available.\n\n\n\n\n\n\nTip\n\n\n\nYou can check by typing which brew and seeing if you get any output\nIn general the which command will tell you where a CLI tool is installed if it‚Äôs installed; no output means it doesn‚Äôt exist!\n\n\nWe can use this to brew install/update/remove/list/search a variety of tools.\nLet‚Äôs use it to make sure you have a few other tools setup for the course:\n\n\nBuilding off of 201A, you‚Äôll use a scientific publishing tool that allows you to mix prose and code-cells to render executable documents in a variety of formats (website, PDF, etc)Quarto to author all your assignment submissions (labs, HWs, final project). While you can install it from the official website it‚Äôs easier to get from brew\n\n\n\n\n\n\nNote\n\n\n\nTry which quarto first to see if it‚Äôs already installed\nIf so, feel free to skip the next command\n\n\nbrew install --cask quarto\n\n\n\n\n\n\nNote\n\n\n\nThe --cask flag to brew is sometimes needed when installing specific libraries and applications. This makes it possible to installed full GUI applications in addition to CLI ones (e.g.¬†like VSCode) all from brew! But don‚Äôt worry about remembering whether you need to use it or not. brew will helpfully complain if you do.\n\n\n\n\n\nTo keep all our work reproducible and easy to collaborate, on we‚Äôll use A library and environment manager for Python making it easy to create/add/update additional Python libraries & tools in a reproducible and isolated way. using a pyproject.toml ‚Äúblueprints‚Äù fileuv to manage our Python Really just a hidden folder on your computer (typically .venv/) that contains an isolated Python installation with all additional libraries and tools. uv handles this all for us!environment. This makes it effortless to add/update/remove any additional Python libraries in an isolated, project-specific way.\nbrew install uv\n\n\n\nLastly we‚Äôll want to make sure we have the latest tools to interact with A CLI to version control LOCAL files and folders called repositories. See the git guide for more details and a command cheatsheet.Git and A online cloud-based service for synchronize local git repositories with REMOTE repositories on github.com. This faciliates collaborative works flows and open-source development. We‚Äôre using the Github Classroom feature built-up on this for our course.Github.\nbrew install git gh\nFirst we‚Äôll want to make sure the local git CLI program knows who we are. Run the following commands in your terminal (you won‚Äôt see any output):\ngit config --global user.name \"your name\"\ngit config --global user.email \"email associated with your github account\"\nWe‚Äôll also set a few other git options to avoid future headaches:\ngit config --global pull.rebase true\nAnd\ngit config --global rebase.autoStash true\nNow we can login to A online cloud-based service for synchronize local git repositories with REMOTE repositories on github.com. This faciliates collaborative works flows and open-source development. We‚Äôre using the Github Classroom feature built-up on this for our course.Github from our computers to ensure that all our future work is associated with the same account:\ngh auth login\nThen you can answer the prompts with the following answers:\nWhere do you use GitHub? GitHub.com\nWhat is your preferred protocol for Git operations on this host? HTTPS\nAuthenticate Git with your GitHub credentials? Yes\nHow would you like to authenticate GitHub CLI? Login with a web browser\nYou should see the following prompt with a unique code for you. Make sure to copy it and then press &lt;enter&gt;\nFirst copy your one-time code: 4722-D256\nPress Enter to open https://github.com/login/device in your browser...\nCopy and paste the code into the browser page and press the green button to approve. When you‚Äôre all set you‚Äôll get the following output with your Your user ‚Äúhandle‚Äù on github.com that identifies you when using git commands (local) and interacting with Github.com (remote). E.g. Eshin‚Äôs is @ejollyGithub userid:\n‚úì Authentication complete.\n- gh config set -h github.com git_protocol https\n‚úì Configured git protocol\n‚úì Logged in as ejolly\nYou can verify your local setup by running git config --list and looking at the output for some sanity checks like your Github username & email:\nuser.name=ejolly\nuser.email=eshin.jolly@gmail.com\npull.ff=false\npull.rebase=false\n\n\n\nFinally let‚Äôs setup our Integrated-development-environment; a program that includes a code-editor, terminal/console, and other helpful features to be a ‚Äúone-stop-shop‚Äù for most of your needsintegrated-development-environment (IDE)\n\nVSCode (recommended)RStudio (advanced)\n\n\nIf you‚Äôve already installed An extremely popular general purpose IDE that supports multiple language (e.g.¬†Python, R, Javascript) and makes use of extensions to add additional functionality (e.g.¬†quarto rendering, Python notebooks).Visual Studio Code before you can skip the next command. Otherwise, copy and paste the following into your terminal:\nbrew install --cask visual-studio-code\nYou can then launch VSCode like any other program on your computer. We‚Äôll orient to the interface in Lab 01.\n\n\nWhile we‚Äôve mostly built this course around VSCode to provide a consistent experience, you can continue using An IDE originally designed to work with R, but also works well with Quarto documents. Also includes a Terminal separate from the R console for running shell commands.RStudio if you‚Äôre an advanced user. When working with .qmd files, you can use familiar buttons to render Quarto documents and RStudio will handle running the Python code-chunks for you.\nHowever, you will not be able to run .py files that we can work with interactively using code-cells and markdown-cells (similar to quarto chunks). A much richer interface for interactively working with pieces of code one-at-a-timeinteractive Python notebooks (.py) files that we provide. Instead, you‚Äôll need to use the integrated Terminal (not the R console!) to run some commands to launch them (we‚Äôll cover this in the Lab 01 assignment later)\n\n\n\n\n\n\n\nNow that you‚Äôre setup with Github let‚Äôs go over the main workflow you‚Äôll regulary use when working on course materials\n\n\n\n\n\n\nTip\n\n\n\nThese steps are also available for quick future reference in the dedicated github classroom guide linked in the top navigation bar\n\n\n\n\n\nClick any course website link that starts with üìö.\nAccept the assignment in your browser. This will create a copy (fork) of the assignment under your own github account\nClick the URL to go the auto-created github repo. This will always be named assignment-name-your-githubid\nClone it to your local computer: git clone REPO-URL You can get the REPO-URL by clicking the green code button on github\nMove into the folder: cd folder-you-cloned\nSetup the Python environment: uv sync && uv run poe setup\nOpen the project in VSCode (or RStudio)\n\n\n\n\nSubmitting an assignment is as easy as pushing your changes to github. We‚Äôll automatically be able to see when you submit, run automatic checks, etc.\n\nCommit your changes locally. Using the VSCode UI or terminal commands git commit -am \"my message\"\nPush your changes to github: git push\n\nThere are no restrictions on how often or the final deadline to git commit and git push your assignments! For any deadlines we announce, you‚Äôll just want to make sure to make the final push you want us to review by the deadline. Later, once we review assignments together in class, you can continue using commit and pushto update your assignments with corrections, notes, etc for updated grading!\n\n\n\nOften we‚Äôll add new files (e.g.¬†solutions) or make corrections to an assignment and we‚Äôll ask you to update your repository after you‚Äôve already run git clone and maybe even git commit and git push. Here‚Äôs how you can do that:\n\nOpen the assignment repository on github.com You can either find the original üìö link OR cd into the folder and run git remote -v to print out the URL\nGo to the ‚ÄúPull Request‚Äù‚Äù page\nChoose the PR called ‚ÄúGitHub Classroom: Sync Assignment‚Äù\nClick the green ‚ÄúMerge pull request‚Äù button near the bottom\nBack on your computer verify you‚Äôve committed any work-in-progress. If you run git status and you don‚Äôt see ‚Äúnothing to commit, working tree clean‚Äù you‚Äôll need to run git commit -am \"my message\" first.\nSync the merged PR to your computer: git pull\n\nNow any files you had open in VSCode will automatically refresh to the latest versions and any new files we be available for editing!\n\n\n\n\n\n\nTip\n\n\n\nWe‚Äôll also use the ‚ÄúPull Requests‚Äù tab to create a ‚ÄúFeedback‚Äù PR.\nYou don‚Äôt need to merge this in. Instead, think of it as an on-going discussion between you and your instructors & peers where you reference specific files and lines of your project.\n\n\n\n\n\n\nNow that you have the basics configured, let‚Äôs try this out to get setup with the first lab:\n\n\n\n\n\n\nCautionüìö Lab 01\n\n\n\nGithub Classroom Assignment\n\n\nOnce you‚Äôve cloned the assignment to your computer you can start going through tutorials in this order:\n\nREADME.md - VSCode introduction and configuration\nindex.qmd - Quarto & Python intro\nnotebooks/python-quickstart.py - Python fundamentals\n\n\n\n\n\n\n\nWarning\n\n\n\nIf the link above does not work for you please send your Your user ‚Äúhandle‚Äù on github.com that identifies you when using git commands (local) and interacting with Github.com (remote). E.g. Eshin‚Äôs is @ejollygithub-userid to Eshin on Slack so he can add you to the Github classroom!",
    "crumbs": [
      "Schedule",
      "Week 1",
      "\"Pre-flight\" Lab Setup"
    ]
  },
  {
    "objectID": "weeks/01/lab/index.html#computer-setup",
    "href": "weeks/01/lab/index.html#computer-setup",
    "title": "Lab ‚ÄúPre-flight‚Äù Setup",
    "section": "",
    "text": "First we‚Äôll install the A command-line package manager for macOS that lets you install packages and applications using the brew commandHomebrew package manager for macOS. You can think of this as an ‚ÄúApp store‚Äù for programs we‚Äôll run from our macOS An application for controlling your computer via commands that you type in (e.g.¬†cd, ls) instead of point-and-clickTerminal.\nStart by copying and paste the following command into a new macOS An application for controlling your computer via commands that you type in (e.g.¬†cd, ls) instead of point-and-clickTerminal\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n\n\n\n\n\n\nNote\n\n\n\nThe ‚Äúprogram‚Äù that‚Äôs running when you first launch your terminal is called a A program that runs automatically when your Terminal starts and interprets the commands you type to control your computer instead of pointing-and-clicking. FYI: the default shell on macOS is zsh not bash.shell. You can think of it like console in RStudio, but for running system commands like cd, pwd, ls, etc.\n\n\nAfter some long messages and setup you should have the brew A program that you interact with exclusively from your terminal; often abbreviated as CLIcommand-line-program (CLI) available.\n\n\n\n\n\n\nTip\n\n\n\nYou can check by typing which brew and seeing if you get any output\nIn general the which command will tell you where a CLI tool is installed if it‚Äôs installed; no output means it doesn‚Äôt exist!\n\n\nWe can use this to brew install/update/remove/list/search a variety of tools.\nLet‚Äôs use it to make sure you have a few other tools setup for the course:\n\n\nBuilding off of 201A, you‚Äôll use a scientific publishing tool that allows you to mix prose and code-cells to render executable documents in a variety of formats (website, PDF, etc)Quarto to author all your assignment submissions (labs, HWs, final project). While you can install it from the official website it‚Äôs easier to get from brew\n\n\n\n\n\n\nNote\n\n\n\nTry which quarto first to see if it‚Äôs already installed\nIf so, feel free to skip the next command\n\n\nbrew install --cask quarto\n\n\n\n\n\n\nNote\n\n\n\nThe --cask flag to brew is sometimes needed when installing specific libraries and applications. This makes it possible to installed full GUI applications in addition to CLI ones (e.g.¬†like VSCode) all from brew! But don‚Äôt worry about remembering whether you need to use it or not. brew will helpfully complain if you do.\n\n\n\n\n\nTo keep all our work reproducible and easy to collaborate, on we‚Äôll use A library and environment manager for Python making it easy to create/add/update additional Python libraries & tools in a reproducible and isolated way. using a pyproject.toml ‚Äúblueprints‚Äù fileuv to manage our Python Really just a hidden folder on your computer (typically .venv/) that contains an isolated Python installation with all additional libraries and tools. uv handles this all for us!environment. This makes it effortless to add/update/remove any additional Python libraries in an isolated, project-specific way.\nbrew install uv\n\n\n\nLastly we‚Äôll want to make sure we have the latest tools to interact with A CLI to version control LOCAL files and folders called repositories. See the git guide for more details and a command cheatsheet.Git and A online cloud-based service for synchronize local git repositories with REMOTE repositories on github.com. This faciliates collaborative works flows and open-source development. We‚Äôre using the Github Classroom feature built-up on this for our course.Github.\nbrew install git gh\nFirst we‚Äôll want to make sure the local git CLI program knows who we are. Run the following commands in your terminal (you won‚Äôt see any output):\ngit config --global user.name \"your name\"\ngit config --global user.email \"email associated with your github account\"\nWe‚Äôll also set a few other git options to avoid future headaches:\ngit config --global pull.rebase true\nAnd\ngit config --global rebase.autoStash true\nNow we can login to A online cloud-based service for synchronize local git repositories with REMOTE repositories on github.com. This faciliates collaborative works flows and open-source development. We‚Äôre using the Github Classroom feature built-up on this for our course.Github from our computers to ensure that all our future work is associated with the same account:\ngh auth login\nThen you can answer the prompts with the following answers:\nWhere do you use GitHub? GitHub.com\nWhat is your preferred protocol for Git operations on this host? HTTPS\nAuthenticate Git with your GitHub credentials? Yes\nHow would you like to authenticate GitHub CLI? Login with a web browser\nYou should see the following prompt with a unique code for you. Make sure to copy it and then press &lt;enter&gt;\nFirst copy your one-time code: 4722-D256\nPress Enter to open https://github.com/login/device in your browser...\nCopy and paste the code into the browser page and press the green button to approve. When you‚Äôre all set you‚Äôll get the following output with your Your user ‚Äúhandle‚Äù on github.com that identifies you when using git commands (local) and interacting with Github.com (remote). E.g. Eshin‚Äôs is @ejollyGithub userid:\n‚úì Authentication complete.\n- gh config set -h github.com git_protocol https\n‚úì Configured git protocol\n‚úì Logged in as ejolly\nYou can verify your local setup by running git config --list and looking at the output for some sanity checks like your Github username & email:\nuser.name=ejolly\nuser.email=eshin.jolly@gmail.com\npull.ff=false\npull.rebase=false\n\n\n\nFinally let‚Äôs setup our Integrated-development-environment; a program that includes a code-editor, terminal/console, and other helpful features to be a ‚Äúone-stop-shop‚Äù for most of your needsintegrated-development-environment (IDE)\n\nVSCode (recommended)RStudio (advanced)\n\n\nIf you‚Äôve already installed An extremely popular general purpose IDE that supports multiple language (e.g.¬†Python, R, Javascript) and makes use of extensions to add additional functionality (e.g.¬†quarto rendering, Python notebooks).Visual Studio Code before you can skip the next command. Otherwise, copy and paste the following into your terminal:\nbrew install --cask visual-studio-code\nYou can then launch VSCode like any other program on your computer. We‚Äôll orient to the interface in Lab 01.\n\n\nWhile we‚Äôve mostly built this course around VSCode to provide a consistent experience, you can continue using An IDE originally designed to work with R, but also works well with Quarto documents. Also includes a Terminal separate from the R console for running shell commands.RStudio if you‚Äôre an advanced user. When working with .qmd files, you can use familiar buttons to render Quarto documents and RStudio will handle running the Python code-chunks for you.\nHowever, you will not be able to run .py files that we can work with interactively using code-cells and markdown-cells (similar to quarto chunks). A much richer interface for interactively working with pieces of code one-at-a-timeinteractive Python notebooks (.py) files that we provide. Instead, you‚Äôll need to use the integrated Terminal (not the R console!) to run some commands to launch them (we‚Äôll cover this in the Lab 01 assignment later)",
    "crumbs": [
      "Schedule",
      "Week 1",
      "\"Pre-flight\" Lab Setup"
    ]
  },
  {
    "objectID": "weeks/01/lab/index.html#github-classroom-workflows",
    "href": "weeks/01/lab/index.html#github-classroom-workflows",
    "title": "Lab ‚ÄúPre-flight‚Äù Setup",
    "section": "",
    "text": "Now that you‚Äôre setup with Github let‚Äôs go over the main workflow you‚Äôll regulary use when working on course materials\n\n\n\n\n\n\nTip\n\n\n\nThese steps are also available for quick future reference in the dedicated github classroom guide linked in the top navigation bar\n\n\n\n\n\nClick any course website link that starts with üìö.\nAccept the assignment in your browser. This will create a copy (fork) of the assignment under your own github account\nClick the URL to go the auto-created github repo. This will always be named assignment-name-your-githubid\nClone it to your local computer: git clone REPO-URL You can get the REPO-URL by clicking the green code button on github\nMove into the folder: cd folder-you-cloned\nSetup the Python environment: uv sync && uv run poe setup\nOpen the project in VSCode (or RStudio)\n\n\n\n\nSubmitting an assignment is as easy as pushing your changes to github. We‚Äôll automatically be able to see when you submit, run automatic checks, etc.\n\nCommit your changes locally. Using the VSCode UI or terminal commands git commit -am \"my message\"\nPush your changes to github: git push\n\nThere are no restrictions on how often or the final deadline to git commit and git push your assignments! For any deadlines we announce, you‚Äôll just want to make sure to make the final push you want us to review by the deadline. Later, once we review assignments together in class, you can continue using commit and pushto update your assignments with corrections, notes, etc for updated grading!\n\n\n\nOften we‚Äôll add new files (e.g.¬†solutions) or make corrections to an assignment and we‚Äôll ask you to update your repository after you‚Äôve already run git clone and maybe even git commit and git push. Here‚Äôs how you can do that:\n\nOpen the assignment repository on github.com You can either find the original üìö link OR cd into the folder and run git remote -v to print out the URL\nGo to the ‚ÄúPull Request‚Äù‚Äù page\nChoose the PR called ‚ÄúGitHub Classroom: Sync Assignment‚Äù\nClick the green ‚ÄúMerge pull request‚Äù button near the bottom\nBack on your computer verify you‚Äôve committed any work-in-progress. If you run git status and you don‚Äôt see ‚Äúnothing to commit, working tree clean‚Äù you‚Äôll need to run git commit -am \"my message\" first.\nSync the merged PR to your computer: git pull\n\nNow any files you had open in VSCode will automatically refresh to the latest versions and any new files we be available for editing!\n\n\n\n\n\n\nTip\n\n\n\nWe‚Äôll also use the ‚ÄúPull Requests‚Äù tab to create a ‚ÄúFeedback‚Äù PR.\nYou don‚Äôt need to merge this in. Instead, think of it as an on-going discussion between you and your instructors & peers where you reference specific files and lines of your project.",
    "crumbs": [
      "Schedule",
      "Week 1",
      "\"Pre-flight\" Lab Setup"
    ]
  },
  {
    "objectID": "weeks/01/lab/index.html#putting-it-all-together-get-lab-01",
    "href": "weeks/01/lab/index.html#putting-it-all-together-get-lab-01",
    "title": "Lab ‚ÄúPre-flight‚Äù Setup",
    "section": "",
    "text": "Now that you have the basics configured, let‚Äôs try this out to get setup with the first lab:\n\n\n\n\n\n\nCautionüìö Lab 01\n\n\n\nGithub Classroom Assignment\n\n\nOnce you‚Äôve cloned the assignment to your computer you can start going through tutorials in this order:\n\nREADME.md - VSCode introduction and configuration\nindex.qmd - Quarto & Python intro\nnotebooks/python-quickstart.py - Python fundamentals\n\n\n\n\n\n\n\nWarning\n\n\n\nIf the link above does not work for you please send your Your user ‚Äúhandle‚Äù on github.com that identifies you when using git commands (local) and interacting with Github.com (remote). E.g. Eshin‚Äôs is @ejollygithub-userid to Eshin on Slack so he can add you to the Github classroom!",
    "crumbs": [
      "Schedule",
      "Week 1",
      "\"Pre-flight\" Lab Setup"
    ]
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Note\n\n\n\nWe‚Äôve marked future assignments as TBD to allow us to adjust the pacing as needed.\nIn total, we‚Äôll aim to cover in total: ~4-5 HWs and ~6-7 labs\n\n\n\n\n\n\nWeek\nDate\nContent\nAssignment Due?\n\n\n\n\n1\nMon Jan 5\nCourse Intro\nN/A\n\n\n1\nTues Jan 6\nLaptop setup, Github Clasroom workflow, quarto, Python notebooks\npush lab 01 at least once\n\n\n1\nWed Jan 6\nPython essentials, dataframes, meet polars\npush updated lab 01 at least once\n\n\n\n\n\nTentative schedule\n\n\n\n\n\n\n\n\nWeek\nDate\nContent\nAssignment Due?\n\n\n\n\n2\nMon Jan 12\nTwo cultures of statistics, sampling theory\nskim readings for Wed Jan 14\n\n\n2\nTues Jan 13\nData visualization, meet seaborn & matplotlib\npush lab 02 at least once\n\n\n2\nWed Jan 14\nWhat is a model? Model-based thinking\npush HW 1 by Tues Jan 20\n\n\n3\nMon Jan 19\nNo class holiday\n-\n\n\n3\nTues Jan 20\nModel comparison, hw 1 review\nTBD\n\n\n3\nWed Jan 21\nGLM I: Foundations & OLS\nTBD\n\n\n4\nMon Jan 26\nGLM II: predictor types & design-matrices\nTBD\n\n\n4\nTues Jan 27\nModel formulas & designs, meet bossanova\nTBD\n\n\n4\nWed Jan 28\nGLM III: categorical predictors\nTBD\n\n\n5\nMon Feb 2\nMarginal effects estimation & prediction\nTBD\n\n\n5\nTues Feb 3\nParameter inference & uncertainty\nTBD\n\n\n5\nWed Feb 4\nOutcomes types: classifications & counts\nTBD\n\n\n6\nMon Feb 9\nResampling I: bootstrapping & permuting\nTBD\n\n\n6\nTues Feb 10\nResampling II: power & simulation, meet numpy & scipy\nTBD\n\n\n6\nWed Feb 11\nResampling III: generalization & cross-validation\nTBD\n\n\n7\nMon Feb 16\nNo class holiday\n-\n\n\n7\nTues Feb 17\nLMMs I\nTBD\n\n\n7\nWed Feb 18\nLMMs II: Complete/partial/non-pooling\nTBD\n\n\n8\nMon Feb 23\nLMMs III: RFX syntax, ‚Äúrm-ANOVA‚Äù\nTBD\n\n\n8\nTues Feb 24\nUnsupervised Learning I: essence of linear algebra\nTBD\n\n\n8\nWed Feb 25\nUnsupervised Learning II: PCA and friends, meet sklearn\npush Last HW by Tues Mar 3\n\n\n9\nMon Mar 2\nTBD\nTBD\n\n\n9\nTues Mar 3\nTBD, last hw review\nTBD\n\n\n9\nWed Mar 4\nTBD\nTBD\n\n\n10\nMon Mar 9\nfinal project time\n-\n\n\n10\nTues Mar 10\nfinal project time\n-\n\n\n10\nWed Mar 11\nCourse Wrap-Up\n-\n\n\nFW\nMar 16-18\nwork on final project\npush Final Project by TBD"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistical Intuitions for Social Scientists",
    "section": "",
    "text": "Current version: Winter 2026\nPre-requisites: PSYC 201A or instructor approval\nRapid advances in computing have revolutionized modern statistical practice, offering approaches that transcend traditional training in psychological statistics (Efron et al 2016). And yet at the heart of these developments are just a handful of fundamental ideas (Gelman 2021). This course is designed to help you interactively develop your own statistical intuitions about these ideas using the Python programming language. At the core of the class is a deep understanding of the General-Linear-Model (GLM) and its extensions (e.g.¬†multi-level models), through which you‚Äôll learn how to adopt model-based thinking rather than classic statistical ritualization. We‚Äôll also explore the ‚ÄúTwo Cultures‚Äù of statistical modeling (explanation vs prediction) (Breiman, 2001), integrating ideas from both to build a robust foundation for you to pursue more advanced topics & coursework (e.g.¬†machine-learning, econometrics).\nThe ‚Äúliving‚Äù open-course materials are available at https://stat-intuitions.com and developed with the following goals in mind:\n\nServe as the primary resource for all course related materials (e.g.¬†slides, readings, notebooks, etc)\n\nBe openly accessible to all past, current, future, students and the general public (live lectures & grading currently only available for enrolled students)\nServe as a reference resource for members of the Psych Department at UCSD, continually updated each course year and between course offerings\n\n\n\n\nLearn to adopt model-based-thinking rather than statistical ritualization\nAcquire a deep understanding of the General-Linear-Model (GLM) and its extensions\nDevelop statistical & inferential intuitions from first principles using modern computational approaches (e.g.¬†simulation, resampling, permutation)\nDevelop cross-disciplinary technical & conceptual skills that lay the foundation for advanced coursework (e.g.¬†deep-learning, econometrics)\n\n\n\n\nLecturer: Eshin Jolly\nTA: Jane Yang\nOffice Hours: Slack or by appointment\n\n\n\nCommunication: Slack\nLocation: Mandler 3545 (Crick Conference Room)\nSchedule: M/T/W 2-3:50pm\n\n\n\n\n\n\nNote\n\n\n\nThe week-by-week schedule is available on the schedule page\n\n\n\n\n\n\n\nWe will be using Github Classroom to manage all course materials (labs, HWs, interactive lectures). Each week, we‚Äôll update the course website with a new assignment repository link that we‚Äôll keep updated that that week‚Äôs materials. At the start of class/lab, or when a HW problem-set is made available, you should accept assignments and git clone them to your local computer to work interactively.\nWhen you‚Äôre finished with an assignment or when you want to get feedback on work-in-progress, you should commit your changes to your local copy of the assignment, and then push them to Github. This will allow your instructors to review your work, provide Feedback, and/or have a private discussion with you while referencing questions/issues in your code directly. At the same time, you‚Äôll be building up a set of references (with feedback) that you can always check-out and refresh after this class is over!\n\n\n\nWhen in doubt, this course website should be the first place you look for any logistical information! We‚Äôll update it regularly and each week with a new sidebar section.\n\n\n\nAll course communications will occur over Slack in #w26-201b channel. Keep an eye out here for all announcements, additional links/resources, and logistics updates.\n\n\n\n\nWe ºre interested in grading you on your ability to achieve the skill sets that are taught in this course regardless of your starting experience with Python. For this reason, you can attempt any Github Classroom assignment (lab or HW) multiple times, especially if you think you could do better or if you want to incorporate instructor feedback. Practically, this just means making additional code changes and pushing another commit to your assignment. Your instructors will automatically be able to see your code changes and your latest submission. We ºll grade you based partially on your accurate completion of the assignment, but mostly on your ability to demonstrate: - You attempted the assignment in good-faith (lecture, lab, or HW notebooks) - You made effort to clearly document and explain your thought process, reasoning, code, and where/why you got stuck if you did - What attempts you made to fix issues you ran into, how you approached debugging, and what you learned from the process - Why you made a particular choice in your code/analysis, and/or what assumptions you made for a particular statistical inference\n\n\n\nComponent\nWeight\n\n\n\n\nLabs & Engagement\n30%\n\n\nHomeworks\n40%\n\n\nFinal Project\n30%\n\n\n\n\n\n\nAdapted from the UC San Diego & University of Waterloo Academic Integrity Offices\n\n\n\n\n\n\nWarningGenAI is known to fabricate sources/facts and can perpetuate biases/misunderstanding\n\n\n\nYou should also be aware that there are copyright and privacy concerns with these tools. You should exercise caution when using large portions of content from AI sources for these reasons. Also, you are accountable for the content and accuracy of all work you submit in this class, including any supported by generative AI.\n\n\nWe encourage the use of Generative artificial intelligence (GenAI) tools like OpenAI‚Äôs ChatGPT, Anthropic‚Äôs Claude, and/or Google‚Äôs Gemini to help you master concepts and skills in this class in accordance with the UCSD Academic Integrity Guidelines on GenAI and the following guidelines:\n\nIf you use GenAI for any submitted coursework, you must attach a link or text transcript to any assignments you submit. Many services offer a ‚Äúshare your chat‚Äù link-creation function or you can use a Google Chrome Browser Extension like ChatGPT Exporter or Claude Exporter. This will help us provide feedback on using LLM tools effectively (if desired) and make it transparent to us how you are completing assignments, while respecting the standards of academic integrity.\nDirectly prompting GenAI with course assignments, or copying/pasting GenAI output instead of performing the work yourself, will not earn you assignment credit and could result in an academic integrity violation.\n\nInstead you should aim to master GenAI as tools that supplement your programming and critical thinking skills, not as a substitute for them. They can be especially helpful for: debugging and troubleshooting unfamiliar code, reviewing Python fundamentals, reasoning about statistical concepts via analogy/example, or simply conversing in natural language about technical concepts.\n\n\n\nAll students are expected to adhere to standards of academic integrity. Cheating of any kind on any assignment will not be tolerated. It is disrespectful to your peers, the university, and to your instructors. If you are unsure what might constitute a violation of academic integrity, ask your instructors and/or the UCSD website on academic integrity: http://academicintegrity.ucsd.edu. Any evidence of academic misconduct will be reported to the Academic Integrity Office.\n\n\n\nFamily emergencies and illness are excused absences, as per UCSD policy. Please do not come to class if you have active symptoms (instead, please rest!). In general, absences will have a direct impact on your ability to learn the skills presented in this course as well as your participation grade.\nThat being said, life happens and we genuinely care about your well-being. Sometimes you simply can‚Äôt be in class or turn in an assignment on time. There may also be times when I‚Äôm unable to make it to class for a given reason, and I will ask for your grace and understanding then as well. Please, prioritize your well-being in graduate school and use this class as a way for you to learn skills that will be useful for your career (versus focusing on passing the requirements for a grade).\n\n\n\nAny student with a documented disability will be accommodated according to University policy. For details, please consult the Office of Students with Disabilities (OSD): http://disabilities.ucsd.edu. If you require accommodation for any component of the course, please provide the instructor with documentation from OSD as soon as possible. Please note that accommodations cannot be made retroactively under any circumstances.\n\nThis syllabus is subject to change. Check the course website (stat-intuitions.com) for the most up-to-date information."
  },
  {
    "objectID": "index.html#how-well-learn",
    "href": "index.html#how-well-learn",
    "title": "Statistical Intuitions for Social Scientists",
    "section": "",
    "text": "We will be using Github Classroom to manage all course materials (labs, HWs, interactive lectures). Each week, we‚Äôll update the course website with a new assignment repository link that we‚Äôll keep updated that that week‚Äôs materials. At the start of class/lab, or when a HW problem-set is made available, you should accept assignments and git clone them to your local computer to work interactively.\nWhen you‚Äôre finished with an assignment or when you want to get feedback on work-in-progress, you should commit your changes to your local copy of the assignment, and then push them to Github. This will allow your instructors to review your work, provide Feedback, and/or have a private discussion with you while referencing questions/issues in your code directly. At the same time, you‚Äôll be building up a set of references (with feedback) that you can always check-out and refresh after this class is over!\n\n\n\nWhen in doubt, this course website should be the first place you look for any logistical information! We‚Äôll update it regularly and each week with a new sidebar section.\n\n\n\nAll course communications will occur over Slack in #w26-201b channel. Keep an eye out here for all announcements, additional links/resources, and logistics updates."
  },
  {
    "objectID": "index.html#mastery-based-grading",
    "href": "index.html#mastery-based-grading",
    "title": "Statistical Intuitions for Social Scientists",
    "section": "",
    "text": "We ºre interested in grading you on your ability to achieve the skill sets that are taught in this course regardless of your starting experience with Python. For this reason, you can attempt any Github Classroom assignment (lab or HW) multiple times, especially if you think you could do better or if you want to incorporate instructor feedback. Practically, this just means making additional code changes and pushing another commit to your assignment. Your instructors will automatically be able to see your code changes and your latest submission. We ºll grade you based partially on your accurate completion of the assignment, but mostly on your ability to demonstrate: - You attempted the assignment in good-faith (lecture, lab, or HW notebooks) - You made effort to clearly document and explain your thought process, reasoning, code, and where/why you got stuck if you did - What attempts you made to fix issues you ran into, how you approached debugging, and what you learned from the process - Why you made a particular choice in your code/analysis, and/or what assumptions you made for a particular statistical inference\n\n\n\nComponent\nWeight\n\n\n\n\nLabs & Engagement\n30%\n\n\nHomeworks\n40%\n\n\nFinal Project\n30%"
  },
  {
    "objectID": "index.html#generative-ai-course-policy",
    "href": "index.html#generative-ai-course-policy",
    "title": "Statistical Intuitions for Social Scientists",
    "section": "",
    "text": "Adapted from the UC San Diego & University of Waterloo Academic Integrity Offices\n\n\n\n\n\n\nWarningGenAI is known to fabricate sources/facts and can perpetuate biases/misunderstanding\n\n\n\nYou should also be aware that there are copyright and privacy concerns with these tools. You should exercise caution when using large portions of content from AI sources for these reasons. Also, you are accountable for the content and accuracy of all work you submit in this class, including any supported by generative AI.\n\n\nWe encourage the use of Generative artificial intelligence (GenAI) tools like OpenAI‚Äôs ChatGPT, Anthropic‚Äôs Claude, and/or Google‚Äôs Gemini to help you master concepts and skills in this class in accordance with the UCSD Academic Integrity Guidelines on GenAI and the following guidelines:\n\nIf you use GenAI for any submitted coursework, you must attach a link or text transcript to any assignments you submit. Many services offer a ‚Äúshare your chat‚Äù link-creation function or you can use a Google Chrome Browser Extension like ChatGPT Exporter or Claude Exporter. This will help us provide feedback on using LLM tools effectively (if desired) and make it transparent to us how you are completing assignments, while respecting the standards of academic integrity.\nDirectly prompting GenAI with course assignments, or copying/pasting GenAI output instead of performing the work yourself, will not earn you assignment credit and could result in an academic integrity violation.\n\nInstead you should aim to master GenAI as tools that supplement your programming and critical thinking skills, not as a substitute for them. They can be especially helpful for: debugging and troubleshooting unfamiliar code, reviewing Python fundamentals, reasoning about statistical concepts via analogy/example, or simply conversing in natural language about technical concepts."
  },
  {
    "objectID": "index.html#academic-integrity",
    "href": "index.html#academic-integrity",
    "title": "Statistical Intuitions for Social Scientists",
    "section": "",
    "text": "All students are expected to adhere to standards of academic integrity. Cheating of any kind on any assignment will not be tolerated. It is disrespectful to your peers, the university, and to your instructors. If you are unsure what might constitute a violation of academic integrity, ask your instructors and/or the UCSD website on academic integrity: http://academicintegrity.ucsd.edu. Any evidence of academic misconduct will be reported to the Academic Integrity Office."
  },
  {
    "objectID": "index.html#absence-policy",
    "href": "index.html#absence-policy",
    "title": "Statistical Intuitions for Social Scientists",
    "section": "",
    "text": "Family emergencies and illness are excused absences, as per UCSD policy. Please do not come to class if you have active symptoms (instead, please rest!). In general, absences will have a direct impact on your ability to learn the skills presented in this course as well as your participation grade.\nThat being said, life happens and we genuinely care about your well-being. Sometimes you simply can‚Äôt be in class or turn in an assignment on time. There may also be times when I‚Äôm unable to make it to class for a given reason, and I will ask for your grace and understanding then as well. Please, prioritize your well-being in graduate school and use this class as a way for you to learn skills that will be useful for your career (versus focusing on passing the requirements for a grade)."
  },
  {
    "objectID": "index.html#osd-accommodations",
    "href": "index.html#osd-accommodations",
    "title": "Statistical Intuitions for Social Scientists",
    "section": "",
    "text": "Any student with a documented disability will be accommodated according to University policy. For details, please consult the Office of Students with Disabilities (OSD): http://disabilities.ucsd.edu. If you require accommodation for any component of the course, please provide the instructor with documentation from OSD as soon as possible. Please note that accommodations cannot be made retroactively under any circumstances.\n\nThis syllabus is subject to change. Check the course website (stat-intuitions.com) for the most up-to-date information."
  },
  {
    "objectID": "guides/python-guide.html",
    "href": "guides/python-guide.html",
    "title": "Python",
    "section": "",
    "text": "Terminal commands cheatsheet\nPython basics cheatsheet\nPython interactive reference\nPython for R users"
  },
  {
    "objectID": "guides/python-guide.html#basics",
    "href": "guides/python-guide.html#basics",
    "title": "Python",
    "section": "",
    "text": "Terminal commands cheatsheet\nPython basics cheatsheet\nPython interactive reference\nPython for R users"
  },
  {
    "objectID": "guides/python-guide.html#python-libraries-well-use",
    "href": "guides/python-guide.html#python-libraries-well-use",
    "title": "Python",
    "section": "Python libraries we‚Äôll use",
    "text": "Python libraries we‚Äôll use\n\nWhen you‚Äôre working in Python it can be super helpful to regularly refer to these resources. Remember that you can always use any API reference link below to get a comprehensive list of all the functions and methods in a library - a bit nicer than only relying on ? in your notebook.\n\nThroughout the course we‚Äô‚Äôll make use of the following Python libraries in case you want quickly reference their documentation:\n\npolars\nseaborn\nmatplotlib\nscipy\nnumpy\nbossanova\nscikit-learn\n\n\npolars - DataFrames & tidy data analysis\n\nPolars user guide\nPolars API reference\nTidyverse and Polars side-by-side\nPolars Rgonomic patterns\nPandas - alternative DataFrame library we‚Äôre NOT using\n\n\n\nseaborn - high-level statistical visualizations\n\nSeaborn user guide\nSeaborn API\nSeaborn cheatsheet\n\n\n\nmatplotlib - lower-level plot customization\n\nMatplotlib user guide\nMatplotlib API reference\nMatplotlib tutorials\nMatplotlib cheatsheets\n\n\n\nscipy - scientific functions & basic stats\n\nSciPy user guide\nSciPy API reference\nSummary statistics\nResampling, i.e.¬†montecarlo, bootstrap, permutation\nHypothesis testing\n\n\n\nnumpy - arrays, matrices, and linear algebra\n\nNumpy tutorials\nNumpy API reference\nNumpy Cheatsheet\nNumpy for MATLAB users\n\n\n\nbossanova - intuitive formula-based statistical modeling\n\nDocumentation & tutorials\n\n\n\nscikit-learn - machine-learning\n\nScikit-learn cheatsheet\nSupervised learning\nDecomposition\nModel selection & evaluations"
  },
  {
    "objectID": "guides/git-guide.html",
    "href": "guides/git-guide.html",
    "title": "Git & Github",
    "section": "",
    "text": "In this course we‚Äôll assume that you‚Äôre at least somewhat familiar with git and github. If not you can toggle the drop-down below for a slide-deck that provides a high-level conceptual overview of version control using the analogy of a ‚Äúsocial time-machine.‚Äù"
  },
  {
    "objectID": "guides/git-guide.html#most-common-commands",
    "href": "guides/git-guide.html#most-common-commands",
    "title": "Git & Github",
    "section": "Most common commands",
    "text": "Most common commands\nRather than spend time on nitty-gritty details of git, we‚Äôre providing a list of the most common commands you‚Äôll use in class (and in your day-to-day work!).\n\ngit status\nSee what files are ready to be made into a ‚Äúsnapshot‚Äù (committed) and which ones are not being kept track of\n\n\n\ngit add\nAdd one or more files to the list of files that should be made into a ‚Äúsnapshot‚Äù (committed) \n\n\ngit reset\nRemove one or more file from the list of files that should be made into a ‚Äúsnapshot‚Äù (committed).\nThis doesn‚Äôt remove/delete files. It just removes them from the file you plan to include in this commit.\n\n\ngit commit\nTake a ‚Äúsnapshot‚Äù of all currently tracked project files. Files need to be ‚Äúprepped‚Äù (staged) for commit using git add beforehand. You‚Äôll almost always use the -m 'some commit message' flag when running this command. These messages will then appear in the git log!\n\n\n\ngit log\nSee the full historical timeline of the project\n\n\n\ngit init\nCreate a new git repository for the first time (will not add any files)"
  },
  {
    "objectID": "guides/git-guide.html#commandsoperations-that-work-with-github",
    "href": "guides/git-guide.html#commandsoperations-that-work-with-github",
    "title": "Git & Github",
    "section": "Commands/operations that work with github",
    "text": "Commands/operations that work with github\nThe following commands communicate between your local computer‚Äôs git repository and a remote github repository.\n\ngit clone\nDuplicate a remote repository (e.g.¬†github) on your local computer\n\n\n\ngit push\nSend latest local changes to a remote location (e.g.¬†github). You‚Äôll run this command after you‚Äôve performed a git commit\n\n\n\ngit pull\nGet the latest changes from a remote location (e.g.¬†github)\n\n\n\nforking\nCopy a remote repository on github, to your own remote account on github. This isn‚Äôt a command per se, but a way to create a copy of another project on Github that you can then clone to your own computer. This is useful when you want to work on your own independent copy of another project, while still being able to suggest changes to the original project owner via a pull request.\n\n\n\npull request\nNotify a github (remote) repository owner you would like them to review+incorporate your commits. You can make a PR against a repository you own or one that someone else owns. PRs are the predominant way that you can collaborate and integrate changes between group members on github."
  },
  {
    "objectID": "guides/git-guide.html#more-advanced-git",
    "href": "guides/git-guide.html#more-advanced-git",
    "title": "Git & Github",
    "section": "More advanced git",
    "text": "More advanced git\nWe won‚Äôt necessarily be making much of use of the following commands in class, but they‚Äôre useful to know about for your own projects.\n\ngit branch\nCreate a new independent ‚Äútimeline‚Äù for the project. This is the ‚Äútrue power‚Äù of git, where you can create a totally independent copy of your project from any point in time (i.e.¬†any commit), without affecting the original project. Branches can be useful for working on different features/ideas/etc or even collaborating with other people.\n\n\n\ngit revert\nUndo changes by reversing any specific ‚Äúsnapshot‚Äù (commit). Think of this is a ‚Äúrollback‚Äù command that adds an entry to your project timeline. In other words, in addition to ‚Äúundoing‚Äù a previous commit, we also keep a record of this ‚Äúundo‚Äù using another commit."
  },
  {
    "objectID": "glossary.html",
    "href": "glossary.html",
    "title": "Course Terminology",
    "section": "",
    "text": "Note\n\n\n\nWe‚Äôll keep this page updated as we come across new technical and statistical terms for easy reference\n\n\n\n\n\n\n\n\n\n Term  Definition \nideIntegrated-development-environment; a program that includes a code-editor, terminal/console, and other helpful features to be a ‚Äúone-stop-shop‚Äù for most of your needs\nhomebrewA command-line package manager for macOS that lets you install packages and applications using the brew command\nterminalAn application for controlling your computer via commands that you type in (e.g.¬†cd, ls) instead of point-and-click\ncommand-line-programA program that you interact with exclusively from your terminal; often abbreviated as CLI"
  },
  {
    "objectID": "guides/classroom-guide.html",
    "href": "guides/classroom-guide.html",
    "title": "Github Classroom",
    "section": "",
    "text": "We‚Äôll be using Github Classroom to share all resources for class. This is the primary way you should be downloading and working with course materials. Each week, we‚Äôll create a new Github Classroom assignment link (prefixed with üìö). Clicking it will automatically create a github repository for you containing all the materials you need.\nYou‚Äôll then be able to clone this repository to your computer, working through files interactively, make edits/updates, and commit and submit your assignment for review. Each time you push your work to Github, your instructors will be able to provide review, feedback, and discussions that directly reference your code.\nYou‚Äôll always be able to access your assignment repositories, history, and instructor feedback after the course is over. So the more effort you put into assignments, the more you engage with instructors, the more you‚Äôll learn, and the higher quality resources you‚Äôll have for your own future reference!"
  },
  {
    "objectID": "guides/classroom-guide.html#getting-assignments",
    "href": "guides/classroom-guide.html#getting-assignments",
    "title": "Github Classroom",
    "section": "Getting Assignments",
    "text": "Getting Assignments\n\nOpen any course link that starts with üìö\nAccept the assignment in your browser\nClick the URL to go the auto-created github repo (this will always be named assignment-name-YOUR-GITHUB-USERNAME)\nClone it to your local computer using git clone\nOpen and work on any notebook files using VSCode\nCommit your changes locally using git add & git commit\nPush your changes to github using git push\nRespond to any feedback discussions under the ‚ÄúPull Requests‚Äù tab on the github repo"
  },
  {
    "objectID": "guides/classroom-guide.html#updating-assignments",
    "href": "guides/classroom-guide.html#updating-assignments",
    "title": "Github Classroom",
    "section": "Updating Assignments",
    "text": "Updating Assignments\nOccasionally, we‚Äôll update assignments that you‚Äôve already accepted and git clone-d to your local computer with additional files (e.g.¬†solutions). Here‚Äôs how you can git pull them to your local computer\n\nFollow the assignment link to go to the repository on github.com that you cloned to first start the assignment. It will be named MM-DD-YOURGITHUBID, e.g.¬†‚Äú01-21-ejolly‚Äù\nClick on ‚ÄúPull Requests‚Äù\nClick on ‚ÄúGithub Classroom: Sync Assignment‚Äù\nClick on green ‚ÄúMerge pull request‚Äù button\nAfter the button turns purple, indicating the ‚Äúmerge is complete‚Äù open up a terminal on your local computer and cd into the folder you cloned from this repository, e.g.¬†‚Äú01-21-ejolly‚Äù\nUse git status to check if you‚Äôve saved some changes, but haven‚Äôt yet git commit them. If you don‚Äôt see ‚Äúnothing to commit, working tree clean‚Äù, you‚Äôll need to git add and git commit your changed files\nRun git pull to download the latest changes from github\nIf you get any wonky error message, run git merge --no-ff, and then type the following commands to exit the window that opens: :, w, q, enter\n\nIf everything worked you should see some new files in your local folder, and you can hack on them as you normally would."
  },
  {
    "objectID": "guides/formulas.html",
    "href": "guides/formulas.html",
    "title": "Statistical Formula Reference",
    "section": "",
    "text": "TipHow to Use This Reference\n\n\n\nThis guide serves as both a quick lookup and a learning resource. Each formula includes:\n\nThe formula itself ‚Äî always visible\nIntuition ‚Äî what it means conceptually (click to expand)\nPython code ‚Äî how to compute it (click to expand)\nCommon pitfalls ‚Äî mistakes to avoid\n\nQuick Jump: Notation | GLM | Model Fit | Inference | Logistic | Resampling | Mixed Models | Linear Algebra | PCA | Index"
  },
  {
    "objectID": "guides/formulas.html#quick-reference",
    "href": "guides/formulas.html#quick-reference",
    "title": "Statistical Formula Reference",
    "section": "Quick Reference",
    "text": "Quick Reference\n\nNotation Key\nThroughout this guide, we use consistent notation:\n\n\n\nSymbol\nMeaning\n\n\n\n\n\\(y\\)\nOutcome/dependent variable (single observation)\n\n\n\\(\\mathbf{y}\\)\nVector of all outcomes (\\(n \\times 1\\))\n\n\n\\(x\\)\nPredictor/independent variable (single value)\n\n\n\\(\\mathbf{X}\\)\nDesign matrix (\\(n \\times p\\))\n\n\n\\(\\beta\\)\nPopulation parameter (true coefficient)\n\n\n\\(\\hat{\\beta}\\)\nEstimated coefficient (from data)\n\n\n\\(\\boldsymbol{\\beta}\\)\nVector of all coefficients\n\n\n\\(\\epsilon\\)\nError term (population)\n\n\n\\(e\\) or \\(\\hat{\\epsilon}\\)\nResidual (sample estimate of error)\n\n\n\\(n\\)\nSample size (number of observations)\n\n\n\\(p\\)\nNumber of predictors (not counting intercept)\n\n\n\\(\\sigma^2\\)\nPopulation variance\n\n\n\\(\\hat{\\sigma}^2\\)\nEstimated variance\n\n\n\\(\\mathbf{I}\\)\nIdentity matrix\n\n\n\\(\\mathbf{X}^T\\)\nTranspose of matrix \\(\\mathbf{X}\\)\n\n\n\\(\\mathbf{X}^{-1}\\)\nInverse of matrix \\(\\mathbf{X}\\)\n\n\n\n\n\nGreek Letters Quick Reference\n\n\n\nLetter\nName\nCommon Use\n\n\n\n\n\\(\\alpha\\)\nalpha\nSignificance level (e.g., 0.05)\n\n\n\\(\\beta\\)\nbeta\nRegression coefficients\n\n\n\\(\\gamma\\)\ngamma\nGroup-level coefficients (mixed models)\n\n\n\\(\\epsilon\\)\nepsilon\nError/residual term\n\n\n\\(\\lambda\\)\nlambda\nRegularization parameter\n\n\n\\(\\mu\\)\nmu\nPopulation mean\n\n\n\\(\\sigma\\)\nsigma\nStandard deviation\n\n\n\\(\\sigma^2\\)\nsigma-squared\nVariance\n\n\n\\(\\tau\\)\ntau\nRandom effect variance\n\n\n\\(\\chi^2\\)\nchi-squared\nChi-squared distribution/statistic"
  },
  {
    "objectID": "guides/formulas.html#model-based-thinking-foundations",
    "href": "guides/formulas.html#model-based-thinking-foundations",
    "title": "Statistical Formula Reference",
    "section": "Model-Based Thinking Foundations",
    "text": "Model-Based Thinking Foundations\nBefore diving into specific formulas, let‚Äôs establish the core idea that underlies everything in this course.\n\nThe Statistical Model\n\n\n\n\n\n\nNoteFormula\n\n\n\n\\[y = f(x) + \\epsilon\\]\n\n\nWhere:\n\n\\(y\\) = observed outcome (what we measure)\n\\(f(x)\\) = systematic component (what we‚Äôre trying to model)\n\\(\\epsilon\\) = random error (what we can‚Äôt explain)\n\n\n\n\n\n\n\nTipIntuition\n\n\n\n\n\nEvery statistical model has the same basic structure: Data = Model + Error\nThe model (\\(f(x)\\)) captures the predictable part ‚Äî the pattern or relationship we believe exists. The error (\\(\\epsilon\\)) captures everything else ‚Äî measurement noise, individual differences, factors we didn‚Äôt measure.\nThe goal of modeling is to find an \\(f(x)\\) that explains as much of \\(y\\) as possible, leaving only random noise in \\(\\epsilon\\).\nWhere it comes from: This isn‚Äôt derived ‚Äî it‚Äôs a framework for thinking about data. We choose to decompose observations this way because it‚Äôs useful.\n\n\n\n\n\n\n\n\n\nNotePython Conceptual Example\n\n\n\n\n\n\n\nCode\nimport numpy as np\n\n# Simulating this idea:\n# True relationship (unknown in practice)\ndef true_f(x):\n    return 2 + 3 * x  # intercept=2, slope=3\n\n# Generate data\nnp.random.seed(42)\nx = np.array([1, 2, 3, 4, 5])\nepsilon = np.random.normal(0, 1, size=5)  # random error\ny = true_f(x) + epsilon  # observed data\n\nprint(f\"x: {x}\")\nprint(f\"true f(x): {true_f(x)}\")\nprint(f\"error: {epsilon.round(2)}\")\nprint(f\"observed y: {y.round(2)}\")\n\n\n\n\n\n\n\nResiduals\n\n\n\n\n\n\nNoteFormula\n\n\n\n\\[e_i = y_i - \\hat{y}_i\\]\n\n\nWhere:\n\n\\(e_i\\) = residual for observation \\(i\\)\n\\(y_i\\) = observed value\n\\(\\hat{y}_i\\) = predicted/fitted value\n\n\n\n\n\n\n\nTipIntuition\n\n\n\n\n\nResiduals are the leftover after your model makes its prediction. They‚Äôre your best estimate of the error term \\(\\epsilon\\).\n\nPositive residual (\\(e_i &gt; 0\\)): Model under-predicted (actual &gt; predicted)\nNegative residual (\\(e_i &lt; 0\\)): Model over-predicted (actual &lt; predicted)\nZero residual (\\(e_i = 0\\)): Perfect prediction (rare!)\n\nWhy they matter: If your model is good, residuals should look like random noise with no pattern. If you see patterns in residuals, your model is missing something systematic.\n\n\n\n\n\n\n\n\n\nNotePython Implementation\n\n\n\n\n\n\n\nCode\nimport numpy as np\n\n# Example data\ny_observed = np.array([5.2, 7.8, 10.1, 12.5, 15.3])\ny_predicted = np.array([5.0, 8.0, 11.0, 14.0, 17.0])\n\n# Calculate residuals\nresiduals = y_observed - y_predicted\n\nprint(f\"Observed:  {y_observed}\")\nprint(f\"Predicted: {y_predicted}\")\nprint(f\"Residuals: {residuals}\")\nprint(f\"Sum of residuals: {residuals.sum():.2f}\")  # Should be ~0 for good models\n\n\n\n\n\n\n\n\n\n\n\nCautionCommon Pitfalls\n\n\n\n\nConfusing residuals with errors: Residuals (\\(e\\)) are estimates of errors (\\(\\epsilon\\)). We never see true errors.\nForgetting the sign: \\(e_i = y_i - \\hat{y}_i\\), not the other way around!"
  },
  {
    "objectID": "guides/formulas.html#the-general-linear-model",
    "href": "guides/formulas.html#the-general-linear-model",
    "title": "Statistical Formula Reference",
    "section": "The General Linear Model",
    "text": "The General Linear Model\nThe GLM is the foundation for most statistical models you‚Äôll encounter.\n\nThe GLM Equation\n\n\n\n\n\n\nNoteFormula\n\n\n\n\\[\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}\\]\nwhere \\(\\boldsymbol{\\epsilon} \\sim N(\\mathbf{0}, \\sigma^2\\mathbf{I})\\)\n\n\nWhere:\n\n\\(\\mathbf{y}\\) = outcome vector (\\(n \\times 1\\))\n\\(\\mathbf{X}\\) = design matrix (\\(n \\times (p+1)\\), includes intercept column)\n\\(\\boldsymbol{\\beta}\\) = coefficient vector (\\((p+1) \\times 1\\))\n\\(\\boldsymbol{\\epsilon}\\) = error vector (\\(n \\times 1\\))\n\\(\\sigma^2\\mathbf{I}\\) = errors are independent with constant variance\n\n\n\n\n\n\n\nTipIntuition\n\n\n\n\n\nThis single equation encompasses:\n\nSimple regression (\\(p = 1\\) predictor)\nMultiple regression (\\(p &gt; 1\\) predictors)\nANOVA (predictors are group indicators)\nANCOVA (mix of continuous and categorical)\n\nMatrix multiplication \\(\\mathbf{X}\\boldsymbol{\\beta}\\) gives you predicted values for all observations at once. Each row of \\(\\mathbf{X}\\) represents one observation‚Äôs predictor values; multiplying by \\(\\boldsymbol{\\beta}\\) gives that observation‚Äôs predicted outcome.\nThe assumption \\(\\boldsymbol{\\epsilon} \\sim N(\\mathbf{0}, \\sigma^2\\mathbf{I})\\) means:\n\nErrors average to zero (no systematic bias)\nAll errors have the same variance (homoscedasticity)\nErrors are uncorrelated with each other (independence)\nErrors are normally distributed\n\n\n\n\n\n\n\n\n\n\nNotePython Implementation\n\n\n\n\n\n\n\nCode\nimport numpy as np\n\n# Example: y = 2 + 3*x with 5 observations\ny = np.array([5, 8, 11, 14, 17])  # outcomes\n\n# Design matrix: first column = 1s (intercept), second = x values\nX = np.array([\n    [1, 1],\n    [1, 2],\n    [1, 3],\n    [1, 4],\n    [1, 5]\n])\n\n# Coefficients (if we knew them)\nbeta = np.array([2, 3])  # intercept=2, slope=3\n\n# Matrix multiplication gives predictions\ny_predicted = X @ beta  # @ is matrix multiplication\nprint(f\"Predicted: {y_predicted}\")\nprint(f\"Observed:  {y}\")\n\n\n\n\n\n\n\nDesign Matrix Structure\n\n\n\n\n\n\nNoteFormula\n\n\n\n\\[\\mathbf{X} = \\begin{bmatrix} 1 & x_{11} & x_{12} & \\cdots & x_{1p} \\\\ 1 & x_{21} & x_{22} & \\cdots & x_{2p} \\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ 1 & x_{n1} & x_{n2} & \\cdots & x_{np} \\end{bmatrix}\\]\n\n\n\n\n\n\n\n\nTipIntuition\n\n\n\n\n\nThe design matrix is how we encode our predictors for the model:\n\nRows = observations (one per participant/trial)\nColumns = predictors (first column is usually 1s for intercept)\nFirst column of 1s = allows us to estimate the intercept\n\nFor categorical predictors, we use dummy coding (0s and 1s) or other contrast coding schemes instead of raw category labels.\n\n\n\n\n\n\n\n\n\nNotePython Implementation\n\n\n\n\n\n\n\nCode\nimport numpy as np\nimport polars as pl\n\n# From a DataFrame\ndf = pl.DataFrame({\n    'y': [5, 8, 11, 14, 17],\n    'x1': [1, 2, 3, 4, 5],\n    'x2': [0.5, 1.0, 1.5, 2.0, 2.5]\n})\n\n# Extract as numpy arrays\ny = df['y'].to_numpy()\n\n# Create design matrix (add intercept column)\nX = np.column_stack([\n    np.ones(len(df)),           # intercept\n    df['x1'].to_numpy(),        # predictor 1\n    df['x2'].to_numpy()         # predictor 2\n])\n\nprint(\"Design matrix X:\")\nprint(X)\nprint(f\"\\nShape: {X.shape}\")  # (n_observations, n_predictors+1)\n\n\n\n\n\n\n\nOLS Estimator\n\n\n\n\n\n\nNoteFormula\n\n\n\n\\[\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\\]\n\n\nWhere:\n\n\\(\\hat{\\boldsymbol{\\beta}}\\) = estimated coefficients\n\\(\\mathbf{X}^T\\) = transpose of design matrix\n\\((\\mathbf{X}^T\\mathbf{X})^{-1}\\) = inverse of \\(\\mathbf{X}^T\\mathbf{X}\\)\n\n\n\n\n\n\n\nTipIntuition\n\n\n\n\n\nOLS = Ordinary Least Squares. This formula finds the coefficients that minimize the sum of squared residuals.\nWhere it comes from:\n\nWe want to minimize: \\(\\sum_i (y_i - \\hat{y}_i)^2 = (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^T(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})\\)\nTake the derivative with respect to \\(\\boldsymbol{\\beta}\\) and set to zero\nSolve for \\(\\boldsymbol{\\beta}\\)‚Ä¶ and you get this formula!\n\nReading the formula:\n\n\\(\\mathbf{X}^T\\mathbf{y}\\) = how much each predictor correlates with the outcome\n\\((\\mathbf{X}^T\\mathbf{X})^{-1}\\) = adjusts for correlations among predictors\n\n\n\n\n\n\n\n\n\n\nNotePython Implementation\n\n\n\n\n\n\n\nCode\nimport numpy as np\n\n# Example data\ny = np.array([5.2, 7.8, 10.1, 13.2, 15.9])\nX = np.array([\n    [1, 1],\n    [1, 2],\n    [1, 3],\n    [1, 4],\n    [1, 5]\n])\n\n# OLS formula step by step\nXtX = X.T @ X                    # X'X\nXtX_inv = np.linalg.inv(XtX)     # (X'X)^(-1)\nXty = X.T @ y                    # X'y\nbeta_hat = XtX_inv @ Xty         # Final estimate\n\nprint(f\"Estimated coefficients: {beta_hat}\")\nprint(f\"  Intercept: {beta_hat[0]:.3f}\")\nprint(f\"  Slope: {beta_hat[1]:.3f}\")\n\n# Verify with numpy's built-in (uses same math internally)\nbeta_numpy, residuals, rank, s = np.linalg.lstsq(X, y, rcond=None)\nprint(f\"\\nNumPy lstsq result: {beta_numpy}\")\n\n\n\n\n\n\n\n\n\n\n\nCautionCommon Pitfalls\n\n\n\n\nForgetting the intercept column: Your design matrix needs a column of 1s unless you‚Äôre centering predictors\nSingular matrix: If predictors are perfectly correlated, \\(\\mathbf{X}^T\\mathbf{X}\\) can‚Äôt be inverted\nOrder matters for matrix multiplication: \\(\\mathbf{X}^T\\mathbf{X}\\) is different from \\(\\mathbf{X}\\mathbf{X}^T\\)\n\n\n\nSee also: Design Matrix, Variance of Coefficients\n\n\nVariance of Coefficients\n\n\n\n\n\n\nNoteFormula\n\n\n\n\\[\\text{Var}(\\hat{\\boldsymbol{\\beta}}) = \\sigma^2(\\mathbf{X}^T\\mathbf{X})^{-1}\\]\n\n\n\n\n\n\n\n\nTipIntuition\n\n\n\n\n\nThis formula tells us how uncertain we are about our coefficient estimates.\nKey insights:\n\nLarger \\(\\sigma^2\\) (more noise) ‚Üí more uncertainty in coefficients\n\\((\\mathbf{X}^T\\mathbf{X})^{-1}\\) depends on your predictor values:\n\nMore spread in \\(X\\) ‚Üí smaller variance (more precise estimates)\nCorrelated predictors ‚Üí larger variance (less precise)\nMore observations ‚Üí smaller variance\n\n\nThe diagonal elements give variances for each coefficient; off-diagonal elements give covariances between coefficients.\n\n\n\n\n\n\n\n\n\nNotePython Implementation\n\n\n\n\n\n\n\nCode\nimport numpy as np\n\n# Continuing from OLS example\n# We need to estimate sigma^2 first (from residuals)\ny = np.array([5.2, 7.8, 10.1, 13.2, 15.9])\nX = np.array([[1, 1], [1, 2], [1, 3], [1, 4], [1, 5]])\n\n# Get beta_hat (from before)\nbeta_hat = np.linalg.inv(X.T @ X) @ X.T @ y\n\n# Calculate residuals and estimate sigma^2\ny_hat = X @ beta_hat\nresiduals = y - y_hat\nn, p_plus_1 = X.shape\nsigma2_hat = np.sum(residuals**2) / (n - p_plus_1)  # n - p - 1\n\n# Variance-covariance matrix of coefficients\nvar_beta = sigma2_hat * np.linalg.inv(X.T @ X)\n\nprint(\"Variance-covariance matrix:\")\nprint(var_beta)\nprint(f\"\\nVariance of intercept: {var_beta[0,0]:.4f}\")\nprint(f\"Variance of slope: {var_beta[1,1]:.4f}\")\n\n\n\n\n\n\n\nStandard Errors\n\n\n\n\n\n\nNoteFormula\n\n\n\n\\[SE(\\hat{\\beta}_j) = \\hat{\\sigma}\\sqrt{(\\mathbf{X}^T\\mathbf{X})^{-1}_{jj}}\\]\n\n\nWhere:\n\n\\(SE(\\hat{\\beta}_j)\\) = standard error of the \\(j\\)th coefficient\n\\(\\hat{\\sigma}\\) = estimated residual standard deviation\n\\((\\mathbf{X}^T\\mathbf{X})^{-1}_{jj}\\) = the \\(j\\)th diagonal element\n\n\n\n\n\n\n\nTipIntuition\n\n\n\n\n\nStandard errors are the square roots of the variances. They‚Äôre in the same units as the coefficients, making them easier to interpret.\nSE tells you: How much would \\(\\hat{\\beta}_j\\) vary if you repeated the study many times?\nA coefficient is typically ‚Äúsignificant‚Äù if it‚Äôs more than ~2 SEs away from zero (this is the t-test!).\n\n\n\n\n\n\n\n\n\nNotePython Implementation\n\n\n\n\n\n\n\nCode\nimport numpy as np\n\n# Continuing from variance example\n# var_beta already computed above\n\n# Standard errors are square roots of diagonal\nse_intercept = np.sqrt(var_beta[0, 0])\nse_slope = np.sqrt(var_beta[1, 1])\n\nprint(f\"SE of intercept: {se_intercept:.4f}\")\nprint(f\"SE of slope: {se_slope:.4f}\")\n\n# Or get all SEs at once\nse_all = np.sqrt(np.diag(var_beta))\nprint(f\"All SEs: {se_all}\")\n\n\n\n\n\n\n\nResidual Variance Estimate\n\n\n\n\n\n\nNoteFormula\n\n\n\n\\[\\hat{\\sigma}^2 = \\frac{\\sum_i(y_i - \\hat{y}_i)^2}{n - p - 1} = \\frac{SSE}{n - p - 1}\\]\n\n\nWhere:\n\n\\(\\hat{\\sigma}^2\\) = estimated error variance\n\\(SSE\\) = sum of squared errors (residuals)\n\\(n - p - 1\\) = degrees of freedom (residual df)\n\n\n\n\n\n\n\nTipIntuition\n\n\n\n\n\nWe divide by \\(n - p - 1\\) (not \\(n\\)) because we ‚Äúused up‚Äù \\(p + 1\\) degrees of freedom estimating the coefficients.\nWhy \\(n - p - 1\\)?\n\n\\(n\\) = total observations\n\\(p\\) = number of predictors\n\\(1\\) = the intercept\nEach coefficient we estimate ‚Äúcosts‚Äù one degree of freedom\n\nThis correction makes \\(\\hat{\\sigma}^2\\) an unbiased estimator of the true \\(\\sigma^2\\).\n\n\n\n\n\n\n\n\n\nNotePython Implementation\n\n\n\n\n\n\n\nCode\nimport numpy as np\n\n# Example\ny = np.array([5.2, 7.8, 10.1, 13.2, 15.9])\nX = np.array([[1, 1], [1, 2], [1, 3], [1, 4], [1, 5]])\n\nbeta_hat = np.linalg.inv(X.T @ X) @ X.T @ y\ny_hat = X @ beta_hat\nresiduals = y - y_hat\n\nn = len(y)\np = X.shape[1] - 1  # number of predictors (not counting intercept)\n\nSSE = np.sum(residuals**2)\ndf_residual = n - p - 1\nsigma2_hat = SSE / df_residual\nsigma_hat = np.sqrt(sigma2_hat)\n\nprint(f\"SSE: {SSE:.4f}\")\nprint(f\"Residual df: {df_residual}\")\nprint(f\"Estimated variance (sigma^2): {sigma2_hat:.4f}\")\nprint(f\"Estimated SD (sigma): {sigma_hat:.4f}\")\n\n\n\n\n\n\n\n\n\n\n\nTipYour Turn: OLS by Hand\n\n\n\nGiven the following data, calculate the OLS estimates by hand (well, with numpy):\n\n\nCode\nimport numpy as np\n\ny = np.array([3, 5, 7, 9, 11])\nx = np.array([1, 2, 3, 4, 5])\n\n# 1. Create design matrix X (with intercept column)\n# 2. Calculate beta_hat using OLS formula\n# 3. Calculate residuals\n# 4. Calculate estimated sigma^2\n# 5. Calculate standard errors\n\n\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\n\nCode\nimport numpy as np\n\ny = np.array([3, 5, 7, 9, 11])\nx = np.array([1, 2, 3, 4, 5])\n\n# 1. Design matrix\nX = np.column_stack([np.ones(5), x])\nprint(\"Design matrix:\\n\", X)\n\n# 2. OLS estimates\nbeta_hat = np.linalg.inv(X.T @ X) @ X.T @ y\nprint(f\"\\nCoefficients: intercept={beta_hat[0]:.2f}, slope={beta_hat[1]:.2f}\")\n\n# 3. Residuals\ny_hat = X @ beta_hat\nresiduals = y - y_hat\nprint(f\"Residuals: {residuals}\")\n\n# 4. Sigma^2\nn, p_plus_1 = X.shape\nsigma2_hat = np.sum(residuals**2) / (n - p_plus_1)\nprint(f\"Sigma^2: {sigma2_hat:.6f}\")\n\n# 5. Standard errors\nvar_beta = sigma2_hat * np.linalg.inv(X.T @ X)\nse = np.sqrt(np.diag(var_beta))\nprint(f\"SE intercept: {se[0]:.6f}\")\nprint(f\"SE slope: {se[1]:.6f}\")"
  },
  {
    "objectID": "guides/formulas.html#model-fit-and-comparison",
    "href": "guides/formulas.html#model-fit-and-comparison",
    "title": "Statistical Formula Reference",
    "section": "Model Fit and Comparison",
    "text": "Model Fit and Comparison\nHow do we know if our model is any good? These metrics help us evaluate and compare models.\n\nR-squared\n\n\n\n\n\n\nNoteFormula\n\n\n\n\\[R^2 = 1 - \\frac{SS_{res}}{SS_{tot}} = 1 - \\frac{\\sum_i(y_i - \\hat{y}_i)^2}{\\sum_i(y_i - \\bar{y})^2}\\]\n\n\nWhere:\n\n\\(SS_{res}\\) = residual sum of squares (unexplained variance)\n\\(SS_{tot}\\) = total sum of squares (total variance)\n\\(\\bar{y}\\) = mean of \\(y\\)\n\n\n\n\n\n\n\nTipIntuition\n\n\n\n\n\n\\(R^2\\) answers: What proportion of variance in \\(y\\) does our model explain?\n\n\\(R^2 = 0\\): Model explains nothing (predictions = \\(\\bar{y}\\))\n\\(R^2 = 1\\): Model explains everything (perfect prediction)\n\\(R^2 = 0.5\\): Model explains 50% of variance\n\nWhere it comes from: We partition total variance into explained + unexplained:\n\\[SS_{tot} = SS_{model} + SS_{res}\\]\nThen \\(R^2 = SS_{model} / SS_{tot} = 1 - SS_{res} / SS_{tot}\\)\nImportant: \\(R^2\\) always increases when you add predictors, even useless ones!\n\n\n\n\n\n\n\n\n\nNotePython Implementation\n\n\n\n\n\n\n\nCode\nimport numpy as np\n\ny = np.array([5.2, 7.8, 10.1, 13.2, 15.9])\ny_hat = np.array([5.3, 7.9, 10.5, 13.1, 15.7])\n\n# Calculate R-squared\ny_bar = np.mean(y)\nSS_tot = np.sum((y - y_bar)**2)\nSS_res = np.sum((y - y_hat)**2)\nR2 = 1 - SS_res / SS_tot\n\nprint(f\"SS_total: {SS_tot:.4f}\")\nprint(f\"SS_residual: {SS_res:.4f}\")\nprint(f\"R-squared: {R2:.4f}\")\nprint(f\"Interpretation: Model explains {R2*100:.1f}% of variance\")\n\n\n\n\n\n\n\n\n\n\n\nCautionCommon Pitfalls\n\n\n\n\nThinking higher is always better: You can inflate \\(R^2\\) by adding noise predictors\nComparing across different outcomes: \\(R^2\\) depends on variance in \\(y\\)\nUsing with small samples: \\(R^2\\) is biased upward in small samples\n\n\n\n\n\nAdjusted R-squared\n\n\n\n\n\n\nNoteFormula\n\n\n\n\\[R^2_{adj} = 1 - (1 - R^2)\\frac{n - 1}{n - p - 1}\\]\n\n\n\n\n\n\n\n\nTipIntuition\n\n\n\n\n\nAdjusted \\(R^2\\) penalizes for adding predictors. It can actually decrease if you add a useless predictor.\nThe penalty term \\(\\frac{n-1}{n-p-1}\\) is always \\(\\geq 1\\) and gets larger as you add more predictors (\\(p\\) increases).\nUse adjusted \\(R^2\\) when: Comparing models with different numbers of predictors.\n\n\n\n\n\n\n\n\n\nNotePython Implementation\n\n\n\n\n\n\n\nCode\nimport numpy as np\n\n# Continuing from R-squared example\nR2 = 0.95  # from previous calculation\nn = 5      # observations\np = 1      # predictors (not counting intercept)\n\nR2_adj = 1 - (1 - R2) * (n - 1) / (n - p - 1)\n\nprint(f\"R-squared: {R2:.4f}\")\nprint(f\"Adjusted R-squared: {R2_adj:.4f}\")\n\n# Compare with more predictors (same R2)\np2 = 3\nR2_adj_2 = 1 - (1 - R2) * (n - 1) / (n - p2 - 1)\nprint(f\"\\nWith {p2} predictors (same R2): {R2_adj_2:.4f}\")\nprint(\"Note: More predictors ‚Üí lower adjusted R2 (if R2 doesn't improve much)\")\n\n\n\n\n\n\n\nF-statistic\n\n\n\n\n\n\nNoteFormula\n\n\n\n\\[F = \\frac{(SS_{tot} - SS_{res})/p}{SS_{res}/(n-p-1)} = \\frac{MS_{model}}{MS_{error}}\\]\n\n\nWhere:\n\n\\(MS_{model}\\) = mean square for model (explained variance / df)\n\\(MS_{error}\\) = mean square for error (unexplained variance / df)\n\\(p\\) = number of predictors\n\\(n - p - 1\\) = residual degrees of freedom\n\n\n\n\n\n\n\nTipIntuition\n\n\n\n\n\nThe F-statistic tests: Does the model explain more variance than expected by chance?\nIt‚Äôs a ratio of two variances:\n\nNumerator: How much variance does the model explain (per predictor)?\nDenominator: How much noise is left over (per residual df)?\n\nIf the model is useful, the numerator should be much larger than the denominator, giving \\(F &gt;&gt; 1\\).\nUnder the null hypothesis (model explains nothing), \\(F \\sim F(p, n-p-1)\\).\n\n\n\n\n\n\n\n\n\nNotePython Implementation\n\n\n\n\n\n\n\nCode\nimport numpy as np\nfrom scipy import stats\n\ny = np.array([5.2, 7.8, 10.1, 13.2, 15.9])\ny_hat = np.array([5.3, 7.9, 10.5, 13.1, 15.7])\ny_bar = np.mean(y)\n\nn = len(y)\np = 1  # predictors\n\nSS_tot = np.sum((y - y_bar)**2)\nSS_res = np.sum((y - y_hat)**2)\nSS_model = SS_tot - SS_res\n\n# Mean squares\nMS_model = SS_model / p\nMS_error = SS_res / (n - p - 1)\n\n# F-statistic\nF = MS_model / MS_error\n\n# p-value\np_value = 1 - stats.f.cdf(F, p, n - p - 1)\n\nprint(f\"MS_model: {MS_model:.4f}\")\nprint(f\"MS_error: {MS_error:.4f}\")\nprint(f\"F-statistic: {F:.4f}\")\nprint(f\"p-value: {p_value:.6f}\")\n\n\n\n\n\n\n\nLikelihood Ratio Test\n\n\n\n\n\n\nNoteFormula\n\n\n\n\\[\\chi^2 = -2(\\log L_{reduced} - \\log L_{full}) = -2 \\log\\left(\\frac{L_{reduced}}{L_{full}}\\right)\\]\nwith \\(df\\) = difference in number of parameters\n\n\n\n\n\n\n\n\nTipIntuition\n\n\n\n\n\nThe LRT compares two nested models (the reduced model is a special case of the full model).\nKey insight: If the full model is much better, the likelihood ratio \\(L_{reduced}/L_{full}\\) will be very small, making \\(-2 \\log(\\cdot)\\) very large.\nWhen to use: Comparing models fit with maximum likelihood (logistic regression, mixed models, etc.) where F-tests don‚Äôt apply.\nUnder \\(H_0\\) (reduced model is adequate), \\(\\chi^2 \\sim \\chi^2(df)\\).\n\n\n\n\n\n\n\n\n\nNotePython Implementation\n\n\n\n\n\n\n\nCode\nfrom scipy import stats\n\n# Example log-likelihoods (you'd get these from your fitted models)\nlogL_reduced = -45.2  # simpler model\nlogL_full = -42.8     # more complex model\n\n# LRT statistic\nchi2 = -2 * (logL_reduced - logL_full)\n\n# Degrees of freedom = difference in parameters\ndf = 2  # full model has 2 more parameters\n\n# p-value\np_value = 1 - stats.chi2.cdf(chi2, df)\n\nprint(f\"Log-likelihood (reduced): {logL_reduced}\")\nprint(f\"Log-likelihood (full): {logL_full}\")\nprint(f\"Chi-squared statistic: {chi2:.4f}\")\nprint(f\"Degrees of freedom: {df}\")\nprint(f\"p-value: {p_value:.6f}\")"
  },
  {
    "objectID": "guides/formulas.html#inference-and-uncertainty",
    "href": "guides/formulas.html#inference-and-uncertainty",
    "title": "Statistical Formula Reference",
    "section": "Inference and Uncertainty",
    "text": "Inference and Uncertainty\nHow do we quantify uncertainty and test hypotheses about our estimates?\n\nt-statistic for Coefficients\n\n\n\n\n\n\nNoteFormula\n\n\n\n\\[t = \\frac{\\hat{\\beta}_j - \\beta_{0}}{SE(\\hat{\\beta}_j)}\\]\nTypically testing \\(\\beta_0 = 0\\):\n\\[t = \\frac{\\hat{\\beta}_j}{SE(\\hat{\\beta}_j)}\\]\n\n\n\n\n\n\n\n\nTipIntuition\n\n\n\n\n\nThe t-statistic measures how many standard errors the estimate is from the null value.\n\n\\(|t| &gt; 2\\) roughly corresponds to \\(p &lt; 0.05\\) (for reasonable \\(df\\))\nLarger \\(|t|\\) = stronger evidence against the null\n\nWhy it works: If \\(H_0\\) is true and assumptions hold, \\(t \\sim t(n - p - 1)\\).\nRelationship to confidence intervals: If the 95% CI doesn‚Äôt include 0, then \\(|t| &gt; t_{crit}\\) and \\(p &lt; 0.05\\).\n\n\n\n\n\n\n\n\n\nNotePython Implementation\n\n\n\n\n\n\n\nCode\nimport numpy as np\nfrom scipy import stats\n\n# Example\nbeta_hat = 2.5\nse_beta = 0.8\nn = 30\np = 2  # predictors\n\n# t-statistic (testing beta = 0)\nt_stat = beta_hat / se_beta\ndf = n - p - 1\n\n# Two-tailed p-value\np_value = 2 * (1 - stats.t.cdf(abs(t_stat), df))\n\nprint(f\"Coefficient: {beta_hat}\")\nprint(f\"SE: {se_beta}\")\nprint(f\"t-statistic: {t_stat:.3f}\")\nprint(f\"df: {df}\")\nprint(f\"p-value: {p_value:.4f}\")\n\n\n\n\n\n\n\nConfidence Intervals\n\n\n\n\n\n\nNoteFormula\n\n\n\n\\[\\hat{\\beta}_j \\pm t_{\\alpha/2, n-p-1} \\cdot SE(\\hat{\\beta}_j)\\]\n\n\nWhere:\n\n\\(t_{\\alpha/2, n-p-1}\\) = critical t-value for desired confidence level\nFor 95% CI: \\(\\alpha = 0.05\\), so we use \\(t_{0.025, df}\\)\n\n\n\n\n\n\n\nTipIntuition\n\n\n\n\n\nA 95% CI means: If we repeated this study many times, 95% of the intervals would contain the true \\(\\beta\\).\nIt does NOT mean: ‚ÄúThere‚Äôs a 95% probability the true value is in this interval.‚Äù (The true value either is or isn‚Äôt in the interval‚Äîwe just don‚Äôt know which.)\nInterpretation tip: The width of the CI reflects uncertainty. Wider CI = more uncertain.\n\n\n\n\n\n\n\n\n\nNotePython Implementation\n\n\n\n\n\n\n\nCode\nimport numpy as np\nfrom scipy import stats\n\nbeta_hat = 2.5\nse_beta = 0.8\nn = 30\np = 2\ndf = n - p - 1\nconfidence = 0.95\n\n# Critical t-value\nt_crit = stats.t.ppf((1 + confidence) / 2, df)\n\n# Confidence interval\nci_lower = beta_hat - t_crit * se_beta\nci_upper = beta_hat + t_crit * se_beta\n\nprint(f\"Coefficient: {beta_hat}\")\nprint(f\"SE: {se_beta}\")\nprint(f\"t critical (95%): {t_crit:.3f}\")\nprint(f\"95% CI: [{ci_lower:.3f}, {ci_upper:.3f}]\")\n\n# Check if CI excludes 0\nif ci_lower &gt; 0 or ci_upper &lt; 0:\n    print(\"CI excludes 0 ‚Üí significant at alpha = 0.05\")\nelse:\n    print(\"CI includes 0 ‚Üí not significant at alpha = 0.05\")\n\n\n\n\n\n\n\n\n\n\n\nCautionCommon Pitfalls\n\n\n\n\nUsing z instead of t: For small samples, you need the t-distribution\nForgetting it‚Äôs two-tailed: For 95% CI, use \\(t_{0.025}\\), not \\(t_{0.05}\\)\nMisinterpreting: It‚Äôs about the procedure, not probability of the interval\n\n\n\n\n\nStandard Error of the Mean\n\n\n\n\n\n\nNoteFormula\n\n\n\n\\[SE(\\bar{x}) = \\frac{s}{\\sqrt{n}}\\]\n\n\nWhere:\n\n\\(s\\) = sample standard deviation\n\\(n\\) = sample size\n\n\n\n\n\n\n\nTipIntuition\n\n\n\n\n\nThe SE of the mean tells you: How much would the sample mean vary across repeated samples?\nKey insights:\n\nLarger \\(n\\) ‚Üí smaller SE (more precise estimate)\nSE decreases with \\(\\sqrt{n}\\), so you need 4x the sample size to halve the SE\nThis is different from \\(s\\) (which describes spread of individual observations)\n\n\n\n\n\n\n\n\n\n\nNotePython Implementation\n\n\n\n\n\n\n\nCode\nimport numpy as np\nfrom scipy import stats\n\ndata = np.array([245, 312, 289, 267, 301, 278, 295])\n\n# Manual calculation\nn = len(data)\nsample_mean = np.mean(data)\nsample_sd = np.std(data, ddof=1)  # ddof=1 for sample SD\nse_mean = sample_sd / np.sqrt(n)\n\nprint(f\"Sample mean: {sample_mean:.2f}\")\nprint(f\"Sample SD: {sample_sd:.2f}\")\nprint(f\"SE of mean: {se_mean:.2f}\")\n\n# Using scipy (same result)\nse_scipy = stats.sem(data)\nprint(f\"scipy.stats.sem: {se_scipy:.2f}\")\n\n\n\n\n\n\n\n\n\n\n\nTipYour Turn: Confidence Interval\n\n\n\nGiven reaction time data from 10 participants with mean = 285ms and SD = 45ms:\n\nCalculate the SE of the mean\nCalculate the 95% CI for the mean\nCalculate the 99% CI for the mean\n\nWhich interval is wider? Why?\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\n\nCode\nimport numpy as np\nfrom scipy import stats\n\nmean_rt = 285\nsd_rt = 45\nn = 10\n\n# 1. SE of mean\nse = sd_rt / np.sqrt(n)\nprint(f\"SE of mean: {se:.2f}\")\n\n# 2. 95% CI\ndf = n - 1\nt_95 = stats.t.ppf(0.975, df)\nci_95 = (mean_rt - t_95 * se, mean_rt + t_95 * se)\nprint(f\"95% CI: [{ci_95[0]:.2f}, {ci_95[1]:.2f}]\")\nprint(f\"  Width: {ci_95[1] - ci_95[0]:.2f}\")\n\n# 3. 99% CI\nt_99 = stats.t.ppf(0.995, df)\nci_99 = (mean_rt - t_99 * se, mean_rt + t_99 * se)\nprint(f\"99% CI: [{ci_99[0]:.2f}, {ci_99[1]:.2f}]\")\nprint(f\"  Width: {ci_99[1] - ci_99[0]:.2f}\")\n\nprint(\"\\n99% CI is wider because higher confidence requires a larger interval!\")"
  },
  {
    "objectID": "guides/formulas.html#different-outcome-types",
    "href": "guides/formulas.html#different-outcome-types",
    "title": "Statistical Formula Reference",
    "section": "Different Outcome Types",
    "text": "Different Outcome Types\nWhen your outcome isn‚Äôt continuous, you need different models.\n\nLogistic Regression Model\n\n\n\n\n\n\nNoteFormula\n\n\n\nLogit form: \\[\\log\\left(\\frac{p}{1-p}\\right) = \\mathbf{x}^T\\boldsymbol{\\beta}\\]\nProbability form: \\[p = \\frac{1}{1 + e^{-\\mathbf{x}^T\\boldsymbol{\\beta}}}\\]\n\n\nWhere:\n\n\\(p\\) = probability of the outcome (e.g., success, ‚Äúyes‚Äù)\n\\(\\frac{p}{1-p}\\) = odds\n\\(\\log\\left(\\frac{p}{1-p}\\right)\\) = log-odds (logit)\n\n\n\n\n\n\n\nTipIntuition\n\n\n\n\n\nLogistic regression models binary outcomes (0/1, yes/no, success/failure).\nWhy log-odds?\n\nProbabilities are bounded (0 to 1)\nThe linear predictor (\\(\\mathbf{x}^T\\boldsymbol{\\beta}\\)) can be any real number\nLog-odds transforms probabilities to an unbounded scale\n\nThe sigmoid function \\(\\frac{1}{1 + e^{-x}}\\) squashes any real number back to (0, 1).\nCoefficients mean: A 1-unit increase in \\(x_j\\) changes the log-odds by \\(\\beta_j\\).\n\n\n\n\n\n\n\n\n\nNotePython Implementation\n\n\n\n\n\n\n\nCode\nimport numpy as np\n\ndef logistic(x):\n    \"\"\"Sigmoid/logistic function: converts log-odds to probability\"\"\"\n    return 1 / (1 + np.exp(-x))\n\ndef logit(p):\n    \"\"\"Logit function: converts probability to log-odds\"\"\"\n    return np.log(p / (1 - p))\n\n# Example: predict probability from coefficients\nintercept = -2\nbeta_x = 0.5\nx_values = np.array([0, 2, 4, 6, 8])\n\n# Calculate log-odds and probabilities\nlog_odds = intercept + beta_x * x_values\nprobabilities = logistic(log_odds)\n\nprint(\"x\\tlog-odds\\tprobability\")\nfor x, lo, p in zip(x_values, log_odds, probabilities):\n    print(f\"{x}\\t{lo:.2f}\\t\\t{p:.3f}\")\n\n\n\n\n\n\n\nOdds Ratio\n\n\n\n\n\n\nNoteFormula\n\n\n\n\\[OR = e^{\\beta_j}\\]\n\n\n\n\n\n\n\n\nTipIntuition\n\n\n\n\n\nThe odds ratio is the multiplicative change in odds for a 1-unit increase in \\(x_j\\).\nInterpretation:\n\n\\(OR = 1\\): No effect (odds unchanged)\n\\(OR &gt; 1\\): Increased odds (e.g., \\(OR = 2\\) means odds double)\n\\(OR &lt; 1\\): Decreased odds (e.g., \\(OR = 0.5\\) means odds halve)\n\nExample: If \\(\\beta = 0.693\\) and \\(OR = e^{0.693} = 2\\), then a 1-unit increase in \\(x\\) doubles the odds of the outcome.\nWhy exponentiate? Because \\(\\beta\\) is on the log-odds scale; \\(e^\\beta\\) converts to the odds scale.\n\n\n\n\n\n\n\n\n\nNotePython Implementation\n\n\n\n\n\n\n\nCode\nimport numpy as np\n\n# Example logistic regression coefficient\nbeta = 0.693\n\n# Convert to odds ratio\nOR = np.exp(beta)\nprint(f\"Beta coefficient: {beta}\")\nprint(f\"Odds Ratio: {OR:.2f}\")\n\n# Interpretation\nif OR &gt; 1:\n    print(f\"A 1-unit increase in x multiplies odds by {OR:.2f}\")\nelif OR &lt; 1:\n    print(f\"A 1-unit increase in x multiplies odds by {OR:.2f} (reduces odds)\")\nelse:\n    print(\"No effect on odds\")\n\n# For a range of betas\nbetas = np.array([-1, -0.5, 0, 0.5, 1])\nORs = np.exp(betas)\nprint(\"\\nBeta\\tOdds Ratio\")\nfor b, o in zip(betas, ORs):\n    print(f\"{b:.1f}\\t{o:.2f}\")\n\n\n\n\n\n\n\n\n\n\n\nCautionCommon Pitfalls\n\n\n\n\nConfusing OR with probability: \\(OR = 2\\) does NOT mean probability doubles\nForgetting to exponentiate: Raw coefficients are log-odds, not odds ratios\nAssuming symmetry: \\(OR = 2\\) (odds double) is not the same magnitude as \\(OR = 0.5\\) (odds halve)"
  },
  {
    "objectID": "guides/formulas.html#resampling-methods",
    "href": "guides/formulas.html#resampling-methods",
    "title": "Statistical Formula Reference",
    "section": "Resampling Methods",
    "text": "Resampling Methods\nWhen parametric assumptions are questionable, resampling provides distribution-free inference.\n\nBootstrap Percentile CI\n\n\n\n\n\n\nNoteFormula\n\n\n\n\\[CI_{1-\\alpha} = [Q_{\\alpha/2}(\\hat{\\theta}^*), Q_{1-\\alpha/2}(\\hat{\\theta}^*)]\\]\n\n\nWhere:\n\n\\(\\hat{\\theta}^*\\) = bootstrap distribution of the statistic\n\\(Q_p(\\cdot)\\) = the \\(p\\)th percentile\n\n\n\n\n\n\n\nTipIntuition\n\n\n\n\n\nThe bootstrap creates a sampling distribution by resampling your data.\nThe procedure:\n\nResample \\(n\\) observations with replacement from your data\nCalculate your statistic on the resample\nRepeat many times (e.g., 10,000)\nUse percentiles of the bootstrap distribution as CI bounds\n\nWhy it works: The variability in bootstrap samples mimics the variability you‚Äôd see if you could actually repeat the study.\nWhen to use: When you‚Äôre unsure about parametric assumptions, or for statistics without easy formulas (e.g., median, correlation).\n\n\n\n\n\n\n\n\n\nNotePython Implementation\n\n\n\n\n\n\n\nCode\nimport numpy as np\nfrom scipy import stats\n\nnp.random.seed(42)\n\n# Original data\ndata = np.array([12, 15, 14, 10, 12, 11, 14, 13, 15, 16])\n\n# Bootstrap parameters\nn_bootstrap = 10000\nn = len(data)\n\n# Bootstrap resampling\nbootstrap_means = np.zeros(n_bootstrap)\nfor i in range(n_bootstrap):\n    # Resample with replacement\n    resample = np.random.choice(data, size=n, replace=True)\n    bootstrap_means[i] = np.mean(resample)\n\n# Percentile CI\nci_lower = np.percentile(bootstrap_means, 2.5)\nci_upper = np.percentile(bootstrap_means, 97.5)\n\nprint(f\"Original mean: {np.mean(data):.2f}\")\nprint(f\"Bootstrap SE: {np.std(bootstrap_means):.3f}\")\nprint(f\"95% Bootstrap CI: [{ci_lower:.2f}, {ci_upper:.2f}]\")\n\n# Compare to parametric CI\nse = stats.sem(data)\nt_crit = stats.t.ppf(0.975, n-1)\nci_param = (np.mean(data) - t_crit*se, np.mean(data) + t_crit*se)\nprint(f\"95% Parametric CI: [{ci_param[0]:.2f}, {ci_param[1]:.2f}]\")\n\n\n\n\n\n\n\n\n\n\n\nCautionCommon Pitfalls\n\n\n\n\nToo few bootstrap samples: Use at least 1,000; 10,000 is better\nForgetting ‚Äúwith replacement‚Äù: This is essential for bootstrap\nSmall original sample: Bootstrap can‚Äôt create information that isn‚Äôt there\n\n\n\n\n\nPower\n\n\n\n\n\n\nNoteFormula\n\n\n\n\\[\\text{Power} = P(\\text{reject } H_0 \\mid H_1 \\text{ is true}) = 1 - \\beta\\]\n\n\nWhere:\n\n\\(\\beta\\) = probability of Type II error (false negative)\nPower = probability of detecting a true effect\n\n\n\n\n\n\n\nTipIntuition\n\n\n\n\n\nPower answers: If there‚Äôs a real effect, what‚Äôs the chance we‚Äôll find it?\nFactors that increase power:\n\nLarger sample size (\\(n\\))\nLarger effect size (bigger true difference)\nSmaller variability (\\(\\sigma\\))\nHigher \\(\\alpha\\) level (but increases false positives)\n\nConvention: We typically want power \\(\\geq 0.80\\) (80% chance of detecting a true effect).\n\n\n\n\n\n\n\n\n\nNotePython Implementation\n\n\n\n\n\n\n\nCode\nimport numpy as np\nfrom scipy import stats\n\n# Power analysis for one-sample t-test\n# Testing if mean differs from 0\n\ndef power_one_sample_t(n, effect_size, alpha=0.05):\n    \"\"\"\n    Calculate power for one-sample t-test.\n    effect_size = (true_mean - null_mean) / sd (Cohen's d)\n    \"\"\"\n    df = n - 1\n    t_crit = stats.t.ppf(1 - alpha/2, df)  # two-tailed\n\n    # Non-central t-distribution parameter\n    ncp = effect_size * np.sqrt(n)\n\n    # Power = P(|t| &gt; t_crit | H1 true)\n    power = 1 - stats.nct.cdf(t_crit, df, ncp) + stats.nct.cdf(-t_crit, df, ncp)\n    return power\n\n# Example: medium effect size (d = 0.5)\neffect_size = 0.5\n\nprint(\"Sample Size\\tPower\")\nfor n in [10, 20, 30, 50, 100]:\n    pwr = power_one_sample_t(n, effect_size)\n    print(f\"{n}\\t\\t{pwr:.3f}\")\n\n# Find sample size for 80% power\nprint(\"\\nSearching for n to achieve 80% power...\")\nfor n in range(10, 200):\n    if power_one_sample_t(n, effect_size) &gt;= 0.80:\n        print(f\"Need n = {n} for 80% power with d = {effect_size}\")\n        break\n\n\n\n\n\n\n\n\n\n\n\nTipYour Turn: Bootstrap\n\n\n\nUse the bootstrap to estimate the 95% CI for the median reaction time:\n\n\nCode\nimport numpy as np\nnp.random.seed(123)\n\nrt_data = np.array([245, 312, 289, 267, 301, 278, 295, 310, 256, 288])\n\n# 1. Calculate the sample median\n# 2. Generate 10,000 bootstrap samples\n# 3. Calculate median for each bootstrap sample\n# 4. Find the 2.5th and 97.5th percentiles\n\n\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\n\nCode\nimport numpy as np\nnp.random.seed(123)\n\nrt_data = np.array([245, 312, 289, 267, 301, 278, 295, 310, 256, 288])\n\n# 1. Sample median\nsample_median = np.median(rt_data)\nprint(f\"Sample median: {sample_median}\")\n\n# 2-3. Bootstrap\nn_bootstrap = 10000\nn = len(rt_data)\nbootstrap_medians = np.zeros(n_bootstrap)\n\nfor i in range(n_bootstrap):\n    resample = np.random.choice(rt_data, size=n, replace=True)\n    bootstrap_medians[i] = np.median(resample)\n\n# 4. Percentile CI\nci_lower = np.percentile(bootstrap_medians, 2.5)\nci_upper = np.percentile(bootstrap_medians, 97.5)\n\nprint(f\"Bootstrap SE: {np.std(bootstrap_medians):.2f}\")\nprint(f\"95% CI for median: [{ci_lower}, {ci_upper}]\")"
  },
  {
    "objectID": "guides/formulas.html#mixed-models",
    "href": "guides/formulas.html#mixed-models",
    "title": "Statistical Formula Reference",
    "section": "Mixed Models",
    "text": "Mixed Models\nWhen observations are clustered (repeated measures, hierarchical data), mixed models account for non-independence.\n\nRandom Intercept Model\n\n\n\n\n\n\nNoteFormula\n\n\n\n\\[y_{ij} = \\beta_0 + u_{0j} + \\beta_1 x_{ij} + \\epsilon_{ij}\\]\nwhere: \\[u_{0j} \\sim N(0, \\tau^2)\\] \\[\\epsilon_{ij} \\sim N(0, \\sigma^2)\\]\n\n\nWhere:\n\n\\(y_{ij}\\) = outcome for observation \\(i\\) in group \\(j\\)\n\\(\\beta_0\\) = fixed intercept (grand mean)\n\\(u_{0j}\\) = random intercept for group \\(j\\) (deviation from grand mean)\n\\(\\beta_1\\) = fixed effect of \\(x\\)\n\\(\\epsilon_{ij}\\) = residual error\n\\(\\tau^2\\) = between-group variance\n\\(\\sigma^2\\) = within-group variance\n\n\n\n\n\n\n\nTipIntuition\n\n\n\n\n\nA random intercept model allows each group to have its own baseline while sharing a common slope.\nExample: Participants in a study each have different average reaction times, but the effect of caffeine is the same for everyone.\nThe variance decomposition:\n\n\\(\\tau^2\\) = how much groups differ from each other\n\\(\\sigma^2\\) = how much observations vary within groups\n\nWhy ‚Äúrandom‚Äù? We treat groups as a random sample from a larger population of possible groups, not as fixed categories we specifically chose.\n\n\n\n\n\n\n\n\n\nNotePython Conceptual Example\n\n\n\n\n\n\n\nCode\nimport numpy as np\n\n# Simulating random intercept data\nnp.random.seed(42)\n\nn_groups = 5\nn_per_group = 10\nn_total = n_groups * n_per_group\n\n# Fixed effects\nbeta_0 = 50  # grand intercept\nbeta_1 = 3   # slope\n\n# Variance components\ntau = 10     # between-group SD\nsigma = 5    # within-group SD\n\n# Generate data\ngroup_ids = np.repeat(range(n_groups), n_per_group)\nx = np.random.normal(0, 1, n_total)\n\n# Random intercepts (one per group)\nu_0 = np.random.normal(0, tau, n_groups)\n\n# Generate y\ny = beta_0 + u_0[group_ids] + beta_1 * x + np.random.normal(0, sigma, n_total)\n\nprint(\"Group random intercepts (u_0j):\")\nfor j, u in enumerate(u_0):\n    print(f\"  Group {j}: {u:+.2f} (effective intercept: {beta_0 + u:.2f})\")\n\nprint(f\"\\nTotal variance: {np.var(y):.2f}\")\nprint(f\"Expected: tau^2 + sigma^2 = {tau**2 + sigma**2}\")\n\n\n\n\n\n\n\nICC (Intraclass Correlation)\n\n\n\n\n\n\nNoteFormula\n\n\n\n\\[ICC = \\frac{\\tau^2}{\\tau^2 + \\sigma^2}\\]\n\n\nWhere:\n\n\\(\\tau^2\\) = between-group variance\n\\(\\sigma^2\\) = within-group variance\n\n\n\n\n\n\n\nTipIntuition\n\n\n\n\n\nICC answers: What proportion of total variance is due to group membership?\nInterpretation:\n\n\\(ICC = 0\\): Groups don‚Äôt matter (all variance is within-group)\n\\(ICC = 1\\): All variance is between groups (observations in same group are identical)\n\\(ICC = 0.5\\): Half the variance is between groups, half within\n\nRule of thumb: If ICC &gt; 0.05-0.10, you probably need to account for clustering.\nAlso means: ICC = correlation between two randomly selected observations from the same group.\n\n\n\n\n\n\n\n\n\nNotePython Implementation\n\n\n\n\n\n\n\nCode\nimport numpy as np\n\n# From model output (you'd get these from your mixed model)\ntau_squared = 100   # between-group variance\nsigma_squared = 25  # within-group variance\n\n# Calculate ICC\nicc = tau_squared / (tau_squared + sigma_squared)\n\nprint(f\"Between-group variance (tau^2): {tau_squared}\")\nprint(f\"Within-group variance (sigma^2): {sigma_squared}\")\nprint(f\"Total variance: {tau_squared + sigma_squared}\")\nprint(f\"ICC: {icc:.3f}\")\nprint(f\"\\nInterpretation: {icc*100:.1f}% of variance is between groups\")\n\n\n\n\n\n\n\n\n\n\n\nCautionCommon Pitfalls\n\n\n\n\nIgnoring clustering: Standard errors will be too small, p-values too low\nICC = 0 doesn‚Äôt mean no groups: It means groups don‚Äôt explain variance\nSample size: You need enough groups AND enough observations per group"
  },
  {
    "objectID": "guides/formulas.html#linear-algebra-essentials",
    "href": "guides/formulas.html#linear-algebra-essentials",
    "title": "Statistical Formula Reference",
    "section": "Linear Algebra Essentials",
    "text": "Linear Algebra Essentials\nThese operations are the building blocks for understanding matrix-based formulas.\n\nVectors and Matrices\n\n\n\n\n\n\nNoteNotation\n\n\n\nColumn vector: \\[\\mathbf{y} = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix}\\]\nMatrix: \\[\\mathbf{X} = \\begin{bmatrix} 1 & x_{11} & \\cdots & x_{1p} \\\\ 1 & x_{21} & \\cdots & x_{2p} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 1 & x_{n1} & \\cdots & x_{np} \\end{bmatrix}\\]\n\n\n\n\n\n\n\n\nTipIntuition\n\n\n\n\n\nVectors are ordered lists of numbers. In statistics:\n\nColumns represent variables (e.g., all values of \\(y\\))\nRows represent observations (e.g., one participant‚Äôs data)\n\nMatrices are 2D arrays. In regression:\n\nEach row = one observation\nEach column = one predictor (first column usually 1s for intercept)\nThe design matrix \\(\\mathbf{X}\\) encodes your predictors\n\n\n\n\n\n\n\n\n\n\nNotePython Implementation\n\n\n\n\n\n\n\nCode\nimport numpy as np\n\n# Column vector (note the double brackets for 2D)\ny = np.array([[5], [8], [11], [14], [17]])\nprint(\"Column vector y:\")\nprint(y)\nprint(f\"Shape: {y.shape}\")  # (5, 1)\n\n# Or simply as 1D (more common in practice)\ny_1d = np.array([5, 8, 11, 14, 17])\nprint(f\"\\n1D array shape: {y_1d.shape}\")  # (5,)\n\n# Matrix\nX = np.array([\n    [1, 1, 0.5],\n    [1, 2, 1.0],\n    [1, 3, 1.5],\n    [1, 4, 2.0],\n    [1, 5, 2.5]\n])\nprint(\"\\nDesign matrix X:\")\nprint(X)\nprint(f\"Shape: {X.shape}\")  # (5, 3) = 5 observations, 3 columns\n\n\n\n\n\n\n\nMatrix Operations\n\n\n\n\n\n\nNoteKey Operations\n\n\n\n\nTranspose: \\(\\mathbf{X}^T\\) ‚Äî flip rows and columns\nInverse: \\(\\mathbf{X}^{-1}\\) ‚Äî the matrix such that \\(\\mathbf{X}\\mathbf{X}^{-1} = \\mathbf{I}\\)\nMultiplication: \\(\\mathbf{AB}\\) ‚Äî columns of \\(\\mathbf{A}\\) must equal rows of \\(\\mathbf{B}\\)\n\n\n\n\n\n\n\n\n\nTipIntuition\n\n\n\n\n\nTranspose (\\(\\mathbf{X}^T\\)):\n\nSwaps rows and columns\nIf \\(\\mathbf{X}\\) is \\(n \\times p\\), then \\(\\mathbf{X}^T\\) is \\(p \\times n\\)\nUseful for aligning dimensions for multiplication\n\nInverse (\\(\\mathbf{X}^{-1}\\)):\n\nOnly exists for square, non-singular matrices\nLike division: \\(\\mathbf{X}^{-1}\\mathbf{X} = \\mathbf{I}\\) (identity matrix)\nIn regression, we need \\((\\mathbf{X}^T\\mathbf{X})^{-1}\\)\n\nMultiplication (\\(\\mathbf{AB}\\)):\n\nResult dimensions: \\((m \\times n) \\times (n \\times p) = (m \\times p)\\)\nNot commutative: \\(\\mathbf{AB} \\neq \\mathbf{BA}\\) in general\n\n\n\n\n\n\n\n\n\n\nNotePython Implementation\n\n\n\n\n\n\n\nCode\nimport numpy as np\n\nA = np.array([[1, 2], [3, 4], [5, 6]])  # 3x2\nB = np.array([[1, 2, 3], [4, 5, 6]])    # 2x3\n\nprint(\"A (3x2):\")\nprint(A)\n\nprint(\"\\nA transpose (2x3):\")\nprint(A.T)\n\nprint(\"\\nA @ B (3x2 @ 2x3 = 3x3):\")\nprint(A @ B)\n\n# Inverse (need square matrix)\nC = np.array([[4, 7], [2, 6]])\nC_inv = np.linalg.inv(C)\nprint(\"\\nC:\")\nprint(C)\nprint(\"\\nC inverse:\")\nprint(C_inv)\nprint(\"\\nC @ C_inv (should be identity):\")\nprint((C @ C_inv).round(10))  # round to avoid floating point noise\n\n\n\n\n\n\n\nEigenvalue Decomposition\n\n\n\n\n\n\nNoteFormula\n\n\n\n\\[\\mathbf{A} = \\mathbf{V}\\boldsymbol{\\Lambda}\\mathbf{V}^T\\]\nOr equivalently, for each eigenvector-eigenvalue pair: \\[\\mathbf{A}\\mathbf{v}_i = \\lambda_i\\mathbf{v}_i\\]\n\n\nWhere:\n\n\\(\\mathbf{A}\\) = symmetric matrix (e.g., covariance matrix)\n\\(\\mathbf{V}\\) = matrix of eigenvectors (columns)\n\\(\\boldsymbol{\\Lambda}\\) = diagonal matrix of eigenvalues\n\\(\\lambda_i\\) = the \\(i\\)th eigenvalue\n\\(\\mathbf{v}_i\\) = the \\(i\\)th eigenvector\n\n\n\n\n\n\n\nTipIntuition\n\n\n\n\n\nEigendecomposition finds the principal directions of a matrix.\nWhat it means:\n\nEigenvectors = directions that only get scaled (not rotated) by the matrix\nEigenvalues = how much scaling happens in each direction\n\nWhy it matters for PCA:\n\nEigenvectors of the covariance matrix = principal components\nEigenvalues = variance explained by each component\nLarger eigenvalue = more important direction\n\n\n\n\n\n\n\n\n\n\nNotePython Implementation\n\n\n\n\n\n\n\nCode\nimport numpy as np\n\n# Covariance matrix (symmetric)\ncov_matrix = np.array([\n    [2.0, 1.2],\n    [1.2, 1.5]\n])\n\n# Eigendecomposition\neigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n\nprint(\"Covariance matrix:\")\nprint(cov_matrix)\n\nprint(\"\\nEigenvalues:\")\nprint(eigenvalues)\n\nprint(\"\\nEigenvectors (as columns):\")\nprint(eigenvectors)\n\n# Verify: A @ v = lambda * v\nfor i in range(len(eigenvalues)):\n    v = eigenvectors[:, i]\n    lam = eigenvalues[i]\n    Av = cov_matrix @ v\n    lam_v = lam * v\n    print(f\"\\nEigenvector {i+1}: A @ v = {Av.round(4)}, lambda * v = {lam_v.round(4)}\")"
  },
  {
    "objectID": "guides/formulas.html#pca-and-dimensionality-reduction",
    "href": "guides/formulas.html#pca-and-dimensionality-reduction",
    "title": "Statistical Formula Reference",
    "section": "PCA and Dimensionality Reduction",
    "text": "PCA and Dimensionality Reduction\nPCA finds the directions of maximum variance in your data.\n\nPCA via Covariance Matrix\n\n\n\n\n\n\nNoteFormula\n\n\n\nGiven centered data \\(\\mathbf{X}\\) (mean-subtracted), the covariance matrix is: \\[\\mathbf{S} = \\frac{1}{n-1}\\mathbf{X}^T\\mathbf{X}\\]\nEigendecomposition: \\[\\mathbf{S} = \\mathbf{V}\\boldsymbol{\\Lambda}\\mathbf{V}^T\\]\n\n\n\n\n\n\n\n\nTipIntuition\n\n\n\n\n\nPCA finds:\n\nThe direction of maximum variance (PC1)\nThe direction of maximum remaining variance, orthogonal to PC1 (PC2)\nAnd so on‚Ä¶\n\nThe eigenvectors of the covariance matrix give these directions. The eigenvalues tell you how much variance each direction captures.\nDimensionality reduction: Project data onto the first \\(k\\) principal components to reduce dimensions while preserving most variance.\n\n\n\n\n\n\n\n\n\nNotePython Implementation\n\n\n\n\n\n\n\nCode\nimport numpy as np\nfrom sklearn.decomposition import PCA\n\n# Example data (5 observations, 3 variables)\nnp.random.seed(42)\nX = np.random.randn(100, 3)\nX[:, 1] = X[:, 0] * 0.8 + np.random.randn(100) * 0.5  # correlated\nX[:, 2] = X[:, 0] * 0.3 + np.random.randn(100) * 0.8\n\n# Manual PCA\nX_centered = X - X.mean(axis=0)\ncov_matrix = np.cov(X_centered, rowvar=False)\neigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n\n# Sort by eigenvalue (descending)\nidx = np.argsort(eigenvalues)[::-1]\neigenvalues = eigenvalues[idx]\neigenvectors = eigenvectors[:, idx]\n\nprint(\"Eigenvalues (variance per PC):\")\nprint(eigenvalues)\n\nprint(\"\\nProportion of variance:\")\nprint(eigenvalues / eigenvalues.sum())\n\n# Using sklearn (easier!)\npca = PCA()\npca.fit(X)\nprint(\"\\nsklearn explained variance ratio:\")\nprint(pca.explained_variance_ratio_)\n\n\n\n\n\n\n\nProportion of Variance Explained\n\n\n\n\n\n\nNoteFormula\n\n\n\n\\[\\text{Proportion}_k = \\frac{\\lambda_k}{\\sum_j \\lambda_j}\\]\n\n\n\n\n\n\n\n\nTipIntuition\n\n\n\n\n\nThis tells you how important each principal component is.\nCumulative variance: Sum up proportions to see how many PCs you need to capture a target amount of variance (e.g., 90%).\nScree plot: Plot eigenvalues to find the ‚Äúelbow‚Äù where adding more PCs doesn‚Äôt help much.\n\n\n\n\n\n\n\n\n\nNotePython Implementation\n\n\n\n\n\n\n\nCode\nimport numpy as np\n\neigenvalues = np.array([2.5, 0.8, 0.3])  # example eigenvalues\n\n# Proportion for each PC\nproportions = eigenvalues / eigenvalues.sum()\n\n# Cumulative proportion\ncumulative = np.cumsum(proportions)\n\nprint(\"PC\\tEigenvalue\\tProportion\\tCumulative\")\nfor i, (eig, prop, cum) in enumerate(zip(eigenvalues, proportions, cumulative)):\n    print(f\"PC{i+1}\\t{eig:.2f}\\t\\t{prop:.3f}\\t\\t{cum:.3f}\")\n\nprint(f\"\\nTo explain 90% of variance, need {np.argmax(cumulative &gt;= 0.9) + 1} PCs\")"
  },
  {
    "objectID": "guides/formulas.html#regularization",
    "href": "guides/formulas.html#regularization",
    "title": "Statistical Formula Reference",
    "section": "Regularization",
    "text": "Regularization\nWhen you have many predictors or multicollinearity, regularization prevents overfitting.\n\nRidge Regression\n\n\n\n\n\n\nNoteFormula\n\n\n\nOptimization: \\[\\hat{\\boldsymbol{\\beta}}_{ridge} = \\arg\\min_\\beta \\left[\\sum_i(y_i - \\mathbf{x}_i^T\\boldsymbol{\\beta})^2 + \\lambda\\sum_j\\beta_j^2\\right]\\]\nClosed form: \\[\\hat{\\boldsymbol{\\beta}}_{ridge} = (\\mathbf{X}^T\\mathbf{X} + \\lambda\\mathbf{I})^{-1}\\mathbf{X}^T\\mathbf{y}\\]\n\n\nWhere:\n\n\\(\\lambda\\) = regularization parameter (penalty strength)\n\\(\\sum_j\\beta_j^2\\) = L2 penalty (squared coefficients)\n\n\n\n\n\n\n\nTipIntuition\n\n\n\n\n\nRidge regression shrinks coefficients toward zero by adding a penalty for large coefficients.\nWhy it helps:\n\nPrevents overfitting when \\(p\\) is large relative to \\(n\\)\nHandles multicollinearity (when \\(\\mathbf{X}^T\\mathbf{X}\\) is nearly singular)\nProduces more stable estimates\n\nThe bias-variance tradeoff:\n\nRidge introduces bias (coefficients shrunk toward 0)\nBut reduces variance (more stable estimates)\nOften improves prediction on new data\n\nNote: Ridge never sets coefficients exactly to zero ‚Äî all predictors stay in the model.\n\n\n\n\n\n\n\n\n\nNotePython Implementation\n\n\n\n\n\n\n\nCode\nimport numpy as np\nfrom sklearn.linear_model import Ridge\n\n# Example data\nnp.random.seed(42)\nn, p = 50, 10\nX = np.random.randn(n, p)\ny = X[:, 0] * 3 + X[:, 1] * 2 + np.random.randn(n)\n\n# Compare OLS and Ridge\nols_beta = np.linalg.lstsq(X, y, rcond=None)[0]\n\n# Ridge with different lambda values\nfor lam in [0.1, 1.0, 10.0]:\n    ridge = Ridge(alpha=lam, fit_intercept=False)\n    ridge.fit(X, y)\n\n    print(f\"\\nlambda = {lam}\")\n    print(f\"  Sum of squared coefficients: {np.sum(ridge.coef_**2):.3f}\")\n    print(f\"  First 3 coefficients: {ridge.coef_[:3].round(3)}\")\n\nprint(\"\\nOLS (no penalty):\")\nprint(f\"  Sum of squared coefficients: {np.sum(ols_beta**2):.3f}\")\nprint(f\"  First 3 coefficients: {ols_beta[:3].round(3)}\")\n\n\n\n\n\n\n\nLasso Regression\n\n\n\n\n\n\nNoteFormula\n\n\n\n\\[\\hat{\\boldsymbol{\\beta}}_{lasso} = \\arg\\min_\\beta \\left[\\sum_i(y_i - \\mathbf{x}_i^T\\boldsymbol{\\beta})^2 + \\lambda\\sum_j|\\beta_j|\\right]\\]\n\n\nWhere:\n\n\\(\\lambda\\) = regularization parameter\n\\(\\sum_j|\\beta_j|\\) = L1 penalty (absolute values)\n\n\n\n\n\n\n\nTipIntuition\n\n\n\n\n\nLasso is like ridge, but uses absolute values instead of squares.\nKey difference: Lasso can set coefficients exactly to zero, effectively doing variable selection.\nWhen to use:\n\nYou suspect only a subset of predictors matter\nYou want an interpretable model with fewer variables\nFeature selection is part of your goal\n\nNo closed form: Unlike ridge, lasso requires iterative optimization.\n\n\n\n\n\n\n\n\n\nNotePython Implementation\n\n\n\n\n\n\n\nCode\nimport numpy as np\nfrom sklearn.linear_model import Lasso\n\n# Same data as ridge example\nnp.random.seed(42)\nn, p = 50, 10\nX = np.random.randn(n, p)\ny = X[:, 0] * 3 + X[:, 1] * 2 + np.random.randn(n)  # only first 2 predictors matter\n\n# Lasso with different lambda values\nfor lam in [0.1, 0.5, 1.0]:\n    lasso = Lasso(alpha=lam, fit_intercept=False)\n    lasso.fit(X, y)\n\n    n_nonzero = np.sum(lasso.coef_ != 0)\n    print(f\"\\nlambda = {lam}\")\n    print(f\"  Non-zero coefficients: {n_nonzero} of {p}\")\n    print(f\"  Coefficients: {lasso.coef_.round(3)}\")\n\n\n\n\n\n\n\n\n\n\n\nCautionCommon Pitfalls\n\n\n\n\nForgetting to standardize: Regularization penalizes magnitude, so predictors should be on same scale\nChoosing lambda: Use cross-validation to select the optimal \\(\\lambda\\)\nRidge vs.¬†Lasso: Ridge when all predictors might matter; Lasso for sparse solutions"
  },
  {
    "objectID": "guides/formulas.html#alphabetical-index",
    "href": "guides/formulas.html#alphabetical-index",
    "title": "Statistical Formula Reference",
    "section": "Alphabetical Index",
    "text": "Alphabetical Index\nQuick links to all formulas:\n\n\n\nFormula\nSection\n\n\n\n\nAdjusted R-squared\nModel Fit\n\n\nBootstrap CI\nResampling\n\n\nConfidence Intervals\nInference\n\n\nDesign Matrix\nGLM\n\n\nEigendecomposition\nLinear Algebra\n\n\nF-statistic\nModel Fit\n\n\nGLM Equation\nGLM\n\n\nICC\nMixed Models\n\n\nLasso Regression\nRegularization\n\n\nLikelihood Ratio Test\nModel Fit\n\n\nLogistic Regression\nOutcome Types\n\n\nMatrix Operations\nLinear Algebra\n\n\nOdds Ratio\nOutcome Types\n\n\nOLS Estimator\nGLM\n\n\nPCA\nPCA\n\n\nPower\nResampling\n\n\nR-squared\nModel Fit\n\n\nRandom Intercept Model\nMixed Models\n\n\nResidual Variance\nGLM\n\n\nResiduals\nFoundations\n\n\nRidge Regression\nRegularization\n\n\nSE of the Mean\nInference\n\n\nStandard Errors\nGLM\n\n\nStatistical Model\nFoundations\n\n\nt-statistic\nInference\n\n\nVariance of Coefficients\nGLM\n\n\nVariance Explained\nPCA\n\n\nVectors and Matrices\nLinear Algebra\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nLast updated: This reference is a living document and will be updated throughout the course.\nFound an error or have a suggestion? Let us know!"
  },
  {
    "objectID": "guides/linear-algebra.html",
    "href": "guides/linear-algebra.html",
    "title": "Interactive Linear Algebra",
    "section": "",
    "text": "TipHow to Use This Guide\n\n\n\nThis guide teaches linear algebra through interactive visualizations. Each concept includes:\n\nVisual intuition ‚Äî see what the math means geometrically\nInteractive widgets ‚Äî manipulate matrices and see results in real-time\nStatistical connections ‚Äî understand why this matters for regression\n\nCore insight: Matrices aren‚Äôt just tables of numbers‚Äîthey‚Äôre transformations of space."
  },
  {
    "objectID": "guides/linear-algebra.html#the-big-picture",
    "href": "guides/linear-algebra.html#the-big-picture",
    "title": "Interactive Linear Algebra",
    "section": "The Big Picture",
    "text": "The Big Picture\nIn statistics, we constantly work with matrices:\n\nDesign matrices (\\(\\mathbf{X}\\)) encode our predictors\nCoefficient vectors (\\(\\boldsymbol{\\beta}\\)) are what we estimate\nThe OLS formula \\(\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\\) involves transpose, multiplication, and inversion\n\nBut what do these operations mean? This guide builds your intuition from the ground up."
  },
  {
    "objectID": "guides/linear-algebra.html#vectors",
    "href": "guides/linear-algebra.html#vectors",
    "title": "Interactive Linear Algebra",
    "section": "1. Vectors: Arrows in Space",
    "text": "1. Vectors: Arrows in Space\nA vector is an arrow with a direction and magnitude. In 2D, we describe it with two numbers: how far right (\\(x\\)) and how far up (\\(y\\)).\n\n\n\n\n\n\n\nYour Vector\n||[\\mathbf{v} = \\begin{bmatrix} 2.0 \\\\ 1.0 \\end{bmatrix}||]Magnitude: ||(|\\mathbf{v}| = \\sqrt{2.0^2 + 1.0^2} = 2.24||)\n\nDrag the sliders to move the vector!\n\n\n\n\n\n\n\n\nNoteKey Insight\n\n\n\nA vector is just an arrow from the origin. The numbers tell you where the tip lands.\n\nFirst number = horizontal position\nSecond number = vertical position\n\n\n\n\nVector Operations\nScalar multiplication stretches or shrinks a vector:\n\n\n\n\n\n\n\n||[k \\cdot \\mathbf{v} = 1.5 \\cdot \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 3.0 \\\\ 1.5 \\end{bmatrix}||]\n\n||(k &gt; 1||): vector stretches\n||(0 &lt; k &lt; 1||): vector shrinks\n||(k &lt; 0||): vector flips direction"
  },
  {
    "objectID": "guides/linear-algebra.html#transformations",
    "href": "guides/linear-algebra.html#transformations",
    "title": "Interactive Linear Algebra",
    "section": "2. Linear Transformations: Matrices as Functions",
    "text": "2. Linear Transformations: Matrices as Functions\nHere‚Äôs the key insight: a matrix is a transformation of space.\nWhen you multiply a vector by a matrix, you‚Äôre transforming that vector‚Äîstretching, rotating, shearing, or reflecting it.\n\n\nEdit the Transformation Matrix\n\nWhat the columns mean:\n\nColumn 1: where ||(\\hat{i} = (1, 0)||) lands ‚Üí ||((1.0, 0.0)||)\nColumn 2: where ||(\\hat{j} = (0, 1)||) lands ‚Üí ||((0.0, 1.0)||)\n\nDeterminant: ||(1.00||) (how much area scales)\n\nLegend: Light blue = original grid, Coral = transformed grid\nSolid arrows = original basis, Dashed = transformed basis\n\n\n\n\n\n\n\n\nImportantThe Fundamental Insight\n\n\n\nThe columns of a matrix tell you where the basis vectors land.\n\\[\\mathbf{A} = \\begin{bmatrix} | & | \\\\ \\mathbf{a}_1 & \\mathbf{a}_2 \\\\ | & | \\end{bmatrix}\\]\n\nFirst column = where \\(\\hat{i} = (1, 0)\\) goes\nSecond column = where \\(\\hat{j} = (0, 1)\\) goes\n\nEvery other vector is a combination of basis vectors, so once you know where the basis lands, you know the whole transformation!\n\n\n\nTry These Transformations\nEdit the matrix above to create:\n\n\n\n\n\n\n\nTransformation\nMatrix\n\n\n\n\nHorizontal stretch by 2\n\\(\\begin{bmatrix} 2 & 0 \\\\ 0 & 1 \\end{bmatrix}\\)\n\n\n90¬∞ rotation\n\\(\\begin{bmatrix} 0 & -1 \\\\ 1 & 0 \\end{bmatrix}\\)\n\n\nHorizontal shear\n\\(\\begin{bmatrix} 1 & 1 \\\\ 0 & 1 \\end{bmatrix}\\)\n\n\nReflection over x-axis\n\\(\\begin{bmatrix} 1 & 0 \\\\ 0 & -1 \\end{bmatrix}\\)"
  },
  {
    "objectID": "guides/linear-algebra.html#multiplication",
    "href": "guides/linear-algebra.html#multiplication",
    "title": "Interactive Linear Algebra",
    "section": "3. Matrix Multiplication: Composing Transformations",
    "text": "3. Matrix Multiplication: Composing Transformations\nWhen you multiply two matrices \\(\\mathbf{BA}\\), you‚Äôre applying transformation \\(\\mathbf{A}\\) first, then transformation \\(\\mathbf{B}\\).\n\n\nMatrix A (applied first)\n\nMatrix B (applied second)\n\nComposition: BA = B @ A\n||[\\mathbf{BA} = \\begin{bmatrix} 0.00 & -1.00 \\\\ 1.00 & 0.50 \\end{bmatrix}||]Why BA and not AB? Because we read right-to-left: ||(\\mathbf{BA}\\mathbf{v}||) means \"apply ||(\\mathbf{A}||) to ||(\\mathbf{v}||), then apply ||(\\mathbf{B}||) to the result.\"\n\n\n\n\nThe Composed Transformation\n\nThis single matrix does both transformations at once!\n\n\n\n\n\n\n\n\nNoteMatrix Multiplication Rule\n\n\n\nTo multiply matrices, each entry in the result is a dot product:\n\\[(\\\\mathbf{BA})_{ij} = \\\\text{row } i \\\\text{ of } \\\\mathbf{B} \\\\cdot \\\\text{column } j \\\\text{ of } \\\\mathbf{A}\\]\nBut geometrically, you‚Äôre just composing transformations!"
  },
  {
    "objectID": "guides/linear-algebra.html#determinant",
    "href": "guides/linear-algebra.html#determinant",
    "title": "Interactive Linear Algebra",
    "section": "4. The Determinant: How Area Scales",
    "text": "4. The Determinant: How Area Scales\nThe determinant measures how much a transformation scales area.\n\n\nEdit the Matrix\n\nArea Scaling\nDeterminant: ||(\\det(\\mathbf{A}) = 2.00||)\n\nBlue square (area = 1) ‚Üí Coral parallelogram (area = ||(|2.00|||) = 2.00)\nThe determinant is positive (preserves orientation).\n\n\n\n\n\n\n\n\nImportantWhat Determinant Tells You\n\n\n\n\n\n\n\n\n\n\nDeterminant\nMeaning\n\n\n\n\n\\(\\det &gt; 1\\)\nTransformation expands area\n\n\n\\(0 &lt; \\det &lt; 1\\)\nTransformation shrinks area\n\n\n\\(\\det &lt; 0\\)\nArea scaled AND orientation flipped\n\n\n\\(\\det = 0\\)\nTransformation collapses to lower dimension (no inverse!)"
  },
  {
    "objectID": "guides/linear-algebra.html#inverse",
    "href": "guides/linear-algebra.html#inverse",
    "title": "Interactive Linear Algebra",
    "section": "5. The Matrix Inverse: Undoing Transformations",
    "text": "5. The Matrix Inverse: Undoing Transformations\nIf \\(\\mathbf{A}\\) transforms space, then \\(\\mathbf{A}^{-1}\\) undoes that transformation.\n\\[\\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{I}\\]\n\n\nMatrix A\n\nDeterminant: 2.00\nInverse A‚Åª¬π\n||[\\mathbf{A}^{-1} = \\begin{bmatrix} 0.500 & -0.500 \\\\ 0.000 & 1.000 \\end{bmatrix}||]Verification: A‚Åª¬πA = I\n||[\\mathbf{A}^{-1}\\mathbf{A} = \\begin{bmatrix} 1.0 & 0.0 \\\\ 0.0 & 1.0 \\end{bmatrix} = \\mathbf{I}||]The inverse \"undoes\" the transformation, returning to the identity.\n\n\n\n\n\n\n\n\nCautionWhen Does an Inverse Exist?\n\n\n\nA matrix has an inverse only if its determinant is non-zero.\nIf \\(\\det(\\mathbf{A}) = 0\\): - The transformation collapses space - Multiple inputs map to the same output - You can‚Äôt uniquely reverse the transformation"
  },
  {
    "objectID": "guides/linear-algebra.html#design-matrices",
    "href": "guides/linear-algebra.html#design-matrices",
    "title": "Interactive Linear Algebra",
    "section": "6. Design Matrices: Connecting to Statistics",
    "text": "6. Design Matrices: Connecting to Statistics\nIn regression, the design matrix \\(\\mathbf{X}\\) encodes your predictors.\nEach row is an observation. Each column is a predictor.\n\n\n\n\n\n\n\nDesign Matrix Structure\nFor 5 observations with 2 predictors (||(x_1||) and ||(x_2||)):\n||[\\mathbf{X} = \\begin{bmatrix}\n1 & x_{11} & x_{21} \\\\\n1 & x_{12} & x_{22} \\\\\n\\vdots & \\vdots & \\vdots \\\\\n1 & x_{1n} & x_{2n}\n\\end{bmatrix}||]Your data:\n\n\n\n\nObs\nIntercept\n||(x_1||)\n||(x_2||)\n\n\n\n\n1\n1\n2.5\n1.6\n\n\n2\n1\n4.8\n0.6\n\n\n3\n1\n3.9\n8.7\n\n\n4\n1\n3.4\n6.0\n\n\n5\n1\n1.6\n7.1\n\n\n\n\nMatrix dimensions: 5 rows √ó 3 columns = ||((5 \\times 3)||)\nThe column of 1s lets us estimate an intercept (||(\\beta_0||))."
  },
  {
    "objectID": "guides/linear-algebra.html#gram-matrix",
    "href": "guides/linear-algebra.html#gram-matrix",
    "title": "Interactive Linear Algebra",
    "section": "7. X‚ÄôX: The Gram Matrix",
    "text": "7. X‚ÄôX: The Gram Matrix\nThe matrix \\(\\mathbf{X}^T\\mathbf{X}\\) is called the Gram matrix. It captures:\n\nDiagonal: How much each predictor varies (related to variance)\nOff-diagonal: How predictors relate to each other (related to covariance)\n\n\n\nComputing X'X\nTranspose: ||(\\mathbf{X}^T||) is ||((3 \\times 5)||)\nProduct: ||(\\mathbf{X}^T\\mathbf{X}||) is ||((3 \\times 3)||)\n||[\\mathbf{X}^T\\mathbf{X} = \\begin{bmatrix}\n5.0 & 16.2 & 24.0 \\\\\n16.2 & 58.6 & 72.6 \\\\\n24.0 & 72.6 & 165.0\n\\end{bmatrix}||]Interpretation:\n\n||((5.0)||) = sum of squared intercepts = ||(n||) = 5\nDiagonal = sum of squares for each predictor\nOff-diagonal = sum of cross-products (related to covariance)\n\n\n\n\n\n\n\n\n\nNoteWhy X‚ÄôX Matters\n\n\n\nThe Gram matrix tells you about multicollinearity:\n\nIf predictors are uncorrelated, off-diagonals are small\nIf predictors are highly correlated, off-diagonals are large\nWhen \\(\\mathbf{X}^T\\mathbf{X}\\) is nearly singular (det ‚âà 0), coefficients become unstable"
  },
  {
    "objectID": "guides/linear-algebra.html#ols",
    "href": "guides/linear-algebra.html#ols",
    "title": "Interactive Linear Algebra",
    "section": "8. The OLS Formula: Putting It Together",
    "text": "8. The OLS Formula: Putting It Together\nFinally, the formula for estimating regression coefficients:\n\\[\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\\]\n\n\nStep-by-Step OLS\nGiven: Design matrix ||(\\mathbf{X}||) (5√ó3) and outcome ||(\\mathbf{y}||) (5√ó1)\nStep 1: Compute ||(\\mathbf{X}^T\\mathbf{X}||)\n$||(\\mathbf{X}^T\\mathbf{X} = \\begin{bmatrix}\n5.00 & 16.20 & 24.00 \\\\\n16.20 & 58.62 & 72.57 \\\\\n24.00 & 72.57 & 165.02\n\\end{bmatrix}||)$\nStep 2: Compute ||((\\mathbf{X}^T\\mathbf{X})^{-1}||)\n$||((3.1642, -0.6689, -0.1660, ...)||)$\nStep 3: Compute ||(\\mathbf{X}^T\\mathbf{y}||)\n$||(\\mathbf{X}^T\\mathbf{y} = \\begin{bmatrix} 41.19 \\\\ 141.31 \\\\ 203.59 \\end{bmatrix}||)$\nStep 4: Compute ||(\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}||)\n||[\\hat{\\boldsymbol{\\beta}} = \\begin{bmatrix} 2.014 \\\\ 1.513 \\\\ 0.275 \\end{bmatrix}||]Estimated model: ||(\\hat{y} = 2.01 + 1.51x_1 + 0.28x_2||)\n(True values were: ||(\\beta_0 = 2.0||), ||(\\beta_1 = 1.5||), ||(\\beta_2 = 0.3||))\n\n\n\n\n\n\n\n\nImportantWhat Each Piece Does\n\n\n\n\n\n\n\n\n\n\nTerm\nMeaning\n\n\n\n\n\\(\\mathbf{X}^T\\mathbf{X}\\)\nHow predictors relate to each other\n\n\n\\((\\mathbf{X}^T\\mathbf{X})^{-1}\\)\nRemoves redundancy between predictors\n\n\n\\(\\mathbf{X}^T\\mathbf{y}\\)\nHow each predictor relates to the outcome\n\n\n\\((\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\\)\nUnique contribution of each predictor"
  },
  {
    "objectID": "guides/linear-algebra.html#summary-the-linear-algebra-of-regression",
    "href": "guides/linear-algebra.html#summary-the-linear-algebra-of-regression",
    "title": "Interactive Linear Algebra",
    "section": "Summary: The Linear Algebra of Regression",
    "text": "Summary: The Linear Algebra of Regression\nYou now understand the geometric meaning behind every piece of OLS:\n\nVectors are arrows in space\nMatrices are transformations of space\nMultiplication composes transformations\nDeterminant measures area scaling\nInverse undoes a transformation\nDesign matrix encodes predictors\nGram matrix captures predictor relationships\nOLS formula finds the best-fitting coefficients\n\n\n\n\n\n\n\nTipGoing Further\n\n\n\n\nSee Formula Reference for all the equations\nWeek 8-9 lectures cover these topics in more depth\n3blue1brown‚Äôs Essence of Linear Algebra is an excellent visual resource"
  },
  {
    "objectID": "guides/terminology.html",
    "href": "guides/terminology.html",
    "title": "Course Terminology",
    "section": "",
    "text": "Note\n\n\n\nWe‚Äôll keep this page updated as we come across new technical and statistical terms for easy reference\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Term  Definition \nvscodeAn extremely popular general purpose IDE that supports multiple language (e.g.¬†Python, R, Javascript) and makes use of extensions to add additional functionality (e.g.¬†quarto rendering, Python notebooks).\npython-notebook.py files that we can work with interactively using code-cells and markdown-cells (similar to quarto chunks). A much richer interface for interactively working with pieces of code one-at-a-time\nshellA program that runs automatically when your Terminal starts and interprets the commands you type to control your computer instead of pointing-and-clicking. FYI: the default shell on macOS is zsh not bash.\nuvA library and environment manager for Python making it easy to create/add/update additional Python libraries & tools in a reproducible and isolated way. using a pyproject.toml ‚Äúblueprints‚Äù file\nenvironmentReally just a hidden folder on your computer (typically .venv/) that contains an isolated Python installation with all additional libraries and tools. uv handles this all for us!\nideIntegrated-development-environment; a program that includes a code-editor, terminal/console, and other helpful features to be a ‚Äúone-stop-shop‚Äù for most of your needs\nterminalAn application for controlling your computer via commands that you type in (e.g.¬†cd, ls) instead of point-and-click\ncommand-line-programA program that you interact with exclusively from your terminal; often abbreviated as CLI\nrstudioAn IDE originally designed to work with R, but also works well with Quarto documents. Also includes a Terminal separate from the R console for running shell commands.\nmarimoA program like Quarto that can render .py files as interactive Python notebooks with code cells. FYI: marimo is the modern alternative to Jupyter notebooks which you may have heard of/used in the past.\nquarto-doc.qmd files that contain a mix of prose (markdown) and code-chunks (Python/R) that you can preview and render with Quarto. By default quarto will always rerun ALL code-chunks in the file upon saving.\nquartoa scientific publishing tool that allows you to mix prose and code-cells to render executable documents in a variety of formats (website, PDF, etc)\ngithubA online cloud-based service for synchronize local git repositories with REMOTE repositories on github.com. This faciliates collaborative works flows and open-source development. We‚Äôre using the Github Classroom feature built-up on this for our course.\ngithub-useridYour user ‚Äúhandle‚Äù on github.com that identifies you when using git commands (local) and interacting with Github.com (remote). E.g. Eshin‚Äôs is @ejolly\ngitA CLI to version control LOCAL files and folders called repositories. See the git guide for more details and a command cheatsheet.\nhomebrewA command-line package manager for macOS that lets you install packages and applications using the brew command"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "References"
  },
  {
    "objectID": "weeks/01/index.html",
    "href": "weeks/01/index.html",
    "title": "Week 1",
    "section": "",
    "text": "This week we‚Äôll cover course logistics, introduce the two cultures of statistical modeling, and discuss some of the foundational concepts that set the stage for later weeks. We‚Äôll also take the time to make sure your computing environment is properly configured and take our first steps with Python.",
    "crumbs": [
      "Schedule",
      "Week 1",
      "Overview"
    ]
  },
  {
    "objectID": "weeks/01/index.html#overview",
    "href": "weeks/01/index.html#overview",
    "title": "Week 1",
    "section": "",
    "text": "This week we‚Äôll cover course logistics, introduce the two cultures of statistical modeling, and discuss some of the foundational concepts that set the stage for later weeks. We‚Äôll also take the time to make sure your computing environment is properly configured and take our first steps with Python.",
    "crumbs": [
      "Schedule",
      "Week 1",
      "Overview"
    ]
  },
  {
    "objectID": "weeks/01/index.html#slides",
    "href": "weeks/01/index.html#slides",
    "title": "Week 1",
    "section": "Slides",
    "text": "Slides\n\n\n\n\n\n\nTipüõù Mon Jan 5th - Course Intro",
    "crumbs": [
      "Schedule",
      "Week 1",
      "Overview"
    ]
  },
  {
    "objectID": "weeks/01/index.html#materials",
    "href": "weeks/01/index.html#materials",
    "title": "Week 1",
    "section": "Materials",
    "text": "Materials\n\n‚ÄúPre-Flight‚Äù Lab Setup (start here)\n\n\n\n\n\n\n\nCautionüìö Lab 01 - Python basics & polars (dataframes) intro\n\n\n\nGitHub Classroom Assignment\n\n\nRemember: There are additional Git/GitHub & Python resources available in the ‚ÄúGuides & Resources‚Äù section above!",
    "crumbs": [
      "Schedule",
      "Week 1",
      "Overview"
    ]
  },
  {
    "objectID": "weeks/01/index.html#mentioned-references",
    "href": "weeks/01/index.html#mentioned-references",
    "title": "Week 1",
    "section": "Mentioned References",
    "text": "Mentioned References\n\nStatistical Thinking\nProgramming as theory-building",
    "crumbs": [
      "Schedule",
      "Week 1",
      "Overview"
    ]
  },
  {
    "objectID": "weeks/01/lab/polars-crash-course.html",
    "href": "weeks/01/lab/polars-crash-course.html",
    "title": "A Crash Course on Python DataFrames",
    "section": "",
    "text": "In this tutorial we‚Äôll build on your basic Python skills and immediately start working with a new kind of object: DataFrame. We‚Äôll meet the first Python library we‚Äôll use throughout the course polars and walkthrough all the basics in this notebook.\nPolars is very user-friendly DataFrame library for working with structured data in Python, but also R and other languages.",
    "crumbs": [
      "Schedule",
      "Week 1",
      "Polars Crash Course"
    ]
  },
  {
    "objectID": "weeks/01/lab/polars-crash-course.html#how-to-use-this-notebook",
    "href": "weeks/01/lab/polars-crash-course.html#how-to-use-this-notebook",
    "title": "A Crash Course on Python DataFrames",
    "section": "How to use this notebook",
    "text": "How to use this notebook\nThis notebook is designed for you to work through at your own pace or use as a reference with other materials.\nAs you go through this notebook, you should regularly refer to the polars documentation to look things up and general help. Try experimenting by creating new code cells and playing around with the demonstrated functionality.\nRemember to use help() from within this notebook to look up how functionality works.",
    "crumbs": [
      "Schedule",
      "Week 1",
      "Polars Crash Course"
    ]
  },
  {
    "objectID": "weeks/01/lab/polars-crash-course.html#how-to-import-polars",
    "href": "weeks/01/lab/polars-crash-course.html#how-to-import-polars",
    "title": "A Crash Course on Python DataFrames",
    "section": "How to import Polars",
    "text": "How to import Polars\nWe can make polars available by using the import statement. It‚Äôs convention to import polars in the following way:\n\n\nCode\nimport polars as pl",
    "crumbs": [
      "Schedule",
      "Week 1",
      "Polars Crash Course"
    ]
  },
  {
    "objectID": "weeks/01/lab/polars-crash-course.html#why-use-polars",
    "href": "weeks/01/lab/polars-crash-course.html#why-use-polars",
    "title": "A Crash Course on Python DataFrames",
    "section": "Why use Polars?",
    "text": "Why use Polars?\nSo far we‚Äôve made most use of Python lists and NumPy arrays. But in practice you‚Äôre probably working with some kind of structured data, i.e.¬†spreadsheet-style data with columns and rows\n\nIn Polars we call this a DataFrame, a 2d table with rows and columns of different types of data (e.g.¬†strings, numbers, etc).\n\nEach column of a DataFrame contains the same type of data. Let‚Äôs look at an example by loading a file with the pl.read_csv() function.\nThis will return a DataFrame we can check out:\n\n\nCode\ndf = pl.read_csv('data/example.csv')\ndf\n\n\n\nshape: (3, 3)\n\n\n\nName\nAge\nSex\n\n\nstr\ni64\nstr\n\n\n\n\n\"Braund, Mr. Owen Harris\"\n22\n\"male\"\n\n\n\"Allen, Mr. William Henry\"\n35\n\"male\"\n\n\n\"Bonnell, Miss. Elizabeth\"\n58\n\"female\"\n\n\n\n\n\n\nNotice how Polars tells us the type of each column below it‚Äôs name.",
    "crumbs": [
      "Schedule",
      "Week 1",
      "Polars Crash Course"
    ]
  },
  {
    "objectID": "weeks/01/lab/polars-crash-course.html#dataframe-fundamentals",
    "href": "weeks/01/lab/polars-crash-course.html#dataframe-fundamentals",
    "title": "A Crash Course on Python DataFrames",
    "section": "DataFrame fundamentals",
    "text": "DataFrame fundamentals\nWe can get basic information about a DataFrame by accessing its attributes using . syntax without () at the end:\n\n\nCode\ndf.shape\n\n\n(3, 3)\n\n\n\n\nCode\ndf.height\n\n\n3\n\n\n\n\nCode\ndf.width\n\n\n3\n\n\n\n\nCode\ndf.columns\n\n\n['Name', 'Age', 'Sex']\n\n\nDataFrames have various methods that we can use with . syntax with a () at the end.\nRemember that methods in Python are just functions that ‚Äúbelong‚Äù to some object. In this case these methods ‚Äúbelong‚Äù to a DataFrame object and can take arguments that operate on it.\nSome might be familiar from R or other languages:\n.head() gives us the first few rows. Since we only have 3 rows, we can pass an argument to the method to as for the first 2:\n\n\nCode\ndf.head(2)\n\n\n\nshape: (2, 3)\n\n\n\nName\nAge\nSex\n\n\nstr\ni64\nstr\n\n\n\n\n\"Braund, Mr. Owen Harris\"\n22\n\"male\"\n\n\n\"Allen, Mr. William Henry\"\n35\n\"male\"\n\n\n\n\n\n\nAnd .tail() is the opposite:\n\n\nCode\ndf.tail(2) # last 2 rows\n\n\n\nshape: (2, 3)\n\n\n\nName\nAge\nSex\n\n\nstr\ni64\nstr\n\n\n\n\n\"Allen, Mr. William Henry\"\n35\n\"male\"\n\n\n\"Bonnell, Miss. Elizabeth\"\n58\n\"female\"\n\n\n\n\n\n\nWe can use .glimpse() to transpose a DataFrame. This can sometimes make it easier to see the column names listed as rows and the values listed as columns:\n\n\nCode\ndf.glimpse()\n\n\nRows: 3\nColumns: 3\n$ Name &lt;str&gt; 'Braund, Mr. Owen Harris', 'Allen, Mr. William Henry', 'Bonnell, Miss. Elizabeth'\n$ Age  &lt;i64&gt; 22, 35, 58\n$ Sex  &lt;str&gt; 'male', 'male', 'female'\n\n\n\nAnd .describe() to get some quick summary stats:\n\n\nCode\ndf.describe()\n\n\n\nshape: (9, 4)\n\n\n\nstatistic\nName\nAge\nSex\n\n\nstr\nstr\nf64\nstr\n\n\n\n\n\"count\"\n\"3\"\n3.0\n\"3\"\n\n\n\"null_count\"\n\"0\"\n0.0\n\"0\"\n\n\n\"mean\"\nnull\n38.333333\nnull\n\n\n\"std\"\nnull\n18.230012\nnull\n\n\n\"min\"\n\"Allen, Mr. William Henry\"\n22.0\n\"female\"\n\n\n\"25%\"\nnull\n35.0\nnull\n\n\n\"50%\"\nnull\n35.0\nnull\n\n\n\"75%\"\nnull\n58.0\nnull\n\n\n\"max\"\n\"Braund, Mr. Owen Harris\"\n58.0\n\"male\"\n\n\n\n\n\n\nThere are many additional methods to calculate statistics as well. But we‚Äôll revisit these later:\n\n\nCode\ndf.mean()\n\n\n\nshape: (1, 3)\n\n\n\nName\nAge\nSex\n\n\nstr\nf64\nstr\n\n\n\n\nnull\n38.333333\nnull\n\n\n\n\n\n\n\n\nCode\ndf.min()\n\n\n\nshape: (1, 3)\n\n\n\nName\nAge\nSex\n\n\nstr\ni64\nstr\n\n\n\n\n\"Allen, Mr. William Henry\"\n22\n\"female\"\n\n\n\n\n\n\nDataFrames also have a .sample() method that allows you resample rows from the DataFrame with or without replacement.\nYou can tell Polars to sample all rows without replacement, aka permuting:\n\n\nCode\ndf.sample(fraction=1, shuffle=True, with_replacement=False)\n\n\n\nshape: (3, 3)\n\n\n\nName\nAge\nSex\n\n\nstr\ni64\nstr\n\n\n\n\n\"Bonnell, Miss. Elizabeth\"\n58\n\"female\"\n\n\n\"Allen, Mr. William Henry\"\n35\n\"male\"\n\n\n\"Braund, Mr. Owen Harris\"\n22\n\"male\"\n\n\n\n\n\n\nOr resample with replacement, aka bootstrapping:\n\n\nCode\ndf.sample(fraction=1, shuffle=True, with_replacement=True)\n\n\n\nshape: (3, 3)\n\n\n\nName\nAge\nSex\n\n\nstr\ni64\nstr\n\n\n\n\n\"Bonnell, Miss. Elizabeth\"\n58\n\"female\"\n\n\n\"Allen, Mr. William Henry\"\n35\n\"male\"\n\n\n\"Braund, Mr. Owen Harris\"\n22\n\"male\"\n\n\n\n\n\n\nThese methods will be handy when we cover resampling statistics later in the course.",
    "crumbs": [
      "Schedule",
      "Week 1",
      "Polars Crash Course"
    ]
  },
  {
    "objectID": "weeks/01/lab/polars-crash-course.html#indexing-a-dataframe-for-simple-stuff-only",
    "href": "weeks/01/lab/polars-crash-course.html#indexing-a-dataframe-for-simple-stuff-only",
    "title": "A Crash Course on Python DataFrames",
    "section": "Indexing a DataFrame (for simple stuff only!)",
    "text": "Indexing a DataFrame (for simple stuff only!)\nBecause a DataFrame is a 2d table, we can use the same indexing and slicing syntax but in 2d for rows and columns.\nRemember these are 0-indexed: the first row/col is at position 0, not position 1\nIf this is our DataFrame:\n\n\nCode\ndf\n\n\n\nshape: (3, 3)\n\n\n\nName\nAge\nSex\n\n\nstr\ni64\nstr\n\n\n\n\n\"Braund, Mr. Owen Harris\"\n22\n\"male\"\n\n\n\"Allen, Mr. William Henry\"\n35\n\"male\"\n\n\n\"Bonnell, Miss. Elizabeth\"\n58\n\"female\"\n\n\n\n\n\n\nWe can slice it like this:\n\n\nCode\n# 0 row index = 1st row\n# 1 col index = 2nd col (age)\ndf[0, 1]\n\n\n22\n\n\nAnd of course we can slice using start:stop:step, which is always up-to the end value we slice to:\n\n\nCode\n# 0:2 slice = rows up to, but not including 2 - just 0, 1\n# 0 col index = 1st col (name)\ndf[0:2,0]\n\n\n\nshape: (2,)\n\n\n\nName\n\n\nstr\n\n\n\n\n\"Braund, Mr. Owen Harris\"\n\n\n\"Allen, Mr. William Henry\"\n\n\n\n\n\n\nWe can also using slicing syntax to quickly refer to columns by name:\n\n\nCode\n# All rows in column 'Name'\ndf['Name']\n\n\n\nshape: (3,)\n\n\n\nName\n\n\nstr\n\n\n\n\n\"Braund, Mr. Owen Harris\"\n\n\n\"Allen, Mr. William Henry\"\n\n\n\"Bonnell, Miss. Elizabeth\"\n\n\n\n\n\n\nWhich is equivalent to:\n\n\nCode\n# Explicitly slice 'all' rows\n# 'Name' = just the Name column\ndf[:, 'Name']\n\n\n\nshape: (3,)\n\n\n\nName\n\n\nstr\n\n\n\n\n\"Braund, Mr. Owen Harris\"\n\n\n\"Allen, Mr. William Henry\"\n\n\n\"Bonnell, Miss. Elizabeth\"\n\n\n\n\n\n\n\n\nCode\n# 0:2 slice = rows up to, but not including 2\n# 'Name' = just the Name column\ndf[0:2, 'Name']\n\n\n\nshape: (2,)\n\n\n\nName\n\n\nstr\n\n\n\n\n\"Braund, Mr. Owen Harris\"\n\n\n\"Allen, Mr. William Henry\"\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhile you can access values this way, what makes Polars powerful is that it offers a much richer ‚Äúlanguage‚Äù or set of ‚Äúpatterns‚Äù for working with DataFrames, like dplyr‚Äôs verbs in R‚Äôs Tidyverse.\nWhile it doesn‚Äôt map on one-to-one, Polars offers a consistent and intuitive way of working with DataFrames that we‚Äôll teach you in this notebook. Once the fundamentals ‚Äúclick‚Äù for you, you‚Äôll be a data manipulating ninja!",
    "crumbs": [
      "Schedule",
      "Week 1",
      "Polars Crash Course"
    ]
  },
  {
    "objectID": "weeks/01/lab/polars-crash-course.html#thinking-in-polars-contexts-expressions",
    "href": "weeks/01/lab/polars-crash-course.html#thinking-in-polars-contexts-expressions",
    "title": "A Crash Course on Python DataFrames",
    "section": "Thinking in Polars: Contexts & Expressions",
    "text": "Thinking in Polars: Contexts & Expressions\nTo understand how to ‚Äúthink‚Äù in polars, we need to understand 2 fundamental concepts: contexts and expressions\nA context in Polars is how you choose what data you want to operate on. There are only a few you‚Äôll use regularly:\n\nContexts\n\ndf.select() - to subset columns\n\n\n\ndf.with_columns() - to add new columns\n\n\n\ndf.filter() - to subset rows\n\n\n\ndf.group_by().agg() - to summarize by group\n\n\n\n\nExpressions\nAn expression is any computation we do inside of a context.\nTo build an expression we first use a selector to choose one or more columns.\nThe most common selector you‚Äôll use is pl.col() to select one or more columns by name.\nThen we used method-chaining, . syntax, to perform operations on the selected columns.\nLet‚Äôs see a simple example.",
    "crumbs": [
      "Schedule",
      "Week 1",
      "Polars Crash Course"
    ]
  },
  {
    "objectID": "weeks/01/lab/polars-crash-course.html#starting-simple",
    "href": "weeks/01/lab/polars-crash-course.html#starting-simple",
    "title": "A Crash Course on Python DataFrames",
    "section": "Starting simple",
    "text": "Starting simple\nLet‚Äôs say we have data from some experiment that contains 3 participants, each of whom made 5 judgments about some stimuli.\nWe can use the pl.read_csv() function to load a file and get back a polars DataFrame:\n\n\nCode\ndf_1 = pl.read_csv('data/example_2.csv')\ndf_1\n\n\n\nshape: (15, 3)\n\n\n\nparticipant\naccuracy\nrt\n\n\ni64\ni64\nf64\n\n\n\n\n1\n44\n261.28435\n\n\n1\n47\n728.208489\n\n\n1\n64\n801.889016\n\n\n1\n67\n713.555026\n\n\n1\n67\n362.682105\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n3\n70\n439.524973\n\n\n3\n88\n55.11928\n\n\n3\n88\n644.801272\n\n\n3\n12\n571.800553\n\n\n3\n58\n715.224208\n\n\n\n\n\n\nWe can verify there are 15 rows and 3 columns:\n\n\nCode\ndf_1.shape\n\n\n(15, 3)",
    "crumbs": [
      "Schedule",
      "Week 1",
      "Polars Crash Course"
    ]
  },
  {
    "objectID": "weeks/01/lab/polars-crash-course.html#selecting-columns-.select",
    "href": "weeks/01/lab/polars-crash-course.html#selecting-columns-.select",
    "title": "A Crash Course on Python DataFrames",
    "section": "Selecting columns: .select()",
    "text": "Selecting columns: .select()\nThe .select context is what you‚Äôll use most often. It lets you apply expressions only to the specific columns you select.\n\nHere‚Äôs a simple example. Let‚Äôs say we want to calculate the average of the accuracy column.\nHow would we start?\nFirst, we need to think about our context. Since we only want the ‚Äúaccuracy‚Äù column we can use .select().\ndf.select(                     # &lt;- this is our context\n\n)\nSecond, we need to create an expression that means: ‚Äúuse the accuracy column, and calculate it‚Äôs mean‚Äù.\nWe can create an expression by combining the selector pl.col() with the operation .mean() using . syntax, i.e.¬†method-chaining.\ndf.select(                     # &lt;- this is our context\n  pl.col('accuracy').mean()   # &lt;- this is an expression, inside this context\n)\nLet‚Äôs try this now:\n\n\nCode\n# start of context\ndf_1.select(\n    pl.col('accuracy').mean() # &lt;- this is our expression\n)\n# end of context\n\n\n\nshape: (1, 1)\n\n\n\naccuracy\n\n\nf64\n\n\n\n\n56.066667\n\n\n\n\n\n\n\n\n\n\n\n\nNoteIndentation within polars context does not matter\n\n\n\nIn this examples throughout this notebook you‚Äôll see that we split up expressions over multiple lines within a polars context. You do not have to do this as indentation does not matter between the () We‚Äôre just trying to keep things a bit more readable for you. But the following code is exactly the same as the cell above:\ndf_1.select(pl.col('accuracy').mean())\n\n\n\n\n\n\n\n\nTipYour Turn\n\n\n\nHow would you build expression to calculate the ‚Äúmedian‚Äù Reaction Time?\n\n\n\n\nCode\ndf_1.select()  # put your expression here!\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\n\nCode\ndf_1.select(pl.col('rt').median())\n\n\n\nshape: (1, 1)\n\n\n\nrt\n\n\nf64\n\n\n\n\n502.974663\n\n\n\n\n\n\n\n\n\nLet‚Äôs make our lives a bit easier and type less by using what we know about import from the previous tutorials:\n\n\nCode\n# now we can use col() instead of pl.col()\nfrom polars import col\n\n\nNow we can use col in place of pl.col\n\nExpressing multiple things\nWe can add as many expressions inside a context as we want. We just need to separate them with a ,.\nEach the result from each expression will be saved to a separate column.\nWe‚Äôll use the col() selector again to perform two different operations: n_unique() and mean() on two different columns:\n\n\nCode\n# start context\ndf_1.select(\n    col('participant').n_unique(), col('accuracy').mean() # &lt;- multiple expressions separate by ,\n)\n# end context\n\n\n\nshape: (1, 2)\n\n\n\nparticipant\naccuracy\n\n\nu32\nf64\n\n\n\n\n3\n56.066667\n\n\n\n\n\n\n\n\n\n\n\n\nTipYour Turn\n\n\n\nHow would you express the following statement in polars?\nSelect only the participant and accuracy columns\nFor each participant, calculate the number of values, i.e.¬†.count()\nFor accuracy, calculate its standard deviation (what method do you think it is?)\n\n\n\n\nCode\n# Your code here\n\n# 1) Use the .select() context\n# 2) Use col() to create 2 expressions separated by a comma\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\n\nCode\ndf_1.select(\n    col('participant').count(),\n    col('accuracy').std()\n)\n\n\n\nshape: (1, 2)\n\n\n\nparticipant\naccuracy\n\n\nu32\nf64\n\n\n\n\n15\n27.043528\n\n\n\n\n\n\n\n\n\n\n\nRepeating the same expression across many columns\nSometimes we might find ourselves repeating the same expression for different columns.\nOne way we can do that is simply by creating multiple expressions like by before, by using col() to select each column separately:\n\n\nCode\ndf_1.select(\n    col('accuracy').median(), col('rt').median()\n    )\n\n\n\nshape: (1, 2)\n\n\n\naccuracy\nrt\n\n\nf64\nf64\n\n\n\n\n64.0\n502.974663\n\n\n\n\n\n\nBut Polars makes this much easier for us - we can condense this down to a single expression by giving our selector - col() - additional column names:\n\n\nCode\ndf_1.select(col('accuracy', 'rt').median())  # &lt;- one expression repeated for both columns\n\n\n\nshape: (1, 2)\n\n\n\naccuracy\nrt\n\n\nf64\nf64\n\n\n\n\n64.0\n502.974663\n\n\n\n\n\n\nThese both do the same thing so if you find it helpful to start explicit, building up each expression one at a time, feel free to do that!\nLater on you might find it helpful to use a single condensed expression, when you find yourself getting annoyed by repeating yourself.\n\n\nRenaming expression outputs\nLet‚Äôs try creating two expressions that operate on the same column. In natural language:\n‚ÄúSelect only the accuracy column. For accuracy, calculate its median For accuracy, calculate its variance‚Äù\nLet‚Äôs try it:\n\n\nCode\ndf_1.select(col('accuracy').mean(), col('accuracy').std())\n\n\n\n---------------------------------------------------------------------------\nDuplicateError                            Traceback (most recent call last)\nCell In[30], line 1\n----&gt; 1 df_1.select(col('accuracy').mean(), col('accuracy').std())\n\nFile ~/Dropbox/docs/teaching/201b/w26/.venv/lib/python3.11/site-packages/polars/dataframe/frame.py:10150, in DataFrame.select(self, *exprs, **named_exprs)\n  10066 \"\"\"\n  10067 Select columns from this DataFrame.\n  10068 \n   (...)  10143 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n  10144 \"\"\"\n  10145 from polars.lazyframe.opt_flags import QueryOptFlags\n  10147 return (\n  10148     self.lazy()\n  10149     .select(*exprs, **named_exprs)\n&gt; 10150     .collect(optimizations=QueryOptFlags._eager())\n  10151 )\n\nFile ~/Dropbox/docs/teaching/201b/w26/.venv/lib/python3.11/site-packages/polars/_utils/deprecation.py:97, in deprecate_streaming_parameter.&lt;locals&gt;.decorate.&lt;locals&gt;.wrapper(*args, **kwargs)\n     93         kwargs[\"engine\"] = \"in-memory\"\n     95     del kwargs[\"streaming\"]\n---&gt; 97 return function(*args, **kwargs)\n\nFile ~/Dropbox/docs/teaching/201b/w26/.venv/lib/python3.11/site-packages/polars/lazyframe/opt_flags.py:328, in forward_old_opt_flags.&lt;locals&gt;.decorate.&lt;locals&gt;.wrapper(*args, **kwargs)\n    325         optflags = cb(optflags, kwargs.pop(key))  # type: ignore[no-untyped-call,unused-ignore]\n    327 kwargs[\"optimizations\"] = optflags\n--&gt; 328 return function(*args, **kwargs)\n\nFile ~/Dropbox/docs/teaching/201b/w26/.venv/lib/python3.11/site-packages/polars/lazyframe/frame.py:2429, in LazyFrame.collect(self, type_coercion, predicate_pushdown, projection_pushdown, simplify_expression, slice_pushdown, comm_subplan_elim, comm_subexpr_elim, cluster_with_columns, collapse_joins, no_optimization, engine, background, optimizations, **_kwargs)\n   2427 # Only for testing purposes\n   2428 callback = _kwargs.get(\"post_opt_callback\", callback)\n-&gt; 2429 return wrap_df(ldf.collect(engine, callback))\n\nDuplicateError: projections contained duplicate output name 'accuracy'. It's possible that multiple expressions are returning the same default column name. If this is the case, try renaming the columns with `.alias(\"new_name\")` to avoid duplicate column names.\n\n\n\n\n\n\n\n\n\nImportantpolars DuplicateError\n\n\n\nPolars automatically enforces the requirement that all column names are must be unique.\nBy default the results of an expression are saved using the same column name that you selected.\nIn this case we selected ‚Äúaccuracy‚Äù using col('accuracy') twice - once to calculate the mean and once to calculate the standard deviation. So Polars is trying to save both results into a column called accuracy causing a conflict!\nTo fix this, we can extend our expression with additional operations using method-chaining with the . syntax.\nThe operation we‚Äôre looking for is .alias() which you‚Äôll often put at the end of an expression in order to give it a new name\n\n\n\n\nCode\ndf_1.select(\n    col('accuracy').mean().alias('acc_mean'),\n    col('accuracy').std().alias('acc_std')\n    )\n\n\n\nshape: (1, 2)\n\n\n\nacc_mean\nacc_std\n\n\nf64\nf64\n\n\n\n\n56.066667\n27.043528\n\n\n\n\n\n\n\n\n\n\n\n\nNoteTwo styles of expressing yourself\n\n\n\nYou might find this style of ‚Äúmethod-chaining‚Äù the use of .alias() unintuitive at first. So Polars also lets your rename your expressions in a different ‚Äústyle‚Äù using = as in other language like R.\nIn English, we could rephrase our expressions as so:\nSelect the accuracy column Create a new column named ‚Äòacc_mean‚Äô, which is the mean of accuracy Create a new column named ‚Äòacc_std‚Äô, which is the standard-deviation of accuracy\nAnd in code like this:\ndf_1.select(\n    acc_mean = col('accuracy').mean(),\n    acc_std = col('accuracy').std()\n)\nYou can use which ever style of ‚Äúphrasing‚Äù an expression that feels more natural to you based on what you‚Äôre doing!\n\n\n\n\n\n\n\n\nTipYour turn\n\n\n\nRun the following code. Why are the values in the accuracy column being overwritten? Can you fix it?\n\n\n\n\nCode\ndf_1.select(col('participant'), col('accuracy').mean())\n\n\n\nshape: (15, 2)\n\n\n\nparticipant\naccuracy\n\n\ni64\nf64\n\n\n\n\n1\n56.066667\n\n\n1\n56.066667\n\n\n1\n56.066667\n\n\n1\n56.066667\n\n\n1\n56.066667\n\n\n‚Ä¶\n‚Ä¶\n\n\n3\n56.066667\n\n\n3\n56.066667\n\n\n3\n56.066667\n\n\n3\n56.066667\n\n\n3\n56.066667\n\n\n\n\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nThe mean of accuracy is being saved to a column named ‚Äúaccuracy‚Äù which overwrites the participant values being selected. Fix by renaming:\n\n\nCode\ndf_1.select(col('participant'), col('accuracy').mean().alias('acc_mean'))\n# Or equivalently:\n# df_1.select(col('participant'), acc_mean=col('accuracy').mean())\n\n\n\nshape: (15, 2)\n\n\n\nparticipant\nacc_mean\n\n\ni64\nf64\n\n\n\n\n1\n56.066667\n\n\n1\n56.066667\n\n\n1\n56.066667\n\n\n1\n56.066667\n\n\n1\n56.066667\n\n\n‚Ä¶\n‚Ä¶\n\n\n3\n56.066667\n\n\n3\n56.066667\n\n\n3\n56.066667\n\n\n3\n56.066667\n\n\n3\n56.066667",
    "crumbs": [
      "Schedule",
      "Week 1",
      "Polars Crash Course"
    ]
  },
  {
    "objectID": "weeks/01/lab/polars-crash-course.html#aggregating-columns-.group_by",
    "href": "weeks/01/lab/polars-crash-course.html#aggregating-columns-.group_by",
    "title": "A Crash Course on Python DataFrames",
    "section": "Aggregating columns: .group_by()",
    "text": "Aggregating columns: .group_by()\nThe .group_by('some_col') context is used for summarizing columns separately by 'some_col'.\n\nYou always follow up a .group_by() with .agg(), and place our expressions inside to tell Polars what should be calculated per group.\nUsing .group_by() will always give you a smaller DataFrame than the original. Specifically you will get back a DataFrame whose rows = number of groups\n\n\nCode\n# start of .agg context\ndf_1.group_by('participant').agg(\n    col('rt').mean(), col('accuracy').mean() # &lt;- expressions like before\n)\n\n\n\nshape: (3, 3)\n\n\n\nparticipant\nrt\naccuracy\n\n\ni64\nf64\nf64\n\n\n\n\n1\n573.523797\n57.8\n\n\n2\n496.969382\n47.2\n\n\n3\n485.294057\n63.2\n\n\n\n\n\n\n\nMaintaining group order\nUnfortunately, by default Polars doesn‚Äôt preserve the order of groups as they exist in the original DataFrame. But we can easily fix this by giving .group_by() and additional argument maintain_order=True:\n\n\nCode\ndf_1.group_by('participant', maintain_order=True).agg(\n    col('rt').mean(), col('accuracy').mean()\n    )\n\n\n\nshape: (3, 3)\n\n\n\nparticipant\nrt\naccuracy\n\n\ni64\nf64\nf64\n\n\n\n\n1\n573.523797\n57.8\n\n\n2\n496.969382\n47.2\n\n\n3\n485.294057\n63.2\n\n\n\n\n\n\n\n\n\n\n\n\nTipYour Turn\n\n\n\nCalculate each participant‚Äôs average reaction time divided by their average accuracy. Remember since there are just 3 unique participants, i.e.¬†3 ‚Äúgroups‚Äù, our result should have 3 rows; one for each participant.\nHint: you can divide 2 columns using the method-chaining style with .truediv() or simply using /\n\n\n\n\nCode\n# Your code here\n\n# Hint: use group_by on 'participant' and then create an expression\n# that divides the average 'rt' by average 'accuracy' and name it 'rt_acc_avg'\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\n\nCode\ndf_1.group_by('participant', maintain_order=True).agg(\n    rt_acc_avg = col('rt').mean() / col('accuracy').mean()\n)\n\n\n\nshape: (3, 2)\n\n\n\nparticipant\nrt_acc_avg\n\n\ni64\nf64\n\n\n\n\n1\n9.922557\n\n\n2\n10.529012\n\n\n3\n7.678703",
    "crumbs": [
      "Schedule",
      "Week 1",
      "Polars Crash Course"
    ]
  },
  {
    "objectID": "weeks/01/lab/polars-crash-course.html#creating-columns-.with_columns",
    "href": "weeks/01/lab/polars-crash-course.html#creating-columns-.with_columns",
    "title": "A Crash Course on Python DataFrames",
    "section": "Creating columns: .with_columns()",
    "text": "Creating columns: .with_columns()\nWhenever we want to return the original DataFrame along with some new columns we can use the .with_columns context instead of .select.\n\nIt will always output the original DataFrame and the outputs of your expressions.\nIf your expression returns just a single value (e.g.¬†the mean of a column), Polars is smart enough to automatically repeat that value over all rows to make sure it fits inside the DataFrame.\n\n\nCode\n# start with_columns context\ndf_1.with_columns(\n    acc_mean=col('accuracy').mean() # &lt;- expression like before\n)\n\n\n\nshape: (15, 4)\n\n\n\nparticipant\naccuracy\nrt\nacc_mean\n\n\ni64\ni64\nf64\nf64\n\n\n\n\n1\n44\n261.28435\n56.066667\n\n\n1\n47\n728.208489\n56.066667\n\n\n1\n64\n801.889016\n56.066667\n\n\n1\n67\n713.555026\n56.066667\n\n\n1\n67\n362.682105\n56.066667\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n3\n70\n439.524973\n56.066667\n\n\n3\n88\n55.11928\n56.066667\n\n\n3\n88\n644.801272\n56.066667\n\n\n3\n12\n571.800553\n56.066667\n\n\n3\n58\n715.224208\n56.066667\n\n\n\n\n\n\nContrast this with the .select context which will only return the mean of accuracy:\n\n\nCode\ndf_1.select(\n    acc_mean=col('accuracy').mean()\n)\n\n\n\nshape: (1, 1)\n\n\n\nacc_mean\n\n\nf64\n\n\n\n\n56.066667\n\n\n\n\n\n\nAs before we can create multiple new columns by including multiple expressions:\n\n\nCode\ndf_1.with_columns(\n    acc_mean=col('accuracy').mean(),\n    rt_scaled=col('rt') / 100\n    )\n\n\n\nshape: (15, 5)\n\n\n\nparticipant\naccuracy\nrt\nacc_mean\nrt_scaled\n\n\ni64\ni64\nf64\nf64\nf64\n\n\n\n\n1\n44\n261.28435\n56.066667\n2.612843\n\n\n1\n47\n728.208489\n56.066667\n7.282085\n\n\n1\n64\n801.889016\n56.066667\n8.01889\n\n\n1\n67\n713.555026\n56.066667\n7.13555\n\n\n1\n67\n362.682105\n56.066667\n3.626821\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n3\n70\n439.524973\n56.066667\n4.39525\n\n\n3\n88\n55.11928\n56.066667\n0.551193\n\n\n3\n88\n644.801272\n56.066667\n6.448013\n\n\n3\n12\n571.800553\n56.066667\n5.718006\n\n\n3\n58\n715.224208\n56.066667\n7.152242\n\n\n\n\n\n\n\n\n\n\n\n\nTipUsing .over() to perform Tidy group-by operations\n\n\n\nA very handy use for .with_columns is to combine it with the .over() operation.\nThis allows us to calculate an expression separately by group, but then save the results into a DataFrame the same size as the original.\nFor example, Polars will keep the tidy-format of the data and correctly repeat the values across rows.\n\n\n\n\nCode\ndf_1.with_columns(\n    acc_mean=col('accuracy').mean().over('participant') # &lt;- chaining .over() handles grouping!\n)\n\n\n\nshape: (15, 4)\n\n\n\nparticipant\naccuracy\nrt\nacc_mean\n\n\ni64\ni64\nf64\nf64\n\n\n\n\n1\n44\n261.28435\n57.8\n\n\n1\n47\n728.208489\n57.8\n\n\n1\n64\n801.889016\n57.8\n\n\n1\n67\n713.555026\n57.8\n\n\n1\n67\n362.682105\n57.8\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n3\n70\n439.524973\n63.2\n\n\n3\n88\n55.11928\n63.2\n\n\n3\n88\n644.801272\n63.2\n\n\n3\n12\n571.800553\n63.2\n\n\n3\n58\n715.224208\n63.2\n\n\n\n\n\n\nChaining .over('some_col') to any expression is like using .group_by but preserving the shape of the original DataFrame:\n\n\nCode\ndf_1.with_columns(\n    acc_mean=col('accuracy').mean().over('participant'),\n    rt_mean=col('rt').mean().over('participant')\n)\n\n\n\nshape: (15, 5)\n\n\n\nparticipant\naccuracy\nrt\nacc_mean\nrt_mean\n\n\ni64\ni64\nf64\nf64\nf64\n\n\n\n\n1\n44\n261.28435\n57.8\n573.523797\n\n\n1\n47\n728.208489\n57.8\n573.523797\n\n\n1\n64\n801.889016\n57.8\n573.523797\n\n\n1\n67\n713.555026\n57.8\n573.523797\n\n\n1\n67\n362.682105\n57.8\n573.523797\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n3\n70\n439.524973\n63.2\n485.294057\n\n\n3\n88\n55.11928\n63.2\n485.294057\n\n\n3\n88\n644.801272\n63.2\n485.294057\n\n\n3\n12\n571.800553\n63.2\n485.294057\n\n\n3\n58\n715.224208\n63.2\n485.294057\n\n\n\n\n\n\nRemember that the .group_by() context will always return a smaller aggregated DataFrame:\n\n\nCode\ndf_1.group_by('participant', maintain_order=True).agg(\n    acc_mean=col('accuracy').mean(),\n    rt_mean=col('rt').mean()\n)\n\n\n\nshape: (3, 3)\n\n\n\nparticipant\nacc_mean\nrt_mean\n\n\ni64\nf64\nf64\n\n\n\n\n1\n57.8\n573.523797\n\n\n2\n47.2\n496.969382\n\n\n3\n63.2\n485.294057\n\n\n\n\n\n\nIn Polars you should only rely on .group_by if you know for sure that you want your output to be smaller than your original DataFrame - and by smaller we mean rows = number of groups.\n\n\n\n\n\n\nTipYour Turn\n\n\n\nCreate a DataFrame that adds 3 new columns:\n\nAccuracy on a 0-1 scale\nRT / Accuracy\nRT / max RT, separately using each participant‚Äôs max RT\n\n\n\n\n\nCode\n# Your code here\n\n\n# Hint: you can wrap an entire expression in () and use .over()\n# on the entire wrapped expression to do things like\n# add, subtract columns multiple columns by \"participant\"\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\n\nCode\ndf_1.with_columns(\n    acc_scaled = col('accuracy') / 100,\n    rt_acc = col('rt') / col('accuracy'),\n    rt_max_scaled = (col('rt') / col('rt').max()).over('participant')\n)\n\n\n\nshape: (15, 6)\n\n\n\nparticipant\naccuracy\nrt\nacc_scaled\nrt_acc\nrt_max_scaled\n\n\ni64\ni64\nf64\nf64\nf64\nf64\n\n\n\n\n1\n44\n261.28435\n0.44\n5.938281\n0.325836\n\n\n1\n47\n728.208489\n0.47\n15.493798\n0.908116\n\n\n1\n64\n801.889016\n0.64\n12.529516\n1.0\n\n\n1\n67\n713.555026\n0.67\n10.650075\n0.889843\n\n\n1\n67\n362.682105\n0.67\n5.413166\n0.452285\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n3\n70\n439.524973\n0.7\n6.278928\n0.614528\n\n\n3\n88\n55.11928\n0.88\n0.626355\n0.077066\n\n\n3\n88\n644.801272\n0.88\n7.327287\n0.901537\n\n\n3\n12\n571.800553\n0.12\n47.650046\n0.79947\n\n\n3\n58\n715.224208\n0.58\n12.331452\n1.0",
    "crumbs": [
      "Schedule",
      "Week 1",
      "Polars Crash Course"
    ]
  },
  {
    "objectID": "weeks/01/lab/polars-crash-course.html#selecting-rows-.filter",
    "href": "weeks/01/lab/polars-crash-course.html#selecting-rows-.filter",
    "title": "A Crash Course on Python DataFrames",
    "section": "Selecting rows: .filter()",
    "text": "Selecting rows: .filter()\nThe .filter context is used for sub-setting rows using a logical expression.\n\nInstead of returning one or more values like other expressions, a logical expression returns True/False values that we can use to filter rows that mean those criteria:\n\n\nCode\n# start filter context\ndf_1.filter(\n    col('participant') == 1 # &lt;- expression like before\n)\n\n\n\nshape: (5, 3)\n\n\n\nparticipant\naccuracy\nrt\n\n\ni64\ni64\nf64\n\n\n\n\n1\n44\n261.28435\n\n\n1\n47\n728.208489\n\n\n1\n64\n801.889016\n\n\n1\n67\n713.555026\n\n\n1\n67\n362.682105\n\n\n\n\n\n\nOr in Polars methods-style using .eq():\n\n\nCode\ndf_1.filter(\n    col('participant').eq(1)\n)\n\n\n\nshape: (5, 3)\n\n\n\nparticipant\naccuracy\nrt\n\n\ni64\ni64\nf64\n\n\n\n\n1\n44\n261.28435\n\n\n1\n47\n728.208489\n\n\n1\n64\n801.889016\n\n\n1\n67\n713.555026\n\n\n1\n67\n362.682105\n\n\n\n\n\n\nOr even the opposite: we can negate or invert any logical expression by putting a ~ in front of it.\nThis is like using not in regular Python or ! in some other languages.\n\n\nCode\ndf_1.filter(\n    ~col('participant').eq(1)\n)\n\n\n\nshape: (10, 3)\n\n\n\nparticipant\naccuracy\nrt\n\n\ni64\ni64\nf64\n\n\n\n\n2\n9\n502.974663\n\n\n2\n83\n424.866821\n\n\n2\n21\n492.355273\n\n\n2\n36\n573.594895\n\n\n2\n87\n491.05526\n\n\n3\n70\n439.524973\n\n\n3\n88\n55.11928\n\n\n3\n88\n644.801272\n\n\n3\n12\n571.800553\n\n\n3\n58\n715.224208\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nBut be careful. If you‚Äôre not using the method-chaining style then you need to wrap you expression in () before using ~:\n\n\n\n\nCode\ndf_1.filter(\n    ~(col('participant') == 1)   # &lt;- notice extra () around expression\n)\n\n\n\nshape: (10, 3)\n\n\n\nparticipant\naccuracy\nrt\n\n\ni64\ni64\nf64\n\n\n\n\n2\n9\n502.974663\n\n\n2\n83\n424.866821\n\n\n2\n21\n492.355273\n\n\n2\n36\n573.594895\n\n\n2\n87\n491.05526\n\n\n3\n70\n439.524973\n\n\n3\n88\n55.11928\n\n\n3\n88\n644.801272\n\n\n3\n12\n571.800553\n\n\n3\n58\n715.224208\n\n\n\n\n\n\nJust like in with other contexts (i.e.¬†.select, .with_columns, .group_by) we can using multiple logical expressions to refine our filtering criteria.\nIf we use , Polars treats them logically as an and statement. For example, we use 2 logical expressions to filter where: participant is 1 AND accuracy is 67:\n\n\nCode\ndf_1.filter(\n    col('participant').eq(1),\n    col('accuracy').eq(67)\n)\n\n\n\nshape: (2, 3)\n\n\n\nparticipant\naccuracy\nrt\n\n\ni64\ni64\nf64\n\n\n\n\n1\n67\n713.555026\n\n\n1\n67\n362.682105\n\n\n\n\n\n\nBut you might find it clearer to use & for and expressions:\n\n\nCode\ndf_1.filter(\n    col('participant').eq(1) & col('accuracy').eq(67)\n)\n\n\n\nshape: (2, 3)\n\n\n\nparticipant\naccuracy\nrt\n\n\ni64\ni64\nf64\n\n\n\n\n1\n67\n713.555026\n\n\n1\n67\n362.682105\n\n\n\n\n\n\nThe | operator can be used for or expressions:\n\n\nCode\ndf_1.filter(\n    col('participant').eq(1) | col('participant').eq(3)\n)\n\n\n\nshape: (10, 3)\n\n\n\nparticipant\naccuracy\nrt\n\n\ni64\ni64\nf64\n\n\n\n\n1\n44\n261.28435\n\n\n1\n47\n728.208489\n\n\n1\n64\n801.889016\n\n\n1\n67\n713.555026\n\n\n1\n67\n362.682105\n\n\n3\n70\n439.524973\n\n\n3\n88\n55.11928\n\n\n3\n88\n644.801272\n\n\n3\n12\n571.800553\n\n\n3\n58\n715.224208\n\n\n\n\n\n\nTo combine more complicated logical expressions, you can wrap them in ().\nBelow we get rows where participant 1‚Äôs accuracy is 67 OR any of participant 2‚Äôs rows:\n\n\nCode\ndf_1.filter(\n    col('participant').eq(1) & col('accuracy').eq(67) | col('participant').eq(2)\n)\n\n\n\nshape: (7, 3)\n\n\n\nparticipant\naccuracy\nrt\n\n\ni64\ni64\nf64\n\n\n\n\n1\n67\n713.555026\n\n\n1\n67\n362.682105\n\n\n2\n9\n502.974663\n\n\n2\n83\n424.866821\n\n\n2\n21\n492.355273\n\n\n2\n36\n573.594895\n\n\n2\n87\n491.05526\n\n\n\n\n\n\n\n\n\n\n\n\nNoteTwo styles of logical expressions\n\n\n\nLike renaming the outputs of an expression, Polars gives us 2 styles we can use to combine logical expressions.\nWe‚Äôve seen the first one using & and |.\nThe second one uses the method-chaining style with the . syntax. Here Polars provides a .and_() and a .or_() method.\ndf_1.filter(\n    col('participant').eq(1).and_(\n        col('accuracy').eq(67)).or_(\n            col('participant').eq(2)\n        )\n)\nFeel free to use which every style you find more intuitive and readable:",
    "crumbs": [
      "Schedule",
      "Week 1",
      "Polars Crash Course"
    ]
  },
  {
    "objectID": "weeks/01/lab/polars-crash-course.html#expressions-are-for-performing-operations",
    "href": "weeks/01/lab/polars-crash-course.html#expressions-are-for-performing-operations",
    "title": "A Crash Course on Python DataFrames",
    "section": "Expressions are for performing operations",
    "text": "Expressions are for performing operations\nSo far we‚Äôve see how to build up an expression that computes some value, e.g.¬†.mean() or performs some logic, e.g.¬†.eq().\nPolars calls these computations operations and include tons of them (accessible via . syntax). Some of the notable ones include:\nArithmetic, e.g.¬†.add, .sub, .mul\nBoolean, e.g.¬†.all, .any, .is_null, .is_not_null\nSummary (aggregate) stats, e.g.¬†.mean, .median, .std, .count\nComparison, e.g.¬†.gt, .lt, .gte, .lte, .eq, .ne.\n\n\n\n\n\n\nTipYour Turn\n\n\n\nUse the linked documentation and contexts you learned about above to complete the following exercises:\n\n\n1. Select the accuracy and RT columns from df_1 and multiply them by 10\n\n\nCode\n# Your code here\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\n\nCode\ndf_1.select(col('accuracy', 'rt') * 10)\n\n\n\nshape: (15, 2)\n\n\n\naccuracy\nrt\n\n\ni64\nf64\n\n\n\n\n440\n2612.843496\n\n\n470\n7282.084892\n\n\n640\n8018.890165\n\n\n670\n7135.550259\n\n\n670\n3626.821046\n\n\n‚Ä¶\n‚Ä¶\n\n\n700\n4395.249735\n\n\n880\n551.192796\n\n\n880\n6448.012718\n\n\n120\n5718.005529\n\n\n580\n7152.242075\n\n\n\n\n\n\n\n\n\n2. Add 2 new columns to the DataFrame: rt_acc and acc_max_scaled\nFor rt_acc divide reaction time by accuracy.\nFor acc_max_scaled divide accuracy by maximum accuracy, separately by participant.\n\n\nCode\n# Your code here\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\n\nCode\ndf_1.with_columns(\n    rt_acc = col('rt') / col('accuracy'),\n    acc_max_scaled = (col('accuracy') / col('accuracy').max()).over('participant')\n)\n\n\n\nshape: (15, 5)\n\n\n\nparticipant\naccuracy\nrt\nrt_acc\nacc_max_scaled\n\n\ni64\ni64\nf64\nf64\nf64\n\n\n\n\n1\n44\n261.28435\n5.938281\n0.656716\n\n\n1\n47\n728.208489\n15.493798\n0.701493\n\n\n1\n64\n801.889016\n12.529516\n0.955224\n\n\n1\n67\n713.555026\n10.650075\n1.0\n\n\n1\n67\n362.682105\n5.413166\n1.0\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n3\n70\n439.524973\n6.278928\n0.795455\n\n\n3\n88\n55.11928\n0.626355\n1.0\n\n\n3\n88\n644.801272\n7.327287\n1.0\n\n\n3\n12\n571.800553\n47.650046\n0.136364\n\n\n3\n58\n715.224208\n12.331452\n0.659091\n\n\n\n\n\n\n\n\n\n3. Filter rows where reaction time is &gt; 100ms and &lt; 725ms\n\n\nCode\n# Your code here\n\n# Hint: You should write a logical expression\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\n\nCode\ndf_1.filter(\n    (col('rt') &gt; 100) & (col('rt') &lt; 725)\n)\n\n\n\nshape: (12, 3)\n\n\n\nparticipant\naccuracy\nrt\n\n\ni64\ni64\nf64\n\n\n\n\n1\n44\n261.28435\n\n\n1\n67\n713.555026\n\n\n1\n67\n362.682105\n\n\n2\n9\n502.974663\n\n\n2\n83\n424.866821\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n2\n87\n491.05526\n\n\n3\n70\n439.524973\n\n\n3\n88\n644.801272\n\n\n3\n12\n571.800553\n\n\n3\n58\n715.224208",
    "crumbs": [
      "Schedule",
      "Week 1",
      "Polars Crash Course"
    ]
  },
  {
    "objectID": "weeks/01/lab/polars-crash-course.html#saving-re-usable-expressions",
    "href": "weeks/01/lab/polars-crash-course.html#saving-re-usable-expressions",
    "title": "A Crash Course on Python DataFrames",
    "section": "Saving re-usable expressions",
    "text": "Saving re-usable expressions\nWhen you find yourself creating complex expressions that you want to re-use later on, perhaps across other DataFrames, you can save them as re-usable functions!\nFor example, Polars doesn‚Äôt include an operation to calculate a z-score by default. But we know how to do that manually. So let‚Äôs create a function called scale that defines an expression we can reuse.\n\n\nCode\ndef scale(column_name):\n    \"\"\"Reminder:\n        z-score = (x - x.mean() / x.std())\n    \"\"\"\n    return (col(column_name) - col(column_name).mean()) / col(column_name).std()\n\n\nThis is a function that accepts a single argument column_name, and then uses the col selector from Polars to select a column and calculate its z-score.\nWe can now use this expression in any context saves us a ton of typing and typos!\nFor example just across all accuracy scores:\n\n\nCode\ndf_1.select(\n    acc_z=scale('accuracy')\n)\n\n\n\nshape: (15, 1)\n\n\n\nacc_z\n\n\nf64\n\n\n\n\n-0.446194\n\n\n-0.335262\n\n\n0.293354\n\n\n0.404287\n\n\n0.404287\n\n\n‚Ä¶\n\n\n0.515219\n\n\n1.180812\n\n\n1.180812\n\n\n-1.629472\n\n\n0.07149\n\n\n\n\n\n\nOr as a more realistic example: z-score separately by participant\nThis is a great example of where .with_columns + .over() can come in super-handy.\nBecause our function returns an expression we can call operations on it just like any other expression:\n\n\nCode\n# .over() works with our scale() function\n# because it return a Polars expression!\ndf_1.with_columns(\n    acc_z=scale('accuracy').over('participant'),\n    rt_z=scale('rt').over('participant')\n)\n\n\n\nshape: (15, 5)\n\n\n\nparticipant\naccuracy\nrt\nacc_z\nrt_z\n\n\ni64\ni64\nf64\nf64\nf64\n\n\n\n\n1\n44\n261.28435\n-1.216438\n-1.281041\n\n\n1\n47\n728.208489\n-0.951995\n0.634633\n\n\n1\n64\n801.889016\n0.546515\n0.936926\n\n\n1\n67\n713.555026\n0.810958\n0.574513\n\n\n1\n67\n362.682105\n0.810958\n-0.865031\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n3\n70\n439.524973\n0.217085\n-0.175214\n\n\n3\n88\n55.11928\n0.791722\n-1.646805\n\n\n3\n88\n644.801272\n0.791722\n0.610629\n\n\n3\n12\n571.800553\n-1.634524\n0.331166\n\n\n3\n58\n715.224208\n-0.166006\n0.880224\n\n\n\n\n\n\nThis is entirely equivalent to the following code, but so much easier to read and so much less potential for errors when typing:\n\n\nCode\ndf_1.with_columns(\n    acc_z=((col('accuracy') - col('accuracy').mean()) / col('accuracy').std()).over('participant'), rt_z=((col('rt') - col('rt').mean()) / col('rt').std()).over('participant')\n)\n\n\n\nshape: (15, 5)\n\n\n\nparticipant\naccuracy\nrt\nacc_z\nrt_z\n\n\ni64\ni64\nf64\nf64\nf64\n\n\n\n\n1\n44\n261.28435\n-1.216438\n-1.281041\n\n\n1\n47\n728.208489\n-0.951995\n0.634633\n\n\n1\n64\n801.889016\n0.546515\n0.936926\n\n\n1\n67\n713.555026\n0.810958\n0.574513\n\n\n1\n67\n362.682105\n0.810958\n-0.865031\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n3\n70\n439.524973\n0.217085\n-0.175214\n\n\n3\n88\n55.11928\n0.791722\n-1.646805\n\n\n3\n88\n644.801272\n0.791722\n0.610629\n\n\n3\n12\n571.800553\n-1.634524\n0.331166\n\n\n3\n58\n715.224208\n-0.166006\n0.880224\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nAs you‚Äôre thinking about how to manipulate data, think about saving an expression you find yourself using a lot as function! In fact Python as a short-hand alternative to def for creating simple one-line functions: lambda\nmyfunc = lambda param1: print(param1)\nWe can rewrite the function above as a lambda expression like this:\nscale = lambda column_name: (col(column_name) - col(column_name).mean()) / col(column_name).std()\nYou‚Äôll often see this in Python code when people are defining and using functions within some other code.\n\n\n\n\n\n\n\n\nTipYour Turn\n\n\n\nCreate a Polars expression that mean-centers a column. You can use def or lambda whatever feels more comfortable right now\n\n\n\n\nCode\n# Your code here\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\n\nCode\ndef mean_center(column_name):\n    return col(column_name) - col(column_name).mean()\n\n# Or with lambda:\n# mean_center = lambda column_name: col(column_name) - col(column_name).mean()\n\n\n\n\n\nAdd 2 new columns to the df_1 DataFrame that include mean-centered accuracy, and mean-centered RT\n\n\nCode\n# Your code here\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\n\nCode\ndf_1.with_columns(\n    acc_centered = mean_center('accuracy'),\n    rt_centered = mean_center('rt')\n)\n\n\n\nshape: (15, 5)\n\n\n\nparticipant\naccuracy\nrt\nacc_centered\nrt_centered\n\n\ni64\ni64\nf64\nf64\nf64\n\n\n\n\n1\n44\n261.28435\n-12.066667\n-257.311396\n\n\n1\n47\n728.208489\n-9.066667\n209.612744\n\n\n1\n64\n801.889016\n7.933333\n283.293271\n\n\n1\n67\n713.555026\n10.933333\n194.95928\n\n\n1\n67\n362.682105\n10.933333\n-155.913641\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n3\n70\n439.524973\n13.933333\n-79.070772\n\n\n3\n88\n55.11928\n31.933333\n-463.476466\n\n\n3\n88\n644.801272\n31.933333\n126.205526\n\n\n3\n12\n571.800553\n-44.066667\n53.204807\n\n\n3\n58\n715.224208\n1.933333\n196.628462",
    "crumbs": [
      "Schedule",
      "Week 1",
      "Polars Crash Course"
    ]
  },
  {
    "objectID": "weeks/01/lab/polars-crash-course.html#more-complex-expression-with-functions-when-and-lit",
    "href": "weeks/01/lab/polars-crash-course.html#more-complex-expression-with-functions-when-and-lit",
    "title": "A Crash Course on Python DataFrames",
    "section": "More complex expression with functions: when and lit",
    "text": "More complex expression with functions: when and lit\nPolars also offers a few other operations as functions you can use inside of a context for building expressions.\nThese are called as pl.something() but we can also directly import them.\nYou should check out the documentation to see what‚Äôs possible, but two common ones you‚Äôre likely to use are pl.when and pl.lit\n\n\nCode\n# Directly import them to make life easier\nfrom polars import when, lit\n\n\nwhen lets you run an if-else statement as an expression, which is particularly useful for creating new columns based on the values in another column.\nlit works in conjunction with when to tell Polars to use the literal value of something rather than try to find a corresponding column name:\nLet‚Äôs use them together to create a new column that splits participant responses that were faster and slower than 300ms:\nWe‚Äôll use the .with_columns context, because we want the result of our expression (the new column) and the original DataFrame:\n\n\nCode\n# Create a new column rt_split that contains the result of the following if/else statement:\n# If RT &gt;= 300, set the value to the lit(eral) string 'slow'\n# Otherwise, set the value to the lit(eral) string 'fast'\n\n# Start with_columns context\nddf = df_1.with_columns(\n    rt_split=when(\n            col('rt') &gt;= 300).then(lit('slow')).otherwise(lit('fast') # expression inside function\n        )\n    )\n# We saved the output to a new variable called ddf\nddf\n\n\n\nshape: (15, 4)\n\n\n\nparticipant\naccuracy\nrt\nrt_split\n\n\ni64\ni64\nf64\nstr\n\n\n\n\n1\n44\n261.28435\n\"fast\"\n\n\n1\n47\n728.208489\n\"slow\"\n\n\n1\n64\n801.889016\n\"slow\"\n\n\n1\n67\n713.555026\n\"slow\"\n\n\n1\n67\n362.682105\n\"slow\"\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n3\n70\n439.524973\n\"slow\"\n\n\n3\n88\n55.11928\n\"fast\"\n\n\n3\n88\n644.801272\n\"slow\"\n\n\n3\n12\n571.800553\n\"slow\"\n\n\n3\n58\n715.224208\n\"slow\"\n\n\n\n\n\n\n\n\n\n\n\n\nTipYour Turn\n\n\n\nUse when and lit to add a column to the DataFrame called performance.\nIt should contain the string ‚Äòsuccess‚Äô if accuracy &gt;= 50, or ‚Äòfail‚Äô if it was &lt; 50.\nSave the result to a new dataframe called df_new and print the first 10 rows:\n\n\n\n\nCode\n# Your code here\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\n\nCode\ndf_new = df_1.with_columns(\n    performance = when(col('accuracy') &gt;= 50).then(lit('success')).otherwise(lit('fail'))\n)\ndf_new.head(10)\n\n\n\nshape: (10, 4)\n\n\n\nparticipant\naccuracy\nrt\nperformance\n\n\ni64\ni64\nf64\nstr\n\n\n\n\n1\n44\n261.28435\n\"fail\"\n\n\n1\n47\n728.208489\n\"fail\"\n\n\n1\n64\n801.889016\n\"success\"\n\n\n1\n67\n713.555026\n\"success\"\n\n\n1\n67\n362.682105\n\"success\"\n\n\n2\n9\n502.974663\n\"fail\"\n\n\n2\n83\n424.866821\n\"success\"\n\n\n2\n21\n492.355273\n\"fail\"\n\n\n2\n36\n573.594895\n\"fail\"\n\n\n2\n87\n491.05526\n\"success\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipYour Turn\n\n\n\nUsing the previous DataFrame (df_new), summarize how many successes and failures each participant had.\nYour result should have 6 rows: 2 for each participant\n\n\n\n\nCode\n# Your code here\n\n\n# Hint: you can group_by multiple columns by passing a list of column names, e.g.\n\n# df_new.group_by(['col_1', 'col_2']).agg(...)\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\n\nCode\n# First create df_new if it wasn't created above\ndf_new = df_1.with_columns(\n    performance = when(col('accuracy') &gt;= 50).then(lit('success')).otherwise(lit('fail'))\n)\n\ndf_new.group_by(['participant', 'performance'], maintain_order=True).agg(\n    count = col('accuracy').count()\n)\n\n\n\nshape: (6, 3)\n\n\n\nparticipant\nperformance\ncount\n\n\ni64\nstr\nu32\n\n\n\n\n1\n\"fail\"\n2\n\n\n1\n\"success\"\n3\n\n\n2\n\"fail\"\n3\n\n\n2\n\"success\"\n2\n\n\n3\n\"success\"\n4\n\n\n3\n\"fail\"\n1",
    "crumbs": [
      "Schedule",
      "Week 1",
      "Polars Crash Course"
    ]
  },
  {
    "objectID": "weeks/01/lab/polars-crash-course.html#more-complex-expressions-with-attribute-type-operations",
    "href": "weeks/01/lab/polars-crash-course.html#more-complex-expressions-with-attribute-type-operations",
    "title": "A Crash Course on Python DataFrames",
    "section": "More complex expressions with attribute (type) operations",
    "text": "More complex expressions with attribute (type) operations\nIn addition to importing functions to build more complicated expressions, Polars also allows you to perform specific operations based upon the type of data in a column.\nYou don‚Äôt need to import anything to use these. Instead, you can use . syntax to ‚Äúnarrow down‚Äù to the type of data attribute you want, and then select the operations you would like.\nFor example, we‚Äôll use the DataFrame we created in the previous section, ddf:\n\n\nCode\nddf.head()\n\n\n\nshape: (5, 4)\n\n\n\nparticipant\naccuracy\nrt\nrt_split\n\n\ni64\ni64\nf64\nstr\n\n\n\n\n1\n44\n261.28435\n\"fast\"\n\n\n1\n47\n728.208489\n\"slow\"\n\n\n1\n64\n801.889016\n\"slow\"\n\n\n1\n67\n713.555026\n\"slow\"\n\n\n1\n67\n362.682105\n\"slow\"\n\n\n\n\n\n\nTo create an expression that converts each value in the new ‚Äúrt_split‚Äù column to uppercase.\nWe can do this by selecting with col() as usual, but then before calling an operation with . like before, we first access the .str attribute, and then call operations that specifically operate on strings!\n\n\nCode\nddf.with_columns(\n    col('rt_split').str.to_uppercase() # .uppercase() is only available to str data!\n)\n\n\n\nshape: (15, 4)\n\n\n\nparticipant\naccuracy\nrt\nrt_split\n\n\ni64\ni64\nf64\nstr\n\n\n\n\n1\n44\n261.28435\n\"FAST\"\n\n\n1\n47\n728.208489\n\"SLOW\"\n\n\n1\n64\n801.889016\n\"SLOW\"\n\n\n1\n67\n713.555026\n\"SLOW\"\n\n\n1\n67\n362.682105\n\"SLOW\"\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n3\n70\n439.524973\n\"SLOW\"\n\n\n3\n88\n55.11928\n\"FAST\"\n\n\n3\n88\n644.801272\n\"SLOW\"\n\n\n3\n12\n571.800553\n\"SLOW\"\n\n\n3\n58\n715.224208\n\"SLOW\"\n\n\n\n\n\n\nWithout .str to ‚Äúnarrow-in‚Äù to the string attribute Polars will complain about an AttributeError, because only str types have a .to_uppercase() operation!\n\n\nCode\nddf.with_columns(\n    col('rt_split').to_uppercase() # no .str\n)\n\n\n\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nCell In[67], line 2\n      1 ddf.with_columns(\n----&gt; 2     col('rt_split').to_uppercase() # no .str\n      3 )\n\nAttributeError: 'Expr' object has no attribute 'to_uppercase'\n\n\n\nPolars includes many attribute operations. The most common ones you‚Äôll use are for working with:\n.str: if your data are strings\n.name: which allows you to quickly change the names of columns from within a more complicated expression.\n.list: if your columns contain Python lists\nFor example, below we using a single expression inside the with_columns context below to calculate the mean of the accuracy and rt columns.\n\n\nCode\ndf_1.with_columns(col('accuracy', 'rt').mean())\n\n\n\nshape: (15, 3)\n\n\n\nparticipant\naccuracy\nrt\n\n\ni64\nf64\nf64\n\n\n\n\n1\n56.066667\n518.595745\n\n\n1\n56.066667\n518.595745\n\n\n1\n56.066667\n518.595745\n\n\n1\n56.066667\n518.595745\n\n\n1\n56.066667\n518.595745\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n3\n56.066667\n518.595745\n\n\n3\n56.066667\n518.595745\n\n\n3\n56.066667\n518.595745\n\n\n3\n56.066667\n518.595745\n\n\n3\n56.066667\n518.595745\n\n\n\n\n\n\nBecause we‚Äôre using .with_columns, the output of our expression is overwriting the original values in the accuracy and rt columns.\nWe saw how to rename output when we had separate col('accuracy').mean() and col('rt').mean() expressions: using .alias() at the end or = at the beginning.\nBut how do we change the names of both columns at the same time?\nAccessing the .name attribute gives us access to additional operations that help us out. In this case we use the .suffix() operation to add a suffix to the output name(s).\n\n\nCode\ndf_1.with_columns(\n    col('accuracy', 'rt').mean().name.suffix('_mean')\n)\n\n\n\nshape: (15, 5)\n\n\n\nparticipant\naccuracy\nrt\naccuracy_mean\nrt_mean\n\n\ni64\ni64\nf64\nf64\nf64\n\n\n\n\n1\n44\n261.28435\n56.066667\n518.595745\n\n\n1\n47\n728.208489\n56.066667\n518.595745\n\n\n1\n64\n801.889016\n56.066667\n518.595745\n\n\n1\n67\n713.555026\n56.066667\n518.595745\n\n\n1\n67\n362.682105\n56.066667\n518.595745\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n3\n70\n439.524973\n56.066667\n518.595745\n\n\n3\n88\n55.11928\n56.066667\n518.595745\n\n\n3\n88\n644.801272\n56.066667\n518.595745\n\n\n3\n12\n571.800553\n56.066667\n518.595745\n\n\n3\n58\n715.224208\n56.066667\n518.595745\n\n\n\n\n\n\nNow we have the original accuracy and rt columns and the newly named ones we created!",
    "crumbs": [
      "Schedule",
      "Week 1",
      "Polars Crash Course"
    ]
  },
  {
    "objectID": "weeks/01/lab/polars-crash-course.html#building-expressions-from-additional-selectors",
    "href": "weeks/01/lab/polars-crash-course.html#building-expressions-from-additional-selectors",
    "title": "A Crash Course on Python DataFrames",
    "section": "Building expressions from additional selectors",
    "text": "Building expressions from additional selectors\nSo far we‚Äôve seen how to use col() to select 1 or more columns we want to create an expression about.\nBut sometimes you need to select things in more complicated ways. Fortunately, Polars has additional selectors that we can use to express ourselves. A common pattern is to import these together using as:\nfrom polars import selectors as cs\nThen we can refer to these using cs.some_selector(). Some of these include:\n\ncs.all()\ncs.exclude()\ncs.starts_with()\ncs.string()\n\nLet‚Äôs see some of these in action using a dataset that include a column of reaction times:\n\n\nCode\nimport polars.selectors as cs\n\ndf_1.select(cs.all().count())  # &lt;- get all cols and calc count()\n\n\n\nshape: (1, 3)\n\n\n\nparticipant\naccuracy\nrt\n\n\nu32\nu32\nu32\n\n\n\n\n15\n15\n15\n\n\n\n\n\n\nThis is as the same as the following code, but many fewer lines!\n\n\nCode\ndf_1.select(\n    col('participant').count(),\n    col('accuracy').count(), col('rt').count()\n)\n\n\n\nshape: (1, 3)\n\n\n\nparticipant\naccuracy\nrt\n\n\nu32\nu32\nu32\n\n\n\n\n15\n15\n15\n\n\n\n\n\n\nAnd cs.exclude is the opposite of cs.all()\n\n\nCode\ndf_1.select(cs.exclude('participant').mean())  # &lt;- all cols except participant\n\n\n\nshape: (1, 2)\n\n\n\naccuracy\nrt\n\n\nf64\nf64\n\n\n\n\n56.066667\n518.595745\n\n\n\n\n\n\nWe can select all columns that start with certain characters:\n\n\nCode\ndf_1.select(cs.starts_with('pa').n_unique())\n\n\n\nshape: (1, 1)\n\n\n\nparticipant\n\n\nu32\n\n\n\n\n3\n\n\n\n\n\n\nOr even select columns based on the type of data they contain. In this case all the columns with Integer data:\n\n\nCode\ndf_1.select(cs.integer())\n\n\n\nshape: (15, 2)\n\n\n\nparticipant\naccuracy\n\n\ni64\ni64\n\n\n\n\n1\n44\n\n\n1\n47\n\n\n1\n64\n\n\n1\n67\n\n\n1\n67\n\n\n‚Ä¶\n‚Ä¶\n\n\n3\n70\n\n\n3\n88\n\n\n3\n88\n\n\n3\n12\n\n\n3\n58\n\n\n\n\n\n\nThere are a many more useful selectors. So check out the selector documentation page when you‚Äôre trying the challenge exercises later on in this notebook",
    "crumbs": [
      "Schedule",
      "Week 1",
      "Polars Crash Course"
    ]
  },
  {
    "objectID": "weeks/01/lab/polars-crash-course.html#reshaping-dataframes",
    "href": "weeks/01/lab/polars-crash-course.html#reshaping-dataframes",
    "title": "A Crash Course on Python DataFrames",
    "section": "Reshaping DataFrames",
    "text": "Reshaping DataFrames\nSometimes you‚Äôll find yourself working ‚Äúnon-tidy‚Äù DataFrames or ‚Äúwide‚Äù format data.\nWhat‚Äôs tidy-data again?\n\nEach variable must have its own column.\nEach observation must have its own row.\nEach value must have its own cell.\n\nIn polars we can achieve this using:\n.pivot(): long -&gt; wide, similar to pivot_wider() in R .unpivot(): wide -&gt; long, similar to pivot_longer() in R pl.concat(): combine 2 or more DataFrames/columns/rows into a larger DataFrame\nHere, I‚Äôve generated data from two participants with three observations. This data frame is not tidy since each row contains more than a single observation.\n\n\nCode\ndf_2 = pl.DataFrame(\n        {'participant': [1, 2],\n        'observation_1': [10, 25],\n        'observation_2': [100, 63],\n        'observation_3': [24, 45]\n        }\n    )\ndf_2\n\n\n\nshape: (2, 4)\n\n\n\nparticipant\nobservation_1\nobservation_2\nobservation_3\n\n\ni64\ni64\ni64\ni64\n\n\n\n\n1\n10\n100\n24\n\n\n2\n25\n63\n45\n\n\n\n\n\n\nWe can make it tidy by using the .unpivot method on the DataFrame, which takes 4 arguments:\non: the column(s) that contain values for each row index: the column(s) to use as the identifier across rows variable_name: name of the column that contains the original column names value_name: name of the column that contains the values that were previous spread across columns\n\n\nCode\n# Just breaking up over lines to keep it readable!\ndf_long = df_2.unpivot(\n    on=cs.starts_with('observation'),\n    index='participant',\n    variable_name='trial',\n    value_name='rating'\n    )\ndf_long\n\n\n\nshape: (6, 3)\n\n\n\nparticipant\ntrial\nrating\n\n\ni64\nstr\ni64\n\n\n\n\n1\n\"observation_1\"\n10\n\n\n2\n\"observation_1\"\n25\n\n\n1\n\"observation_2\"\n100\n\n\n2\n\"observation_2\"\n63\n\n\n1\n\"observation_3\"\n24\n\n\n2\n\"observation_3\"\n45\n\n\n\n\n\n\nThe .pivot method is the counter-part of .unpivot. We can use it to turn tidydata (long) to wide format. It takes 4 arguments as well:\non: the column(s) whose values will be turned into new columns index: the column(s) that are unique rows in the new DataFrame values: the values that will be moved into new columns with each row aggregate_function: how to aggregate multiple rows within each index, e.g.¬†None, mean, first, sum, etc\n\n\nCode\ndf_long.pivot(\n    on='trial',\n    index='participant',\n    values='rating',\n    aggregate_function=None\n    )\n\n\n\nshape: (2, 4)\n\n\n\nparticipant\nobservation_1\nobservation_2\nobservation_3\n\n\ni64\ni64\ni64\ni64\n\n\n\n\n1\n10\n100\n24\n\n\n2\n25\n63\n45\n\n\n\n\n\n\nYou can safely set aggregate_function = None if you don‚Äôt have repeated observations within each unique combination of index and on. In this case each participant only has a single ‚Äúobservation_1‚Äù, ‚Äúobservation_2‚Äù, and ‚Äúobservation_3‚Äù.\nBut if they had multiple, Polars will raise an error and ask you to specify how to aggregate them",
    "crumbs": [
      "Schedule",
      "Week 1",
      "Polars Crash Course"
    ]
  },
  {
    "objectID": "weeks/01/lab/polars-crash-course.html#splitting-1-column-into-many",
    "href": "weeks/01/lab/polars-crash-course.html#splitting-1-column-into-many",
    "title": "A Crash Course on Python DataFrames",
    "section": "Splitting 1 column into many",
    "text": "Splitting 1 column into many\nSometimes you‚Äôll need to split one column into multiple columns. Let‚Äôs say we wanted to split the ‚Äúyear_month‚Äù column into 2 separate columns of ‚Äúyear‚Äù and ‚Äúmonth‚Äù:\n\n\nCode\ndf_3 = pl.DataFrame({'id': [1, 2, 3], 'year_month': ['2021-01', '2021-02', '2021-03']})\ndf_3\n\n\n\nshape: (3, 2)\n\n\n\nid\nyear_month\n\n\ni64\nstr\n\n\n\n\n1\n\"2021-01\"\n\n\n2\n\"2021-02\"\n\n\n3\n\"2021-03\"\n\n\n\n\n\n\nYou can use attribute operations for .str to do this!\nSpecifically we use can .split_exact to split a str into a n+1 parts.\n\n\nCode\ndf_split = df_3.with_columns(\n    col('year_month').str.split_exact('-', 1)\n)\ndf_split  # string attribute method, to split by delimiter \"-\" into 2 parts\n\n\n\nshape: (3, 2)\n\n\n\nid\nyear_month\n\n\ni64\nstruct[2]\n\n\n\n\n1\n{\"2021\",\"01\"}\n\n\n2\n{\"2021\",\"02\"}\n\n\n3\n{\"2021\",\"03\"}\n\n\n\n\n\n\nPolars stores these parts in a struct which is just a Python dictionary:\n\n\nCode\n# First row, second column value\ndf_split[0, 1]\n\n\n{'field_0': '2021', 'field_1': '01'}\n\n\nPolars provides additional attribute operations on the .struct to create new columns.\nFirst we‚Äôll call .rename_fields to rename the fields of the struct (equivalent to renaming the keys of a Python dictionary).\n\n\nCode\n# string attribute method, to split by delimiter \"-\" into 2 parts\n# struct attribute method to rename fields\ndf_split_1 = df_3.with_columns(\n    col('year_month').str.split_exact('-', 1).struct.rename_fields(['year', 'month'])\n    )\ndf_split_1\n\n\n\nshape: (3, 2)\n\n\n\nid\nyear_month\n\n\ni64\nstruct[2]\n\n\n\n\n1\n{\"2021\",\"01\"}\n\n\n2\n{\"2021\",\"02\"}\n\n\n3\n{\"2021\",\"03\"}\n\n\n\n\n\n\n\n\nCode\n# First row, second column value\ndf_split_1[0, 1]\n\n\n{'year': '2021', 'month': '01'}\n\n\nThen we‚Äôll call struct.unnest() to create new columns, 1 per field\n\n\nCode\n# string attribute method, to split by delimiter \"-\" into 2 parts\n# struct attribute method to rename fields  # struct attribute method to create 1 column per field\ndf_split_2 = df_3.with_columns(\n    col('year_month').str.split_exact('-', 1).struct.rename_fields(['year', 'month']).struct.unnest()\n    )\ndf_split_2\n\n\n\nshape: (3, 4)\n\n\n\nid\nyear_month\nyear\nmonth\n\n\ni64\nstr\nstr\nstr\n\n\n\n\n1\n\"2021-01\"\n\"2021\"\n\"01\"\n\n\n2\n\"2021-02\"\n\"2021\"\n\"02\"\n\n\n3\n\"2021-03\"\n\"2021\"\n\"03\"\n\n\n\n\n\n\nWe can also split up values in a column over rows .explode('column_name') method on the DataFrame itself:\n\n\nCode\ndf_4 = pl.DataFrame({'letters': ['a', 'a', 'b', 'c'], 'numbers': [[1], [2, 3], [4, 5], [6, 7, 8]]})\ndf_4\n\n\n\nshape: (4, 2)\n\n\n\nletters\nnumbers\n\n\nstr\nlist[i64]\n\n\n\n\n\"a\"\n[1]\n\n\n\"a\"\n[2, 3]\n\n\n\"b\"\n[4, 5]\n\n\n\"c\"\n[6, 7, 8]\n\n\n\n\n\n\n\n\nCode\ndf_4.explode('numbers')\n\n\n\nshape: (8, 2)\n\n\n\nletters\nnumbers\n\n\nstr\ni64\n\n\n\n\n\"a\"\n1\n\n\n\"a\"\n2\n\n\n\"a\"\n3\n\n\n\"b\"\n4\n\n\n\"b\"\n5\n\n\n\"c\"\n6\n\n\n\"c\"\n7\n\n\n\"c\"\n8",
    "crumbs": [
      "Schedule",
      "Week 1",
      "Polars Crash Course"
    ]
  },
  {
    "objectID": "weeks/01/lab/polars-crash-course.html#combining-many-columns-into-1",
    "href": "weeks/01/lab/polars-crash-course.html#combining-many-columns-into-1",
    "title": "A Crash Course on Python DataFrames",
    "section": "Combining many columns into 1",
    "text": "Combining many columns into 1\nWe can combine columns into a single column using additional functions in an expression like pl.concat_list() and pl.concat_str(), which take column names as input:\n\n\nCode\ndf_split_2.with_columns(\n    month_year=pl.concat_str('month', 'year', separator='-')\n)\n\n\n\nshape: (3, 5)\n\n\n\nid\nyear_month\nyear\nmonth\nmonth_year\n\n\ni64\nstr\nstr\nstr\nstr\n\n\n\n\n1\n\"2021-01\"\n\"2021\"\n\"01\"\n\"01-2021\"\n\n\n2\n\"2021-02\"\n\"2021\"\n\"02\"\n\"02-2021\"\n\n\n3\n\"2021-03\"\n\"2021\"\n\"03\"\n\"03-2021\"\n\n\n\n\n\n\nPolars also includes various functions that end with _horizontal.\nLike the suffix implies, these functions are design to operate horizontally across columns within each row separately.\nLet‚Äôs say our DataFrame had these 3 numeric columns:\n\n\nCode\nimport numpy as np  # we haven't met this library yet, just using it to generate data\ndf_5 = df_4.with_columns(\n    a=np.random.normal(size=df_4.height),\n    b=np.random.normal(size=df_4.height),\n    c=np.random.normal(size=df_4.height)\n)\ndf_5\n\n\n\nshape: (4, 5)\n\n\n\nletters\nnumbers\na\nb\nc\n\n\nstr\nlist[i64]\nf64\nf64\nf64\n\n\n\n\n\"a\"\n[1]\n1.38304\n1.517516\n-0.485741\n\n\n\"a\"\n[2, 3]\n1.255348\n-1.143134\n2.937848\n\n\n\"b\"\n[4, 5]\n-1.109448\n0.366913\n-1.339077\n\n\n\"c\"\n[6, 7, 8]\n1.188979\n1.125729\n0.025955\n\n\n\n\n\n\nAnd we want to create a new column that is the average of these 3 columns within each row. We can easily to that using a horizontal function like pl.mean_horizontal\n\n\nCode\ndf_5.with_columns(abc_mean=pl.mean_horizontal('a', 'b', 'c'))\n\n\n\nshape: (4, 6)\n\n\n\nletters\nnumbers\na\nb\nc\nabc_mean\n\n\nstr\nlist[i64]\nf64\nf64\nf64\nf64\n\n\n\n\n\"a\"\n[1]\n1.38304\n1.517516\n-0.485741\n0.804938\n\n\n\"a\"\n[2, 3]\n1.255348\n-1.143134\n2.937848\n1.016687\n\n\n\"b\"\n[4, 5]\n-1.109448\n0.366913\n-1.339077\n-0.693871\n\n\n\"c\"\n[6, 7, 8]\n1.188979\n1.125729\n0.025955\n0.780221",
    "crumbs": [
      "Schedule",
      "Week 1",
      "Polars Crash Course"
    ]
  },
  {
    "objectID": "weeks/01/lab/polars-crash-course.html#your-turn",
    "href": "weeks/01/lab/polars-crash-course.html#your-turn",
    "title": "A Crash Course on Python DataFrames",
    "section": "Your Turn",
    "text": "Your Turn\n\n\n\n\n\n\nTipYour Turn\n\n\n\nMake the following DataFrame ‚Äútidy‚Äù, i.e.¬†long-format with 4 columns:\n\nparticipant: integer of participant ID\norder: integer stimulus and observation order (from column names)\nstimulus: string of stimulus name\nobservation: float of numeric rating each participant gave\n\n\n\n\n\nCode\nreshape = pl.DataFrame({\n    'participant': [1., 2.],\n    'stimulus_1': ['flower', 'car'],\n    'observation_1': [10., 25.,],\n    'stimulus_2': ['house', 'flower'],\n    'observation_2': [100., 63.,],\n    'stimulus_3': ['car', 'house'],\n    'observation_3': [24., 45.,]\n})\nreshape\n\n\n\nshape: (2, 7)\n\n\n\nparticipant\nstimulus_1\nobservation_1\nstimulus_2\nobservation_2\nstimulus_3\nobservation_3\n\n\nf64\nstr\nf64\nstr\nf64\nstr\nf64\n\n\n\n\n1.0\n\"flower\"\n10.0\n\"house\"\n100.0\n\"car\"\n24.0\n\n\n2.0\n\"car\"\n25.0\n\"flower\"\n63.0\n\"house\"\n45.0\n\n\n\n\n\n\nHints\nThink about this as a sequence of 4 steps. We‚Äôve created 4 code cells below with an image above each of the expected result:\n1. unpivot wide -&gt; long\n\n\n\nCode\n# Your code here\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\n\nCode\nstep1 = reshape.unpivot(\n    on=cs.exclude('participant'),\n    index='participant',\n    variable_name='trial',\n    value_name='rating'\n)\nstep1\n\n\n\nshape: (12, 3)\n\n\n\nparticipant\ntrial\nrating\n\n\nf64\nstr\nstr\n\n\n\n\n1.0\n\"stimulus_1\"\n\"flower\"\n\n\n2.0\n\"stimulus_1\"\n\"car\"\n\n\n1.0\n\"observation_1\"\n\"10.0\"\n\n\n2.0\n\"observation_1\"\n\"25.0\"\n\n\n1.0\n\"stimulus_2\"\n\"house\"\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n2.0\n\"observation_2\"\n\"63.0\"\n\n\n1.0\n\"stimulus_3\"\n\"car\"\n\n\n2.0\n\"stimulus_3\"\n\"house\"\n\n\n1.0\n\"observation_3\"\n\"24.0\"\n\n\n2.0\n\"observation_3\"\n\"45.0\"\n\n\n\n\n\n\n\n\n\n2. split the variable_name column from the previous step (I called it trial) into 2 new columns by _ (which I called index and order)\n\n\n\nCode\n# Your code here\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\n\nCode\nstep2 = step1.with_columns(\n    col('trial').str.split_exact('_', 1).struct.rename_fields(['index', 'order']).struct.unnest()\n)\nstep2\n\n\n\nshape: (12, 5)\n\n\n\nparticipant\ntrial\nrating\nindex\norder\n\n\nf64\nstr\nstr\nstr\nstr\n\n\n\n\n1.0\n\"stimulus_1\"\n\"flower\"\n\"stimulus\"\n\"1\"\n\n\n2.0\n\"stimulus_1\"\n\"car\"\n\"stimulus\"\n\"1\"\n\n\n1.0\n\"observation_1\"\n\"10.0\"\n\"observation\"\n\"1\"\n\n\n2.0\n\"observation_1\"\n\"25.0\"\n\"observation\"\n\"1\"\n\n\n1.0\n\"stimulus_2\"\n\"house\"\n\"stimulus\"\n\"2\"\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n2.0\n\"observation_2\"\n\"63.0\"\n\"observation\"\n\"2\"\n\n\n1.0\n\"stimulus_3\"\n\"car\"\n\"stimulus\"\n\"3\"\n\n\n2.0\n\"stimulus_3\"\n\"house\"\n\"stimulus\"\n\"3\"\n\n\n1.0\n\"observation_3\"\n\"24.0\"\n\"observation\"\n\"3\"\n\n\n2.0\n\"observation_3\"\n\"45.0\"\n\"observation\"\n\"3\"\n\n\n\n\n\n\n\n\n\n3. select only the columns: participant, 2 you created (I called mine index and order), and the value_name column from the first step (I called it rating)\n\n\n\nCode\n# Your code here\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\n\nCode\nstep3 = step2.select(col('participant', 'index', 'order', 'rating'))\nstep3\n\n\n\nshape: (12, 4)\n\n\n\nparticipant\nindex\norder\nrating\n\n\nf64\nstr\nstr\nstr\n\n\n\n\n1.0\n\"stimulus\"\n\"1\"\n\"flower\"\n\n\n2.0\n\"stimulus\"\n\"1\"\n\"car\"\n\n\n1.0\n\"observation\"\n\"1\"\n\"10.0\"\n\n\n2.0\n\"observation\"\n\"1\"\n\"25.0\"\n\n\n1.0\n\"stimulus\"\n\"2\"\n\"house\"\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n2.0\n\"observation\"\n\"2\"\n\"63.0\"\n\n\n1.0\n\"stimulus\"\n\"3\"\n\"car\"\n\n\n2.0\n\"stimulus\"\n\"3\"\n\"house\"\n\n\n1.0\n\"observation\"\n\"3\"\n\"24.0\"\n\n\n2.0\n\"observation\"\n\"3\"\n\"45.0\"\n\n\n\n\n\n\n\n\n\n4. pivot long -&gt; wide to break-out the value_name column (I called it rating) into multiple columns\n\n\n\nCode\n# Your code here\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\n\nCode\nstep4 = step3.pivot(\n    on='index',\n    index=['participant', 'order'],\n    values='rating',\n    aggregate_function=None\n)\nstep4\n\n\n\nshape: (6, 4)\n\n\n\nparticipant\norder\nstimulus\nobservation\n\n\nf64\nstr\nstr\nstr\n\n\n\n\n1.0\n\"1\"\n\"flower\"\n\"10.0\"\n\n\n2.0\n\"1\"\n\"car\"\n\"25.0\"\n\n\n1.0\n\"2\"\n\"house\"\n\"100.0\"\n\n\n2.0\n\"2\"\n\"flower\"\n\"63.0\"\n\n\n1.0\n\"3\"\n\"car\"\n\"24.0\"\n\n\n2.0\n\"3\"\n\"house\"\n\"45.0\"",
    "crumbs": [
      "Schedule",
      "Week 1",
      "Polars Crash Course"
    ]
  },
  {
    "objectID": "weeks/01/lab/polars-crash-course.html#additional-resources",
    "href": "weeks/01/lab/polars-crash-course.html#additional-resources",
    "title": "A Crash Course on Python DataFrames",
    "section": "Additional Resources",
    "text": "Additional Resources\nHere a few additional resources that might be helpful on your journey:\n\nPolars official intro tutorial\nMore Comprehensive intro to Polars\nTidyData analysis in Polars\nPolars patterns vs R‚Äôs dplyr",
    "crumbs": [
      "Schedule",
      "Week 1",
      "Polars Crash Course"
    ]
  }
]