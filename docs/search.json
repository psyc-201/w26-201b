[
  {
    "objectID": "weeks/final/project.html",
    "href": "weeks/final/project.html",
    "title": "Final Project",
    "section": "",
    "text": "The final project is your opportunity to apply the statistical modeling skills you‚Äôve learned throughout the course to a research question of your choice."
  },
  {
    "objectID": "weeks/final/project.html#overview",
    "href": "weeks/final/project.html#overview",
    "title": "Final Project",
    "section": "",
    "text": "The final project is your opportunity to apply the statistical modeling skills you‚Äôve learned throughout the course to a research question of your choice."
  },
  {
    "objectID": "weeks/final/project.html#timeline",
    "href": "weeks/final/project.html#timeline",
    "title": "Final Project",
    "section": "Timeline",
    "text": "Timeline\n\n\n\nDate\nMilestone\n\n\n\n\nWeek 8 (Feb 27)\nProject proposal due\n\n\nWeek 10 (Mar 13)\nDraft analysis due (optional feedback)\n\n\nFinal Exam Week (Mar 21)\nFinal project due"
  },
  {
    "objectID": "weeks/final/project.html#requirements",
    "href": "weeks/final/project.html#requirements",
    "title": "Final Project",
    "section": "Requirements",
    "text": "Requirements\n\n1. Research Question\nFormulate a clear, answerable research question that requires statistical analysis.\n\n\n2. Data\n\nUse a publicly available dataset OR\nUse data from your own research (with appropriate permissions)\nMinimum 100 observations recommended\n\n\n\n3. Analysis\nYour analysis should include:\n\nExploratory Data Analysis\n\nSummary statistics\nVisualizations\nData quality assessment\n\nStatistical Modeling\n\nAt least one model from the course (GLM, LMM, etc.)\nModel diagnostics\nModel comparison (if appropriate)\n\nInference or Prediction\n\nParameter estimates with uncertainty\nHypothesis tests or confidence intervals\nOR cross-validated prediction performance\n\nInterpretation\n\nWhat do the results mean?\nLimitations\nFuture directions\n\n\n\n\n4. Deliverables\n\nRendered .qmd file (HTML or PDF)\n\nIntroduction and research question\nMethods\nResults (with code)\nDiscussion\n\nData and code\n\nAll code should be reproducible\nInclude data or instructions for obtaining it"
  },
  {
    "objectID": "weeks/final/project.html#grading-rubric",
    "href": "weeks/final/project.html#grading-rubric",
    "title": "Final Project",
    "section": "Grading Rubric",
    "text": "Grading Rubric\n\n\n\nComponent\nPoints\n\n\n\n\nResearch question clarity\n10\n\n\nData preparation and EDA\n15\n\n\nAppropriate model choice\n20\n\n\nModel fitting and diagnostics\n20\n\n\nInterpretation and conclusions\n20\n\n\nCode quality and reproducibility\n10\n\n\nWriting and presentation\n5\n\n\nTotal\n100"
  },
  {
    "objectID": "weeks/final/project.html#proposal-template",
    "href": "weeks/final/project.html#proposal-template",
    "title": "Final Project",
    "section": "Proposal Template",
    "text": "Proposal Template\nSubmit a 1-page proposal including:\n\nResearch question (1-2 sentences)\nData source (what data, where from, sample size)\nProposed analysis (what models/methods)\nExpected challenges (what might be tricky?)"
  },
  {
    "objectID": "weeks/final/project.html#example-projects",
    "href": "weeks/final/project.html#example-projects",
    "title": "Final Project",
    "section": "Example Projects",
    "text": "Example Projects\nPast successful projects have included:\n\nPredicting movie ratings with mixed models (repeated measures per user)\nAnalyzing experimental psychology data with factorial ANOVA/LMM\nLogistic regression for predicting customer churn\nPower analysis for a planned study\nFactor analysis of survey responses"
  },
  {
    "objectID": "weeks/final/project.html#resources",
    "href": "weeks/final/project.html#resources",
    "title": "Final Project",
    "section": "Resources",
    "text": "Resources\n\nOffice hours: TBD\nCourse materials: All previous weeks\nStatistical consulting: TBD"
  },
  {
    "objectID": "weeks/final/project.html#submission",
    "href": "weeks/final/project.html#submission",
    "title": "Final Project",
    "section": "Submission",
    "text": "Submission\nSubmit via GitHub Classroom by Saturday, March 21, 2026.\nGood luck!"
  },
  {
    "objectID": "weeks/03/index.html",
    "href": "weeks/03/index.html",
    "title": "Week 3",
    "section": "",
    "text": "Review HW 1\nIron out remaining Python confusions\nBias, Variance, and Model Comparison",
    "crumbs": [
      "Schedule",
      "Week 3",
      "Overview"
    ]
  },
  {
    "objectID": "weeks/03/index.html#overview",
    "href": "weeks/03/index.html#overview",
    "title": "Week 3",
    "section": "",
    "text": "Review HW 1\nIron out remaining Python confusions\nBias, Variance, and Model Comparison",
    "crumbs": [
      "Schedule",
      "Week 3",
      "Overview"
    ]
  },
  {
    "objectID": "weeks/03/index.html#slides",
    "href": "weeks/03/index.html#slides",
    "title": "Week 3",
    "section": "Slides",
    "text": "Slides\n\n\n\n\n\n\nTipSlides Wed Jan 22",
    "crumbs": [
      "Schedule",
      "Week 3",
      "Overview"
    ]
  },
  {
    "objectID": "weeks/03/index.html#materials",
    "href": "weeks/03/index.html#materials",
    "title": "Week 3",
    "section": "Materials",
    "text": "Materials\n\n\n\n\n\n\nCautionüìö HW1 (we‚Äôll update to get solutions together)\n\n\n\nGithub Classroom Assignment",
    "crumbs": [
      "Schedule",
      "Week 3",
      "Overview"
    ]
  },
  {
    "objectID": "weeks/03/index.html#mentioned-references",
    "href": "weeks/03/index.html#mentioned-references",
    "title": "Week 3",
    "section": "Mentioned References",
    "text": "Mentioned References\n\nThe Bias Variance Tradeoff",
    "crumbs": [
      "Schedule",
      "Week 3",
      "Overview"
    ]
  },
  {
    "objectID": "weeks/02/lab/eda-workflows.html",
    "href": "weeks/02/lab/eda-workflows.html",
    "title": "Exploratory Data Analysis Workflows",
    "section": "",
    "text": "In this notebook, we‚Äôll work through a complete Exploratory Data Analysis (EDA) workflow. EDA is the process of getting to know your data before any formal analysis - understanding its structure, spotting patterns, identifying problems, and developing intuitions.\nWe‚Äôll assume that you‚Äôre familiar with generally familiar with EDA from PSYC 201A, so this notebook will demonstrate some common tasks in Python combining polars and seaborn. We‚Äôll use a dataset of characters from the Star Wars universe.",
    "crumbs": [
      "Schedule",
      "Week 2",
      "EDA Workflows"
    ]
  },
  {
    "objectID": "weeks/02/lab/eda-workflows.html#how-to-use-this-notebook",
    "href": "weeks/02/lab/eda-workflows.html#how-to-use-this-notebook",
    "title": "Exploratory Data Analysis Workflows",
    "section": "How to Use This Notebook",
    "text": "How to Use This Notebook\n\n\n\n\n\n\nNote\n\n\n\nThis notebook walks through some realistic EDA workflow steps you‚Äôre likely to encounter. Follow along, run the code, and make sure you understand what‚Äôs happening and why. When we move on to building and fitting statistical models, it‚Äôll be important to have an understanding of some key properties of your data:\n\nWhat variables do you have? Numeric? Categorical? Mixed?\nWhat‚Äôs the distribution of each? What‚Äôs the general shape of your data? the range? how common are certain values?\nWhat‚Äôs missing? What‚Äôs the pattern of missing data? Random? Correlated with some variable of interest?\nWhat variables share relationships? No need to fit models to see if varaible simple move together - just plot them!\nWhat‚Äôs the data telling you? Contextualize what you see - anything particularly noteworthy? what might guide your future analysis choices?\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nEDA is visual detective work. You‚Äôre trying to understand what you‚Äôre working with to better inform and guide the later modeling choices you might make.",
    "crumbs": [
      "Schedule",
      "Week 2",
      "EDA Workflows"
    ]
  },
  {
    "objectID": "weeks/02/lab/eda-workflows.html#setup",
    "href": "weeks/02/lab/eda-workflows.html#setup",
    "title": "Exploratory Data Analysis Workflows",
    "section": "Setup",
    "text": "Setup\nLet‚Äôs import our tools:\n\n\nCode\nimport polars as pl\nfrom polars import col, when, lit\nimport polars.selectors as cs\n\nimport seaborn as sns",
    "crumbs": [
      "Schedule",
      "Week 2",
      "EDA Workflows"
    ]
  },
  {
    "objectID": "weeks/02/lab/eda-workflows.html#phase-1-first-contact-with-the-data",
    "href": "weeks/02/lab/eda-workflows.html#phase-1-first-contact-with-the-data",
    "title": "Exploratory Data Analysis Workflows",
    "section": "Phase 1: First Contact with the Data",
    "text": "Phase 1: First Contact with the Data\nThe first step in any EDA is simply looking at your data. What do you have?\n\nLoad and Inspect\nLet‚Äôs load the Star Wars dataset. This contains information about characters from the Star Wars universe:\n\n\nCode\nsw = pl.read_csv(\"data/starwars.csv\")\nsw\n\n\n\nshape: (87, 11)\n\n\n\nname\nheight\nmass\nhair_color\nskin_color\neye_color\nbirth_year\nsex\ngender\nhomeworld\nspecies\n\n\nstr\nf64\nf64\nstr\nstr\nstr\nf64\nstr\nstr\nstr\nstr\n\n\n\n\n\"Luke Skywalker\"\n172.0\n77.0\n\"blond\"\n\"fair\"\n\"blue\"\n19.0\n\"male\"\n\"masculine\"\n\"Tatooine\"\n\"Human\"\n\n\n\"C-3PO\"\n167.0\n75.0\nnull\n\"gold\"\n\"yellow\"\n112.0\n\"none\"\n\"masculine\"\n\"Tatooine\"\n\"Droid\"\n\n\n\"R2-D2\"\n96.0\n32.0\nnull\n\"white, blue\"\n\"red\"\n33.0\n\"none\"\n\"masculine\"\n\"Naboo\"\n\"Droid\"\n\n\n\"Darth Vader\"\n202.0\n136.0\n\"none\"\n\"white\"\n\"yellow\"\n41.9\n\"male\"\n\"masculine\"\n\"Tatooine\"\n\"Human\"\n\n\n\"Leia Organa\"\n150.0\n49.0\n\"brown\"\n\"light\"\n\"brown\"\n19.0\n\"female\"\n\"feminine\"\n\"Alderaan\"\n\"Human\"\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n\"Finn\"\nnull\nnull\n\"black\"\n\"dark\"\n\"dark\"\nnull\n\"male\"\n\"masculine\"\nnull\n\"Human\"\n\n\n\"Rey\"\nnull\nnull\n\"brown\"\n\"light\"\n\"hazel\"\nnull\n\"female\"\n\"feminine\"\nnull\n\"Human\"\n\n\n\"Poe Dameron\"\nnull\nnull\n\"brown\"\n\"light\"\n\"brown\"\nnull\n\"male\"\n\"masculine\"\nnull\n\"Human\"\n\n\n\"BB8\"\nnull\nnull\n\"none\"\n\"none\"\n\"black\"\nnull\n\"none\"\n\"masculine\"\nnull\n\"Droid\"\n\n\n\"Captain Phasma\"\nnull\nnull\n\"none\"\n\"none\"\n\"unknown\"\nnull\n\"female\"\n\"feminine\"\nnull\n\"Human\"\n\n\n\n\n\n\nPolars shows us the first and last few rows, plus the column types. Notice: - str columns contain text (names, categories) - i64 columns contain integers - f64 columns contain decimals (floating point numbers)\nLet‚Äôs get the basic dimensions:\n\n\nCode\nprint(f\"Rows: {sw.height}\")\nprint(f\"Columns: {sw.width}\")\n\n\nRows: 87\nColumns: 11\n\n\n87 characters with 11 attributes each. Let‚Äôs see all the column names:\n\n\nCode\nsw.columns\n\n\n['name',\n 'height',\n 'mass',\n 'hair_color',\n 'skin_color',\n 'eye_color',\n 'birth_year',\n 'sex',\n 'gender',\n 'homeworld',\n 'species']\n\n\n\n\n\n\n\n\nNoteWhat do these columns mean?\n\n\n\nBefore diving into analysis, we should understand what we‚Äôre working with: - name: Character name - height: Height in centimeters - mass: Weight in kilograms - hair_color, skin_color, eye_color: Physical appearance - birth_year: Years before the Battle of Yavin (BBY) - sex: Biological sex (male, female, hermaphroditic, none) - gender: Gender identity (masculine, feminine) - homeworld: Planet of origin - species: Species (Human, Droid, Wookiee, etc.)\nThis context matters! ‚Äúmass‚Äù in Star Wars includes robots and aliens with very different body compositions than humans.\n\n\n\n\nCheck for Missing Data\nMissing data is everywhere in real datasets. Let‚Äôs see how much we‚Äôre dealing with:\n\n\nCode\nsw.null_count()\n\n\n\nshape: (1, 11)\n\n\n\nname\nheight\nmass\nhair_color\nskin_color\neye_color\nbirth_year\nsex\ngender\nhomeworld\nspecies\n\n\nu32\nu32\nu32\nu32\nu32\nu32\nu32\nu32\nu32\nu32\nu32\n\n\n\n\n0\n6\n28\n5\n0\n0\n44\n4\n4\n10\n4\n\n\n\n\n\n\nThis tells us: - height is missing for 6 characters - mass is missing for 28 characters (about 1/3 of the data!) - birth_year is missing for 44 characters (over half!) - sex, gender, homeworld, species have some missing values too\nLet‚Äôs calculate the percentage missing for each column. If we wrap the entire code-block in () Python won‚Äôt care about indentation, so we can lay out the code in a more reader friendly way (like R‚Äôs pipes):\n\n\nCode\n# Using '*' inside col() selects all columns\n(\n    sw\n    .null_count()\n    .select(col('*') / sw.height * 100)\n    .transpose(\n        include_header=True,\n        header_name=\"column\",\n        column_names=[\"pct_missing\"]\n    )\n    .sort(\"pct_missing\", descending=True)\n)\n\n\n\nshape: (11, 2)\n\n\n\ncolumn\npct_missing\n\n\nstr\nf64\n\n\n\n\n\"birth_year\"\n50.574713\n\n\n\"mass\"\n32.183908\n\n\n\"homeworld\"\n11.494253\n\n\n\"height\"\n6.896552\n\n\n\"hair_color\"\n5.747126\n\n\n‚Ä¶\n‚Ä¶\n\n\n\"gender\"\n4.597701\n\n\n\"species\"\n4.597701\n\n\n\"name\"\n0.0\n\n\n\"skin_color\"\n0.0\n\n\n\"eye_color\"\n0.0\n\n\n\n\n\n\n\n\nVisualize Missing Data Patterns\nSometimes missing data has patterns. Let‚Äôs visualize which rows have missing values:\n\n\n\n\n\n\nNoteseaborn.heatmap\n\n\n\nThis is new function that we didn‚Äôt meet in the previous notebook. And we‚Äôve never called sns.FacetGrid directly - so far we‚Äôve just working with the outputs of sns.relplot, sns.catplot, etc\nDon‚Äôt sweat it - just try to build your intuitions about what‚Äôs happening below. Remember that we saw .map_dataframe when trying to layer plots by passing in any plotting function that understands our data. Well if we start with an empty FacetGrid then we can use the same approach to use sns.heatmap to build it from scratch!\n\n\n\n\nCode\n# Create a boolean DataFrame: True where data is missing\nmissing_matrix = sw.select(col('*').is_null()).to_pandas()\n\n# Initialize the facet grid first - just data no aesthetic mappings\ngrid = sns.FacetGrid(data=missing_matrix, aspect=.75, height=8)\n\n# Map the seaborn heatmap function\ngrid.map_dataframe(sns.heatmap, cmap='Grays', yticklabels=sw['name'], cbar=False)\n\n# Adjust aesthetics\ngrid.tick_params(axis='y', labelsize=6)\ngrid\n\n\n\n\n\n\n\n\n\nEach row is a character, each column is a variable. Dark cells indicate missing values.\nNotice that birth_year and mass have scattered missing values across many characters - it‚Äôs not concentrated in a particular group.\n\n\n\n\n\n\nTipYour Turn\n\n\n\nTry to combine some other polars and seaborn workflows to explore the dataset for yourself and understand what you‚Äôre working with\n\n\n\n\nCode\n# Your code here\n\n\n\n\nCode\n# Your code here\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\n\nCode\n# Explore the distribution of species\nsw[\"species\"].value_counts().sort(\"count\", descending=True).head(10)\n\n\n\nshape: (10, 2)\n\n\n\nspecies\ncount\n\n\nstr\nu32\n\n\n\n\n\"Human\"\n35\n\n\n\"Droid\"\n6\n\n\nnull\n4\n\n\n\"Gungan\"\n3\n\n\n\"Mirialan\"\n2\n\n\n\"Kaminoan\"\n2\n\n\n\"Wookiee\"\n2\n\n\n\"Twi'lek\"\n2\n\n\n\"Zabrak\"\n2\n\n\n\"Quermian\"\n1\n\n\n\n\n\n\n\n\nCode\n# Visualize the relationship between height and sex\nsns.catplot(\n    data=sw.filter(col(\"sex\").is_in([\"male\", \"female\"])),\n    x=\"sex\",\n    y=\"height\",\n    kind=\"violin\"\n).set_axis_labels(\"Sex\", \"Height (cm)\")",
    "crumbs": [
      "Schedule",
      "Week 2",
      "EDA Workflows"
    ]
  },
  {
    "objectID": "weeks/02/lab/eda-workflows.html#phase-2-understanding-individual-variables",
    "href": "weeks/02/lab/eda-workflows.html#phase-2-understanding-individual-variables",
    "title": "Exploratory Data Analysis Workflows",
    "section": "Phase 2: Understanding Individual Variables",
    "text": "Phase 2: Understanding Individual Variables\nBefore looking at relationships, understand each variable on its own.\n\nNumeric Variables\nWe have three numeric variables: height, mass, and birth_year.\nLet‚Äôs get summary statistics:\n\n\nCode\n# cs.numeric() is a polars selector that gets all numeric columns\nsw.select(cs.numeric()).describe()\n\n\n\nshape: (9, 4)\n\n\n\nstatistic\nheight\nmass\nbirth_year\n\n\nstr\nf64\nf64\nf64\n\n\n\n\n\"count\"\n81.0\n59.0\n43.0\n\n\n\"null_count\"\n6.0\n28.0\n44.0\n\n\n\"mean\"\n174.604938\n97.311864\n87.565116\n\n\n\"std\"\n34.774157\n169.457163\n154.691439\n\n\n\"min\"\n66.0\n15.0\n8.0\n\n\n\"25%\"\n167.0\n56.2\n37.0\n\n\n\"50%\"\n180.0\n79.0\n52.0\n\n\n\"75%\"\n191.0\n85.0\n72.0\n\n\n\"max\"\n264.0\n1358.0\n896.0\n\n\n\n\n\n\nSome observations: - height: Ranges from 66cm to 264cm. Mean is 175cm (about 5‚Äô9‚Äù). - mass: Ranges from 15kg to 1,358kg! That max is suspicious‚Ä¶ - birth_year: Ranges from 8 to 896 BBY. That‚Äôs a huge range.\nLet‚Äôs visualize these distributions:\n\n\nCode\nsns.displot(\n    data=sw,\n    x=\"height\",\n    kind=\"hist\",\n    bins=20\n).set_axis_labels(\"Height (cm)\", \"Count\")\n\n\n\n\n\n\n\n\n\nHeight looks roughly normal, maybe slightly right-skewed. Most characters are between 150-200cm.\n\n\nCode\nsns.displot(\n    data=sw,\n    x=\"mass\",\n    kind=\"hist\",\n    bins=30\n).set_axis_labels(\"Mass (kg)\", \"Count\")\n\n\n\n\n\n\n\n\n\nWhoa! The mass distribution is extremely right-skewed. Most characters cluster at low values, but there‚Äôs an extreme outlier pulling the scale.\nWho is that outlier?\n\n\nCode\nsw.filter(col(\"mass\") &gt; 500).select(\"name\", \"mass\", \"species\")\n\n\n\nshape: (1, 3)\n\n\n\nname\nmass\nspecies\n\n\nstr\nf64\nstr\n\n\n\n\n\"Jabba Desilijic Tiure\"\n1358.0\n\"Hutt\"\n\n\n\n\n\n\nJabba the Hutt weighs 1,358 kg! That‚Äôs not an error - Hutts are massive slug-like aliens.\nThis is why EDA matters: an outlier like this would massively distort any mean or correlation we calculate.\nLet‚Äôs look at mass excluding extreme outliers to see the main distribution:\n\n\nCode\nsns.displot(\n    data=sw.filter(col(\"mass\") &lt; 200),\n    x=\"mass\",\n    kind=\"hist\",\n    bins=20\n).set_axis_labels(\"Mass (kg)\", \"Count\")\n\n\n\n\n\n\n\n\n\nNow we can see the actual distribution. Most characters are between 50-100kg, with a cluster of lighter characters (possibly droids or smaller species).\n\n\nCategorical Variables\nFor categorical variables, we want to know: What are the categories and how frequent is each?\n\n\nCode\nsw[\"species\"].value_counts().sort(\"count\", descending=True).head(10)\n\n\n\nshape: (10, 2)\n\n\n\nspecies\ncount\n\n\nstr\nu32\n\n\n\n\n\"Human\"\n35\n\n\n\"Droid\"\n6\n\n\nnull\n4\n\n\n\"Gungan\"\n3\n\n\n\"Mirialan\"\n2\n\n\n\"Twi'lek\"\n2\n\n\n\"Wookiee\"\n2\n\n\n\"Zabrak\"\n2\n\n\n\"Kaminoan\"\n2\n\n\n\"Pau'an\"\n1\n\n\n\n\n\n\nHumans dominate the dataset (35 characters), followed by Droids (6). Most species appear only once.\n\n\nCode\nsw[\"homeworld\"].value_counts().sort(\"count\", descending=True).head(10)\n\n\n\nshape: (10, 2)\n\n\n\nhomeworld\ncount\n\n\nstr\nu32\n\n\n\n\n\"Naboo\"\n11\n\n\nnull\n10\n\n\n\"Tatooine\"\n10\n\n\n\"Kamino\"\n3\n\n\n\"Coruscant\"\n3\n\n\n\"Alderaan\"\n3\n\n\n\"Ryloth\"\n2\n\n\n\"Kashyyyk\"\n2\n\n\n\"Corellia\"\n2\n\n\n\"Mirial\"\n2\n\n\n\n\n\n\nNaboo and Tatooine are the most common homeworlds, which makes sense - many main characters come from these planets.\n\n\nCode\nsns.catplot(\n    data=sw,\n    y=\"sex\",\n    kind=\"count\",\n    order=sw[\"sex\"].value_counts().sort(\"count\", descending=True)[\"sex\"].to_list()\n).set_axis_labels(\"Count\", \"Sex\")\n\n\n\n\n\n\n\n\n\nThe dataset is heavily male-dominated, reflecting the original Star Wars films. ‚Äúnone‚Äù likely refers to droids.\nReflection: What does this imbalance mean for any analysis we might do comparing sexes? We have ~60 males but only ~16 females.\n\n\n\n\n\n\nTipYour Turn\n\n\n\nTry creating a few plots that help you understand the other variables in the dataset\n\n\n\n\nCode\n# Your code here\n\n\n\n\nCode\n# Your code here\n\n\n\n\nCode\n# Your code here\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\n\nCode\n# Eye color distribution\nsns.catplot(\n    data=sw,\n    y=\"eye_color\",\n    kind=\"count\",\n    order=sw[\"eye_color\"].value_counts().sort(\"count\", descending=True)[\"eye_color\"].to_list(),\n    height=6\n).set_axis_labels(\"Count\", \"Eye Color\")\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Gender breakdown\nsw[\"gender\"].value_counts().sort(\"count\", descending=True)\n\n\n\nshape: (3, 2)\n\n\n\ngender\ncount\n\n\nstr\nu32\n\n\n\n\n\"masculine\"\n66\n\n\n\"feminine\"\n17\n\n\nnull\n4\n\n\n\n\n\n\n\n\nCode\n# Birth year distribution (excluding extreme values)\nsns.displot(\n    data=sw.filter(col(\"birth_year\").is_not_null() & (col(\"birth_year\") &lt; 200)),\n    x=\"birth_year\",\n    kind=\"hist\",\n    bins=15\n).set_axis_labels(\"Birth Year (BBY)\", \"Count\")",
    "crumbs": [
      "Schedule",
      "Week 2",
      "EDA Workflows"
    ]
  },
  {
    "objectID": "weeks/02/lab/eda-workflows.html#phase-3-exploring-relationships",
    "href": "weeks/02/lab/eda-workflows.html#phase-3-exploring-relationships",
    "title": "Exploratory Data Analysis Workflows",
    "section": "Phase 3: Exploring Relationships",
    "text": "Phase 3: Exploring Relationships\nNow let‚Äôs look at how variables relate to each other.\n\nNumeric vs Numeric: Scatter Plots\nThe classic way to explore relationships between two numeric variables is a scatter plot.\n\n\nCode\nsns.relplot(\n    data=sw,\n    x=\"height\",\n    y=\"mass\"\n).set_axis_labels(\"Height (cm)\", \"Mass (kg)\")\n\n\n\n\n\n\n\n\n\nThere‚Äôs a general positive relationship (taller characters tend to be heavier), but Jabba is way off in the corner distorting our view.\nLet‚Äôs exclude extreme mass values and add a regression line:\n\n\nCode\nsns.lmplot(\n    data=sw.filter(col(\"mass\") &lt; 200),\n    x=\"height\",\n    y=\"mass\"\n).set_axis_labels(\"Height (cm)\", \"Mass (kg)\")\n\n\n\n\n\n\n\n\n\nNow the relationship is clearer. But wait - are humans and droids following the same pattern?\n\n\nCode\n# Focus on the most common species\ncommon_species = sw.filter(\n    col(\"species\").is_in([\"Human\", \"Droid\", \"Gungan\", \"Wookiee\"])\n)\n\nsns.lmplot(\n    data=common_species,\n    x=\"height\",\n    y=\"mass\",\n    hue=\"species\",\n    height=5,\n    ci=None,\n).set_axis_labels(\"Height (cm)\", \"Mass (kg)\")\n\n\n\n\n\n\n\n\n\n\n\nNumeric vs Categorical: Comparing Groups\nHow does height differ between male and female characters?\n\n\nCode\n# Filter to male/female only for cleaner comparison\nsw_mf = sw.filter(col(\"sex\").is_in([\"male\", \"female\"]))\n\nsns.catplot(\n    data=sw_mf,\n    x=\"sex\",\n    y=\"height\",\n    kind=\"box\"\n).set_axis_labels(\"Sex\", \"Height (cm)\")\n\n\n\n\n\n\n\n\n\nMale characters tend to be taller, with more variability (wider box, more outliers).\nLet‚Äôs see the individual data points too:\n\n\nCode\nsns.catplot(\n    data=sw_mf,\n    x=\"sex\",\n    y=\"height\",\n    kind=\"swarm\"\n).set_axis_labels(\"Sex\", \"Height (cm)\")\n\n\n\n\n\n\n\n\n\nWith only ~16 female characters, we should be cautious about drawing strong conclusions. The samples are very unequal.\n\n\nFaceting: Breaking Down by Multiple Categories\nOne of seaborn‚Äôs strengths is easily splitting visualizations by category. Let‚Äôs look at how mass varies by sex across different homeworlds:\n\n\nCode\n# Focus on homeworlds with enough characters\ntop_homeworlds = sw[\"homeworld\"].value_counts().filter(col(\"count\") &gt;= 3)[\"homeworld\"].to_list()\n\nsw_subset = sw.filter(\n    col(\"homeworld\").is_in(top_homeworlds) &\n    col(\"sex\").is_in([\"male\", \"female\"]) &\n    col(\"mass\").is_not_null()\n)\n\nsns.catplot(\n    data=sw_subset,\n    x=\"sex\",\n    y=\"mass\",\n    col=\"homeworld\",\n    kind=\"strip\",\n    col_wrap=3,\n    height=3\n).set_axis_labels(\"Sex\", \"Mass (kg)\")\n\n\n\n\n\n\n\n\n\nWe can see a few patterns: - Most homeworlds have more male characters - Tatooine has characters spanning a wide mass range - Naboo characters tend to be lighter\n\n\n\n\n\n\nTipYour Turn\n\n\n\nWhy might Naboo characters tend to be lighter? (Hint: think about which characters are from Naboo) Or explore some other relationships that you are interested\n\n\n\n\nCode\n# Your code here\n\n\n\n\nCode\n# Your code here\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\n\nCode\n# Let's see who is from Naboo\nsw.filter(col(\"homeworld\") == \"Naboo\").select(\"name\", \"species\", \"sex\", \"mass\")\n\n\n\nshape: (11, 4)\n\n\n\nname\nspecies\nsex\nmass\n\n\nstr\nstr\nstr\nf64\n\n\n\n\n\"R2-D2\"\n\"Droid\"\n\"none\"\n32.0\n\n\n\"Palpatine\"\n\"Human\"\n\"male\"\n75.0\n\n\n\"Padm√© Amidala\"\n\"Human\"\n\"female\"\n45.0\n\n\n\"Jar Jar Binks\"\n\"Gungan\"\n\"male\"\n66.0\n\n\n\"Roos Tarpals\"\n\"Gungan\"\n\"male\"\n82.0\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n\"Ric Oli√©\"\n\"Human\"\n\"male\"\nnull\n\n\n\"Quarsh Panaka\"\n\"Human\"\n\"male\"\nnull\n\n\n\"Gregar Typho\"\nnull\nnull\n85.0\n\n\n\"Cord√©\"\nnull\nnull\nnull\n\n\n\"Dorm√©\"\n\"Human\"\n\"female\"\nnull\n\n\n\n\n\n\nNaboo characters include Padm√© Amidala and other human politicians/royalty (typically lighter build), plus Jar Jar Binks and other Gungans. The dataset doesn‚Äôt include many heavy Naboo characters.\n\n\nCode\n# Let's explore another relationship: species and eye color\nsw.filter(col(\"species\").is_in([\"Human\", \"Droid\"])).group_by(\n    [\"species\", \"eye_color\"]\n).agg(\n    count=col(\"name\").count()\n).sort([\"species\", \"count\"], descending=[False, True])\n\n\n\nshape: (11, 3)\n\n\n\nspecies\neye_color\ncount\n\n\nstr\nstr\nu32\n\n\n\n\n\"Droid\"\n\"red\"\n3\n\n\n\"Droid\"\n\"yellow\"\n1\n\n\n\"Droid\"\n\"red, blue\"\n1\n\n\n\"Droid\"\n\"black\"\n1\n\n\n\"Human\"\n\"brown\"\n16\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n\"Human\"\n\"yellow\"\n2\n\n\n\"Human\"\n\"hazel\"\n2\n\n\n\"Human\"\n\"blue-gray\"\n1\n\n\n\"Human\"\n\"dark\"\n1\n\n\n\"Human\"\n\"unknown\"\n1",
    "crumbs": [
      "Schedule",
      "Week 2",
      "EDA Workflows"
    ]
  },
  {
    "objectID": "weeks/02/lab/eda-workflows.html#phase-4-asking-questions",
    "href": "weeks/02/lab/eda-workflows.html#phase-4-asking-questions",
    "title": "Exploratory Data Analysis Workflows",
    "section": "Phase 4: Asking Questions",
    "text": "Phase 4: Asking Questions\n\nQuestion 1: Which species is the tallest on average?\n\n\nCode\nspecies_height = sw.group_by(\"species\").agg(\n    mean_height=col(\"height\").mean(),\n    count=col(\"height\").count()\n).filter(\n    col(\"count\") &gt;= 2  # Only species with multiple characters\n).sort(\"mean_height\", descending=True)\n\nspecies_height.head(10)\n\n\n\nshape: (9, 3)\n\n\n\nspecies\nmean_height\ncount\n\n\nstr\nf64\nu32\n\n\n\n\n\"Wookiee\"\n231.0\n2\n\n\n\"Kaminoan\"\n221.0\n2\n\n\n\"Gungan\"\n208.666667\n3\n\n\n\"Twi'lek\"\n179.0\n2\n\n\n\"Human\"\n178.0\n30\n\n\nnull\n175.0\n4\n\n\n\"Zabrak\"\n173.0\n2\n\n\n\"Mirialan\"\n168.0\n2\n\n\n\"Droid\"\n131.2\n5\n\n\n\n\n\n\n\n\nCode\nsns.catplot(\n    data=species_height.head(10),\n    y=\"species\",\n    x=\"mean_height\",\n    kind=\"bar\",\n    height=5,\n    aspect=1.2\n).set_axis_labels(\"Mean Height (cm)\", \"Species\")\n\n\n\n\n\n\n\n\n\nKaminoans are the tallest species on average, followed by Wookiees. But note we filtered to species with at least 2 characters - with only 1-2 members, ‚Äúaverages‚Äù don‚Äôt mean much.\n\n\nQuestion 2: Is there a relationship between height and birth year?\nDo older characters tend to be shorter (like in real human populations)?\n\n\nCode\n# Filter out extreme birth years and missing values\nsw_age = sw.filter(\n    col(\"birth_year\").is_not_null() &\n    (col(\"birth_year\") &lt; 200)  # Exclude Yoda and others\n)\n\nsns.lmplot(\n    data=sw_age,\n    x=\"birth_year\",\n    y=\"height\",\n    height=5\n).set_axis_labels(\"Birth Year (BBY)\", \"Height (cm)\")\n\n\n\n\n\n\n\n\n\nThere‚Äôs a very weak positive relationship - older characters (higher BBY) might be slightly shorter. But the relationship is weak and could easily be noise.\nLet‚Äôs check humans only:\n\n\nCode\nsw_age_human = sw_age.filter(col(\"species\") == \"Human\")\n(\n    sns.lmplot(\n        data=sw_age_human,\n        x=\"birth_year\",\n        y=\"height\",\n        height=5\n    )\n    .set_axis_labels(\"Birth Year (BBY)\", \"Height (cm)\")\n    .figure.suptitle(\"Humans Only\", y=1.02, x=.55)\n)\n\n\nText(0.55, 1.02, 'Humans Only')\n\n\n\n\n\n\n\n\n\n\n\nQuestion 3: What‚Äôs the most common eye color for each species?\n\n\nCode\n# For species with at least 3 characters\ncommon_species_list = sw[\"species\"].value_counts().filter(col(\"count\") &gt;= 3)[\"species\"].to_list()\n\neye_by_species = sw.filter(\n    col(\"species\").is_in(common_species_list)\n).group_by([\"species\", \"eye_color\"]).agg(\n    count=col(\"name\").count()\n).sort([\"species\", \"count\"], descending=[False, True])\n\n# Get the most common eye color for each species\neye_by_species.group_by('species', 'eye_color').len().sort(\n    ['species', 'len'], descending=[False, True]).group_by('species').first()\n\n\n\nshape: (3, 3)\n\n\n\nspecies\neye_color\nlen\n\n\nstr\nstr\nu32\n\n\n\n\n\"Droid\"\n\"yellow\"\n1\n\n\n\"Gungan\"\n\"orange\"\n1\n\n\n\"Human\"\n\"blue-gray\"\n1\n\n\n\n\n\n\n\nDroids typically have sensor arrays (typically yellow)\nHumans have diverse eye colors (brown/blue/yellow/hazel)\nGungans have orange eyes",
    "crumbs": [
      "Schedule",
      "Week 2",
      "EDA Workflows"
    ]
  },
  {
    "objectID": "weeks/02/lab/eda-workflows.html#phase-5-creating-derived-variables",
    "href": "weeks/02/lab/eda-workflows.html#phase-5-creating-derived-variables",
    "title": "Exploratory Data Analysis Workflows",
    "section": "Phase 5: Creating Derived Variables",
    "text": "Phase 5: Creating Derived Variables\nSometimes you need to create new variables to answer your questions.\n\nBMI (Body Mass Index)\nLet‚Äôs calculate BMI for the humanoid characters:\n\\[BMI = \\frac{mass}{height^2} \\times 10000\\]\n(The 10000 converts cm to m)\n\n\nCode\nsw_bmi = sw.filter(\n    col(\"mass\").is_not_null() &\n    col(\"height\").is_not_null() &\n    (col(\"mass\") &lt; 500)  # Exclude Jabba\n).with_columns(\n    bmi=(col(\"mass\") / (col(\"height\") ** 2) * 10000).round(1)\n)\n\nsw_bmi.select(\"name\", \"height\", \"mass\", \"species\", \"bmi\").head(10)\n\n\n\nshape: (10, 5)\n\n\n\nname\nheight\nmass\nspecies\nbmi\n\n\nstr\nf64\nf64\nstr\nf64\n\n\n\n\n\"Luke Skywalker\"\n172.0\n77.0\n\"Human\"\n26.0\n\n\n\"C-3PO\"\n167.0\n75.0\n\"Droid\"\n26.9\n\n\n\"R2-D2\"\n96.0\n32.0\n\"Droid\"\n34.7\n\n\n\"Darth Vader\"\n202.0\n136.0\n\"Human\"\n33.3\n\n\n\"Leia Organa\"\n150.0\n49.0\n\"Human\"\n21.8\n\n\n\"Owen Lars\"\n178.0\n120.0\n\"Human\"\n37.9\n\n\n\"Beru Whitesun Lars\"\n165.0\n75.0\n\"Human\"\n27.5\n\n\n\"R5-D4\"\n97.0\n32.0\n\"Droid\"\n34.0\n\n\n\"Biggs Darklighter\"\n183.0\n84.0\n\"Human\"\n25.1\n\n\n\"Obi-Wan Kenobi\"\n182.0\n77.0\n\"Human\"\n23.2\n\n\n\n\n\n\n\n\nCode\nsns.displot(\n    data=sw_bmi,\n    x=\"bmi\",\n    kind=\"hist\",\n    bins=20\n).set_axis_labels(\"BMI\", \"Count\")\n\n\n\n\n\n\n\n\n\nMost characters have BMIs in the 20-30 range, which would be ‚Äúnormal‚Äù to ‚Äúoverweight‚Äù for humans. But BMI was designed for humans - it doesn‚Äôt really make sense for Wookiees or droids!\n\n\nCode\n# Who has the highest BMI?\nsw_bmi.sort(\"bmi\", descending=True).select(\"name\", \"species\", \"bmi\").head(5)\n\n\n\nshape: (5, 3)\n\n\n\nname\nspecies\nbmi\n\n\nstr\nstr\nf64\n\n\n\n\n\"Dud Bolt\"\n\"Vulptereen\"\n50.9\n\n\n\"Yoda\"\n\"Yoda's species\"\n39.0\n\n\n\"Owen Lars\"\n\"Human\"\n37.9\n\n\n\"IG-88\"\n\"Droid\"\n35.0\n\n\n\"R2-D2\"\n\"Droid\"\n34.7\n\n\n\n\n\n\n\n\nCode\n# Who has the lowest BMI?\nsw_bmi.sort(\"bmi\").select(\"name\", \"species\", \"bmi\").head(5)\n\n\n\nshape: (5, 3)\n\n\n\nname\nspecies\nbmi\n\n\nstr\nstr\nf64\n\n\n\n\n\"Wat Tambor\"\n\"Skakoan\"\n12.9\n\n\n\"Padm√© Amidala\"\n\"Human\"\n13.1\n\n\n\"Adi Gallia\"\n\"Tholothian\"\n14.8\n\n\n\"Sly Moore\"\nnull\n15.1\n\n\n\"Roos Tarpals\"\n\"Gungan\"\n16.3\n\n\n\n\n\n\nThe lowest BMI belongs to droids (makes sense - they‚Äôre light for their size).\n\n\nCreating Categories from Numeric Variables\nLet‚Äôs categorize characters by height:\n\n\nCode\nsw_height_cat = sw.with_columns(\n    height_category=when(col(\"height\") &lt; 100).then(lit(\"short\"))\n        .when(col(\"height\") &lt; 180).then(lit(\"medium\"))\n        .when(col(\"height\") &lt; 220).then(lit(\"tall\"))\n        .otherwise(lit(\"very tall\"))\n)\n\nsw_height_cat[\"height_category\"].value_counts().sort(\"count\", descending=True)\n\n\n\nshape: (4, 2)\n\n\n\nheight_category\ncount\n\n\nstr\nu32\n\n\n\n\n\"tall\"\n39\n\n\n\"medium\"\n30\n\n\n\"very tall\"\n11\n\n\n\"short\"\n7\n\n\n\n\n\n\n\n\nCode\n# Visualize mass by height category\nsns.catplot(\n    data=sw_height_cat.filter(col(\"mass\") &lt; 200),\n    x=\"height_category\",\n    y=\"mass\",\n    kind=\"box\",\n    order=[\"short\", \"medium\", \"tall\", \"very tall\"]\n).set_axis_labels(\"Height Category\", \"Mass (kg)\")\n\n\n\n\n\n\n\n\n\nAs expected, taller categories have higher mass on average, but there‚Äôs overlap between groups.\n\n\n\n\n\n\nTipYour Turn\n\n\n\nTry playing around to create some variables you are interested in\n\n\n\n\nCode\n# Your code here\n\n\n\n\nCode\n# Your code here\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\n\nCode\n# Create a \"power ratio\" - mass per unit height (density proxy)\nsw_power = sw.filter(\n    col(\"mass\").is_not_null() & col(\"height\").is_not_null()\n).with_columns(\n    power_ratio=(col(\"mass\") / col(\"height\")).round(2)\n)\n\n# Who has the highest power ratio?\nsw_power.sort(\"power_ratio\", descending=True).select(\"name\", \"species\", \"mass\", \"height\", \"power_ratio\").head(10)\n\n\n\nshape: (10, 5)\n\n\n\nname\nspecies\nmass\nheight\npower_ratio\n\n\nstr\nstr\nf64\nf64\nf64\n\n\n\n\n\"Jabba Desilijic Tiure\"\n\"Hutt\"\n1358.0\n175.0\n7.76\n\n\n\"Grievous\"\n\"Kaleesh\"\n159.0\n216.0\n0.74\n\n\n\"IG-88\"\n\"Droid\"\n140.0\n200.0\n0.7\n\n\n\"Darth Vader\"\n\"Human\"\n136.0\n202.0\n0.67\n\n\n\"Owen Lars\"\n\"Human\"\n120.0\n178.0\n0.67\n\n\n\"Jek Tono Porkins\"\nnull\n110.0\n180.0\n0.61\n\n\n\"Bossk\"\n\"Trandoshan\"\n113.0\n190.0\n0.59\n\n\n\"Tarfful\"\n\"Wookiee\"\n136.0\n234.0\n0.58\n\n\n\"Dexter Jettster\"\n\"Besalisk\"\n102.0\n198.0\n0.52\n\n\n\"Chewbacca\"\n\"Wookiee\"\n112.0\n228.0\n0.49\n\n\n\n\n\n\n\n\nCode\n# Create age categories based on birth_year\nsw_age_cat = sw.filter(col(\"birth_year\").is_not_null()).with_columns(\n    age_category=when(col(\"birth_year\") &lt; 30).then(lit(\"young\"))\n        .when(col(\"birth_year\") &lt; 60).then(lit(\"middle-aged\"))\n        .when(col(\"birth_year\") &lt; 100).then(lit(\"older\"))\n        .otherwise(lit(\"ancient\"))\n)\n\nsw_age_cat[\"age_category\"].value_counts().sort(\"count\", descending=True)\n\n\n\nshape: (4, 2)\n\n\n\nage_category\ncount\n\n\nstr\nu32\n\n\n\n\n\"middle-aged\"\n19\n\n\n\"older\"\n11\n\n\n\"young\"\n8\n\n\n\"ancient\"\n5",
    "crumbs": [
      "Schedule",
      "Week 2",
      "EDA Workflows"
    ]
  },
  {
    "objectID": "weeks/02/lab/eda-workflows.html#phase-6-documenting-your-findings",
    "href": "weeks/02/lab/eda-workflows.html#phase-6-documenting-your-findings",
    "title": "Exploratory Data Analysis Workflows",
    "section": "Phase 6: Documenting Your Findings",
    "text": "Phase 6: Documenting Your Findings\nGood EDA isn‚Äôt just about making plots - it‚Äôs about learning something and communicating it.\nHere‚Äôs what we learned about the Star Wars dataset:\n\nSummary of Findings\nData Quality: - 87 characters, 11 variables - Significant missing data: birth_year (50%), mass (32%), height (7%) - One extreme outlier: Jabba the Hutt (mass = 1,358 kg)\nVariable Distributions: - Height is roughly normal, centered around 175 cm - Mass is heavily right-skewed due to outliers - Dataset is dominated by male human characters\nKey Relationships: - Height and mass are positively correlated (taller = heavier) - This relationship varies by species (especially for droids) - No clear relationship between age and height within humans\nCaveats: - Small sample sizes for most species (only Humans have n&gt;10) - Gender imbalance limits sex comparisons - BMI and similar metrics designed for humans don‚Äôt generalize well",
    "crumbs": [
      "Schedule",
      "Week 2",
      "EDA Workflows"
    ]
  },
  {
    "objectID": "weeks/02/lab/eda-workflows.html#your-turn",
    "href": "weeks/02/lab/eda-workflows.html#your-turn",
    "title": "Exploratory Data Analysis Workflows",
    "section": "Your Turn",
    "text": "Your Turn\n\n\n\n\n\n\nTipYour Turn\n\n\n\nUse what you‚Äôve learned to answer these questions. Create as many new cells as you need below each question.\n\n\n1. Which homeworld has the greatest diversity of species?\nHint: group by homeworld and count unique species\n\n\nCode\n# Your code here (create more cells as needed)\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\n\nCode\nsw.filter(\n    col(\"homeworld\").is_not_null() & col(\"species\").is_not_null()\n).group_by(\"homeworld\").agg(\n    n_species=col(\"species\").n_unique(),\n    species_list=col(\"species\").unique()\n).sort(\"n_species\", descending=True).head(10)\n\n\n\nshape: (10, 3)\n\n\n\nhomeworld\nn_species\nspecies_list\n\n\nstr\nu32\nlist[str]\n\n\n\n\n\"Naboo\"\n3\n[\"Droid\", \"Human\", \"Gungan\"]\n\n\n\"Tatooine\"\n2\n[\"Human\", \"Droid\"]\n\n\n\"Kamino\"\n2\n[\"Human\", \"Kaminoan\"]\n\n\n\"Coruscant\"\n2\n[\"Human\", \"Tholothian\"]\n\n\n\"Cerea\"\n1\n[\"Cerean\"]\n\n\n\"Vulpter\"\n1\n[\"Vulptereen\"]\n\n\n\"Trandosha\"\n1\n[\"Trandoshan\"]\n\n\n\"Stewjon\"\n1\n[\"Human\"]\n\n\n\"Dathomir\"\n1\n[\"Zabrak\"]\n\n\n\"Troiken\"\n1\n[\"Xexto\"]\n\n\n\n\n\n\nNaboo has the greatest diversity with 3 different species (Human, Gungan, and Droid).\n\n\n\n2. Create a visualization comparing the height distributions of Humans vs Droids\nHint: use displot with hue\n\n\nCode\n# Your code here (create more cells as needed)\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\n\nCode\nsns.displot(\n    data=sw.filter(col(\"species\").is_in([\"Human\", \"Droid\"])),\n    x=\"height\",\n    hue=\"species\",\n    kind=\"hist\",\n    bins=15,\n    alpha=0.6\n).set_axis_labels(\"Height (cm)\", \"Count\")\n\n\n\n\n\n\n\n\n\nHumans have a wider height distribution centered around 175-180cm, while Droids are more variable with some very short (R2-D2) and some tall (IG-88) units.\n\n\n\n3. Who is the heaviest character from each homeworld?\nHint: sort by mass within each homeworld group, then take the first\n\n\nCode\n# Your code here (create more cells as needed)\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\n\nCode\nsw.filter(\n    col(\"homeworld\").is_not_null() & col(\"mass\").is_not_null()\n).sort(\"mass\", descending=True).group_by(\"homeworld\").first().select(\n    \"homeworld\", \"name\", \"species\", \"mass\"\n).sort(\"mass\", descending=True)\n\n\n\nshape: (39, 4)\n\n\n\nhomeworld\nname\nspecies\nmass\n\n\nstr\nstr\nstr\nf64\n\n\n\n\n\"Nal Hutta\"\n\"Jabba Desilijic Tiure\"\n\"Hutt\"\n1358.0\n\n\n\"Kalee\"\n\"Grievous\"\n\"Kaleesh\"\n159.0\n\n\n\"Tatooine\"\n\"Darth Vader\"\n\"Human\"\n136.0\n\n\n\"Kashyyyk\"\n\"Tarfful\"\n\"Wookiee\"\n136.0\n\n\n\"Trandosha\"\n\"Bossk\"\n\"Trandoshan\"\n113.0\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n\"Umbara\"\n\"Sly Moore\"\nnull\n48.0\n\n\n\"Vulpter\"\n\"Dud Bolt\"\n\"Vulptereen\"\n45.0\n\n\n\"Malastare\"\n\"Sebulba\"\n\"Dug\"\n40.0\n\n\n\"Endor\"\n\"Wicket Systri Warrick\"\n\"Ewok\"\n20.0\n\n\n\"Aleen Minor\"\n\"Ratts Tyerel\"\n\"Aleena\"\n15.0\n\n\n\n\n\n\nJabba the Hutt dominates from Nal Hutta (1,358 kg), followed by Grievous from Kalee (159 kg) and IG-88 (140 kg, unknown homeworld).\n\n\n\n4. Is there a relationship between hair color and height for human characters?\nHint: filter to humans, then create a boxplot of height by hair color\n\n\nCode\n# Your code here (create more cells as needed)\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\n\nCode\n# Filter to humans with non-null hair color and height\nhumans = sw.filter(\n    (col(\"species\") == \"Human\") &\n    col(\"hair_color\").is_not_null() &\n    col(\"height\").is_not_null()\n)\n\n# See what hair colors we have\nhumans[\"hair_color\"].value_counts().sort(\"count\", descending=True)\n\n\n\nshape: (10, 2)\n\n\n\nhair_color\ncount\n\n\nstr\nu32\n\n\n\n\n\"brown\"\n10\n\n\n\"black\"\n7\n\n\n\"none\"\n3\n\n\n\"blond\"\n3\n\n\n\"white\"\n2\n\n\n\"grey\"\n1\n\n\n\"auburn, grey\"\n1\n\n\n\"brown, grey\"\n1\n\n\n\"auburn\"\n1\n\n\n\"auburn, white\"\n1\n\n\n\n\n\n\n\n\nCode\nsns.catplot(\n    data=humans,\n    x=\"hair_color\",\n    y=\"height\",\n    kind=\"box\",\n    height=5,\n    aspect=1.5,\n    order=humans[\"hair_color\"].value_counts().sort(\"count\", descending=True)[\"hair_color\"].to_list()\n).set_axis_labels(\"Hair Color\", \"Height (cm)\")\n\n\n\n\n\n\n\n\n\nTheres no clear relationship between hair color and height. Brown-haired humans show the widest range (most data points), while other colors have too few observations to draw conclusions.",
    "crumbs": [
      "Schedule",
      "Week 2",
      "EDA Workflows"
    ]
  },
  {
    "objectID": "weeks/01/lab/python-fundamentals.html",
    "href": "weeks/01/lab/python-fundamentals.html",
    "title": "Python Core Concepts",
    "section": "",
    "text": "This notebook is designed to teach you the essence of Python building upon your understanding of R. We‚Äôll skip some programming basics (see the quickstart in the assignment repo for that) but add references resources to the course website. We‚Äôve structured this notebook to focus on the key bits of Python that might give you trouble coming from R and how to handle them gracefully.",
    "crumbs": [
      "Schedule",
      "Week 1",
      "Python Fundamentals"
    ]
  },
  {
    "objectID": "weeks/01/lab/python-fundamentals.html#variables-and-types",
    "href": "weeks/01/lab/python-fundamentals.html#variables-and-types",
    "title": "Python Core Concepts",
    "section": "Variables and Types",
    "text": "Variables and Types\n\n\nCode\n# We assign variables using `=`\nfirst_name = 'Eshin'\nfirst_name\n\n\n'Eshin'\n\n\n\n\nCode\n# Strings can use single or double quotes\nlast_name = \"Jolly\"\nlast_name\n\n\n'Jolly'\n\n\n\n\nCode\n# Integers\nmy_number = 3\nmy_number\n\n\n3\n\n\n\n\nCode\n# Floats contain decimal points\nmy_decimal = 3.1\nmy_decimal\n\n\n3.1\n\n\nWhat happens if you do this?\n# What happens if you do this?\nmy_variable\n= 3\n\n\n\n\n\n\nImportantSyntaxError\n\n\n\nThe most common error message you‚Äôll encounter early on. It just means you mistyped something and Python doesn‚Äôt understand it.",
    "crumbs": [
      "Schedule",
      "Week 1",
      "Python Fundamentals"
    ]
  },
  {
    "objectID": "weeks/01/lab/python-fundamentals.html#comparisons",
    "href": "weeks/01/lab/python-fundamentals.html#comparisons",
    "title": "Python Core Concepts",
    "section": "Comparisons",
    "text": "Comparisons\n\n\nCode\n# We can make comparisons\nmy_decimal &gt; my_number\n\n\nTrue\n\n\n\n\nCode\n# Not equal\nmy_decimal != my_number\n\n\nTrue\n\n\n\n\nCode\n# We can intuitively combine comparisons with `and`\nmy_number &gt; 2 and my_number &lt; 10\n\n\nTrue\n\n\n\n\nCode\n# Using `or`\nmy_number &gt; 0 or my_number &lt; 1000\n\n\nTrue\n\n\n\n\nCode\n# What happens here?\nmy_number &gt; first_name\n\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[9], line 2\n      1 # What happens here?\n----&gt; 2 my_number &gt; first_name\n\nTypeError: '&gt;' not supported between instances of 'int' and 'str'\n\n\n\n\n\n\n\n\n\nImportantTypeError\n\n\n\nAnother common message telling you that you‚Äôre not providing the expected input to the operation you‚Äôre trying. In this case Python has no way to check a number is greater than a string!",
    "crumbs": [
      "Schedule",
      "Week 1",
      "Python Fundamentals"
    ]
  },
  {
    "objectID": "weeks/01/lab/python-fundamentals.html#types-and-functions",
    "href": "weeks/01/lab/python-fundamentals.html#types-and-functions",
    "title": "Python Core Concepts",
    "section": "Types and Functions",
    "text": "Types and Functions\n\n\nCode\n# We call functions using `function(inputs)`\ntype(first_name)\n\n\nstr\n\n\n\n\nCode\n# the type function tells us what kind of object something is\n# the variable we defined above\ntype(my_number)\n\n\nint\n\n\n\n\nCode\n# Float\ntype(1.2)\n\n\nfloat\n\n\n\n\n\n\n\n\nNoteIntegers vs Floats\n\n\n\nPython like many programming languages distinguishes between numerical values that do or do not require decimal-point precision. Python will always convert to the highest precision it can for you.\n\n\n\n\nCode\n# Integer + Float = Float\ntype(my_number + my_decimal)\n\n\nfloat\n\n\n\n\nCode\n# You can always get help on variables and functions using the `help()` function\n\n# What is print?\nhelp(print)\n\n\nHelp on built-in function print in module builtins:\n\nprint(*args, sep=' ', end='\\n', file=None, flush=False)\n    Prints the values to a stream, or to sys.stdout by default.\n    \n    sep\n      string inserted between values, default a space.\n    end\n      string appended after the last value, default a newline.\n    file\n      a file-like object (stream); defaults to the current sys.stdout.\n    flush\n      whether to forcibly flush the stream.",
    "crumbs": [
      "Schedule",
      "Week 1",
      "Python Fundamentals"
    ]
  },
  {
    "objectID": "weeks/01/lab/python-fundamentals.html#lists",
    "href": "weeks/01/lab/python-fundamentals.html#lists",
    "title": "Python Core Concepts",
    "section": "Lists",
    "text": "Lists\n\n\nCode\n# We can put multiple variables in a list using square brackets `[]`\nmy_list = [first_name, last_name, 'third_name', 'fourth_name']\nmy_list\n\n\n['Eshin', 'Jolly', 'third_name', 'fourth_name']\n\n\n\n\n\n\n\n\nNoteNote: 0-based indexing\n\n\n\nNotice how the variables in the list start at 0? That‚Äôs because unlike R, Python counts starting from 0 not from 1! This is usually the first major difference to get used to and applies to all Python libraries and tools. For example, the first row of a dataframe is row 0 not row 1.\n\n\n\n\nCode\n# We can index into the list to get a single item using `[]`\nmy_list[0]\n\n\n'Eshin'\n\n\n\n\nCode\n# 2nd item\nmy_list[1]\n\n\n'Jolly'\n\n\n\n\nCode\n# We can use negative position to index items backwards\n# last item\nmy_list[-1]\n\n\n'fourth_name'\n\n\n\n\nCode\n# 2nd-to-last item\nmy_list[-2]\n\n\n'third_name'\n\n\n\n\nCode\n# What happens if we try this?\nmy_list[4]\n\n\n\n---------------------------------------------------------------------------\nIndexError                                Traceback (most recent call last)\nCell In[20], line 2\n      1 # What happens if we try this?\n----&gt; 2 my_list[4]\n\nIndexError: list index out of range\n\n\n\n\n\n\n\n\n\nImportantIndexError\n\n\n\nOne the most common error messages is just telling you‚Äôre trying to retrieve an item in a position that doesn‚Äôt exist. In other words the list has too few items and Python doesn‚Äôt know what to do.\n\n\n\n\nCode\n# We can use the `len` function to get the size of the list\nlen(my_list)\n\n\n4",
    "crumbs": [
      "Schedule",
      "Week 1",
      "Python Fundamentals"
    ]
  },
  {
    "objectID": "weeks/01/lab/python-fundamentals.html#slicing",
    "href": "weeks/01/lab/python-fundamentals.html#slicing",
    "title": "Python Core Concepts",
    "section": "Slicing",
    "text": "Slicing\n\n\nCode\n# To retrieve multiple items we can use `[start:stop]` to index the list\nmy_list[0:3]\n\n\n['Eshin', 'Jolly', 'third_name']\n\n\n\n\n\n\n\n\nNoteNote: Slicing doesn‚Äôt include stop\n\n\n\nBy default Python slices up-to but not-including the stop value. Notice how the 3rd index (\"fourth name\") was not included even though we used 3.\n\n\n\n\nCode\n# Leaving off the `start` or `stop` will get all items from-the-start or until-the-end\n# from-the-start\nmy_list[:3]\n\n\n['Eshin', 'Jolly', 'third_name']\n\n\n\n\nCode\n# until-the-end\nmy_list[1:]\n\n\n['Jolly', 'third_name', 'fourth_name']\n\n\n\n\nCode\n# We can optionally control `step` size using a third value\n# from-the-start -&gt; 3rd index -&gt; by two (every other)\nmy_list[0:3:2]\n\n\n['Eshin', 'third_name']\n\n\n\n\nCode\n# If we use a negative `step` we can slice backwards\n# from-the-start -&gt; until-the-end -&gt; backwards\nmy_list[::-1]\n\n\n['fourth_name', 'third_name', 'Jolly', 'Eshin']\n\n\n\n\n\n\n\n\nTip\n\n\n\nUsing list[::-1] is a very common pattern for quickly reversing a list in Python",
    "crumbs": [
      "Schedule",
      "Week 1",
      "Python Fundamentals"
    ]
  },
  {
    "objectID": "weeks/01/lab/python-fundamentals.html#control-flow",
    "href": "weeks/01/lab/python-fundamentals.html#control-flow",
    "title": "Python Core Concepts",
    "section": "Control Flow",
    "text": "Control Flow\n\n\nCode\n# We use indentation and `:` to create blocks of logic (control flow)\nif my_number &gt; 0:\n    print(\"Greater than 0\")\n\n\nGreater than 0\n\n\n\n\n\n\n\n\nNoteNote: Indentation\n\n\n\nPython is often loved for being very readable in part because it doesn‚Äôt use {} to surround code-block like R, Javascript and other languages. However, that means you need to carefully indent or de-indent to accomplish the same thing.\n\n\n\n\nCode\n# We can create branches of logic using indentation with `if/else` and `elif`\nif my_number &gt; 0:\n    print(\"Greater than 0\")\nelse:\n    print(\"Less than 0\")\n\n\nGreater than 0\n\n\nWhat happens here?\n# What happens here?\nif my_number &gt; 0:\nprint(\"Greater than 0\")\n\n\n\n\n\n\nImportantIndentationError\n\n\n\nPython will let you know if your spacing is off and where it‚Äôs happening. You‚Äôll mostly encounter this when you‚Äôre editing code, because VSCode will try to be helpful and automatically indent correctly as you‚Äôre writing code.\n\n\n\n\nCode\n# We can keep branching with `elif`\nif my_number &lt; 0:\n    print(\"Less that 0\")\nelif 4 &gt; my_number &gt; 0: # notice how we can express this like in English\n    print(\"Between 4 and 0\")\nelse:\n    print(\"Very large\")\n\n\nBetween 4 and 0",
    "crumbs": [
      "Schedule",
      "Week 1",
      "Python Fundamentals"
    ]
  },
  {
    "objectID": "weeks/01/lab/python-fundamentals.html#loops",
    "href": "weeks/01/lab/python-fundamentals.html#loops",
    "title": "Python Core Concepts",
    "section": "Loops",
    "text": "Loops\n\n\nCode\n# We can loop in the same way using `for`, indentation and `:`\nfor elem in my_list:\n    # Everything indented at this level happens for EACH item\n    print(elem)\n\n\nEshin\nJolly\nthird_name\nfourth_name\n\n\n\n\nCode\n# The name of the looping variable is arbitrary. Using `elem` is just a convention\nfor boogity_bop in my_list:\n    print(boogity_bop)\n\n\nEshin\nJolly\nthird_name\nfourth_name\n\n\n\n\nCode\n# To operate on each item AND its position/index we use the `enumerate()` function\nhelp(enumerate)\n\n\nHelp on class enumerate in module builtins:\n\nclass enumerate(object)\n |  enumerate(iterable, start=0)\n |  \n |  Return an enumerate object.\n |  \n |    iterable\n |      an object supporting iteration\n |  \n |  The enumerate object yields pairs containing a count (from start, which\n |  defaults to zero) and a value yielded by the iterable argument.\n |  \n |  enumerate is useful for obtaining an indexed list:\n |      (0, seq[0]), (1, seq[1]), (2, seq[2]), ...\n |  \n |  Methods defined here:\n |  \n |  __getattribute__(self, name, /)\n |      Return getattr(self, name).\n |  \n |  __iter__(self, /)\n |      Implement iter(self).\n |  \n |  __next__(self, /)\n |      Implement next(self).\n |  \n |  __reduce__(...)\n |      Return state information for pickling.\n |  \n |  ----------------------------------------------------------------------\n |  Class methods defined here:\n |  \n |  __class_getitem__(...)\n |      See PEP 585\n |  \n |  ----------------------------------------------------------------------\n |  Static methods defined here:\n |  \n |  __new__(*args, **kwargs)\n |      Create and return a new object.  See help(type) for accurate signature.\n\n\n\n\n\n\n\n\n\nTipYour Turn\n\n\n\nTry using help() and the examples above to figure out how to use the enumerate() function to print out each item and its position. If the notebook gives you an error about reusing a variable name (e.g.¬†elem) just call your looping variable something else.\n\n\n\n\nCode\n# Your code below\nfor ...\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\n\nCode\nfor idx, item in enumerate(my_list):\n    print(f\"Position {idx}: {item}\")\n\n\nPosition 0: Eshin\nPosition 1: Jolly\nPosition 2: third_name\nPosition 3: fourth_name",
    "crumbs": [
      "Schedule",
      "Week 1",
      "Python Fundamentals"
    ]
  },
  {
    "objectID": "weeks/01/lab/python-fundamentals.html#functions",
    "href": "weeks/01/lab/python-fundamentals.html#functions",
    "title": "Python Core Concepts",
    "section": "Functions",
    "text": "Functions\n\n\n\n\n\n\nNoteCreating functions\n\n\n\nPython makes it easy to write your own functions to create usable blocks of code using the def keyword (not function like in R). Then we just use indentation like before:\ndef myfunction(first_argument, second_argument...):\n  # Everything indented is inside the function\n  print(\"I'm calculating...\")\n  output = first_argument + second_argument\n  # Optionally return something\n  return output\n\n\n\n\nCode\n# Running this code cell defines the function for use anywhere in the notebook\n\ndef myfunction(first_argument, second_argument):\n    \"\"\"This is optional documentation string for function help\"\"\"\n\n    print(\"I'm calculating...\")\n    output = first_argument + second_argument\n    return output\n\n\n\n\nCode\n# Now lets use it like any other function\nmyfunction(1, 2)\n\n\nI'm calculating...\n\n\n3\n\n\n\n\nCode\nmyfunction(4, 5)\n\n\nI'm calculating...\n\n\n9\n\n\n\n\nCode\n# We can even get help on our function\nhelp(myfunction)\n\n\nHelp on function myfunction in module __main__:\n\nmyfunction(first_argument, second_argument)\n    This is optional documentation string for function help",
    "crumbs": [
      "Schedule",
      "Week 1",
      "Python Fundamentals"
    ]
  },
  {
    "objectID": "weeks/01/lab/python-fundamentals.html#methods",
    "href": "weeks/01/lab/python-fundamentals.html#methods",
    "title": "Python Core Concepts",
    "section": "Methods",
    "text": "Methods\n\n\nCode\n# Unlike R sometimes we use \"functions\" attached to objects with the `.` syntax\nfirst_name.upper()\n\n\n'ESHIN'\n\n\n\n\n\n\n\n\nNoteMethods are functions attached to objects called with .\n\n\n\nUnlike R, Python is an object-oriented-language which means functions can be attached to objects.\nWe call these methods but you can intuitively treat them the same.\nIn the example above, Python doesn‚Äôt have an upper() function, but strings have a .upper() method. In your head when you see first_name.upper() just think upper(first_name).\nThis allows for method-chaining which is Python‚Äôs alternative to R‚Äôs %&gt;% syntax.\nIn R we might do: function() %&gt;% function() %&gt;% function()\nIn Python we‚Äôll often do: object.method().method().method() to achieve the same effect.\n\n\n\n\nCode\n# This is a method-chain\nfirst_name.upper().lower()\n\n\n'eshin'\n\n\n\n\nCode\n# We can use the `dir()` function to see all the methods that belong to an object\n# Since our variable is a list this show all list methods\ndir(my_list)\n\n\n['__add__',\n '__class__',\n '__class_getitem__',\n '__contains__',\n '__delattr__',\n '__delitem__',\n '__dir__',\n '__doc__',\n '__eq__',\n '__format__',\n '__ge__',\n '__getattribute__',\n '__getitem__',\n '__getstate__',\n '__gt__',\n '__hash__',\n '__iadd__',\n '__imul__',\n '__init__',\n '__init_subclass__',\n '__iter__',\n '__le__',\n '__len__',\n '__lt__',\n '__mul__',\n '__ne__',\n '__new__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__reversed__',\n '__rmul__',\n '__setattr__',\n '__setitem__',\n '__sizeof__',\n '__str__',\n '__subclasshook__',\n 'append',\n 'clear',\n 'copy',\n 'count',\n 'extend',\n 'index',\n 'insert',\n 'pop',\n 'remove',\n 'reverse',\n 'sort']\n\n\n\n\nCode\n# Using the `.append()` method\nmy_list.append(\"another_item\")\n\n\n\n\n\n\n\n\nImportantNot all methods are chainable\n\n\n\nNotice how .append() didn‚Äôt return anything? Some methods cannot be chained because they modify the object in-place\nRun the cell below to see how the value of the variable my_list has changed Then run the cell below that to .append() a second time and see what happens\n\n\n\n\nCode\n# my_list was updated in place\nprint(f\"There are {len(my_list)} items:\\n{my_list}\")\n\n\nThere are 5 items:\n['Eshin', 'Jolly', 'third_name', 'fourth_name', 'another_item']\n\n\n\n\nCode\n# Let's append again\nmy_list.append(\"add_another\")\n\n\n\n\nCode\n# Now what does it show?\nprint(f\"There are {len(my_list)} items:\\n{my_list}\")\n\n\nThere are 6 items:\n['Eshin', 'Jolly', 'third_name', 'fourth_name', 'another_item', 'add_another']",
    "crumbs": [
      "Schedule",
      "Week 1",
      "Python Fundamentals"
    ]
  },
  {
    "objectID": "weeks/01/lab/python-fundamentals.html#importing-libraries",
    "href": "weeks/01/lab/python-fundamentals.html#importing-libraries",
    "title": "Python Core Concepts",
    "section": "Importing Libraries",
    "text": "Importing Libraries\n\n\nCode\n# We use the `import` keyword to bring in functionality from other libraries\nimport polars\n\n# Use something from the module with `.`\nmy_empty_dataframe = polars.DataFrame()\nmy_empty_dataframe\n\n\n\nshape: (0, 0)\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteImporting libraries with import and as\n\n\n\nWhereas in R you might use library(lme4) to import a library and automatically get all it‚Äôs functions (e.g.¬†lmer), in Python you have to be more explicit. This is because in Python everything is an object including other libraries, which means you can do accidental things like overwrite a library you imported with a variable:\n# Import the library\nimport mylibrary\n\n# Use it\nmylibrary.myfunction()\n\n# Oops Python will let you do this but DONT\nmylibrary = \"Eshin\"\n\n# This doesn't work anymore!\nmylibrary.myfunction()\n\n\n\n\nCode\n# We use typically using `as` to shorten common library names by convention\nimport polars as pl\n\n# Less typing, fewer mistakes!\nnew_df = pl.DataFrame()\n\n# Show it\nnew_df\n\n\n\nshape: (0, 0)\n\n\n\n\n\n\n\n\n\nCode\n# Or to just import specific functionality\nfrom polars import DataFrame\n\n# Use it\nanother_df = DataFrame()\n\n# Show it\nanother_df\n\n\n\nshape: (0, 0)\n\n\n\n\n\n\n\n\n\nCode\n# Here's a convention Eshin likes, but make sure to never create a variable called `c`\n# (you shouldn't be doing that anyway)\nfrom polars import col as c\n\nhelp(c)\n\n\nHelp on Col in module polars.functions.col:\n\n&lt;Expr ['col(\"__origin__\")']&gt;",
    "crumbs": [
      "Schedule",
      "Week 1",
      "Python Fundamentals"
    ]
  },
  {
    "objectID": "weeks/01/lab/python-fundamentals.html#pro-tips",
    "href": "weeks/01/lab/python-fundamentals.html#pro-tips",
    "title": "Python Core Concepts",
    "section": "Pro-tips",
    "text": "Pro-tips\n\nReference help docs often\nChange-and-rerun often\nDon‚Äôt reuse variable names (the notebook won‚Äôt let you!)",
    "crumbs": [
      "Schedule",
      "Week 1",
      "Python Fundamentals"
    ]
  },
  {
    "objectID": "weeks/01/lab/index.html",
    "href": "weeks/01/lab/index.html",
    "title": "Lab ‚ÄúPre-flight‚Äù Setup",
    "section": "",
    "text": "NoteGoals\n\n\n\n\nSetup your coding tools\nLearn the GitHub Classroom assignment workflow\nAccept & pull Lab 01 to your laptop\n\n\n\nFirst we‚Äôll focus on getting your personal computer ready for the rest of the course. We‚Äôll be making using of the macOS An application for controlling your computer via commands that you type in (e.g.¬†cd, ls) instead of point-and-clickterminal for the first section. Don‚Äôt worry if you‚Äôve never used it before or have limited experience. We‚Äôve written the instructions below so you can follow along step-by-step and just copy and paste the commands into your terminal to avoid typos.\n\n\n\n\n\n\nTip\n\n\n\nYou can click and hold any linked words in the text on this page to get a definition. All definitions are available on the terminology page.\n\n\n\n\nFirst we‚Äôll install the A command-line package manager for macOS that lets you install packages and applications using the brew commandHomebrew package manager for macOS. You can think of this as an ‚ÄúApp store‚Äù for programs we‚Äôll run from our macOS An application for controlling your computer via commands that you type in (e.g.¬†cd, ls) instead of point-and-clickTerminal.\nStart by copying and paste the following command into a new macOS An application for controlling your computer via commands that you type in (e.g.¬†cd, ls) instead of point-and-clickTerminal\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n\n\n\n\n\n\nNote\n\n\n\nThe ‚Äúprogram‚Äù that‚Äôs running when you first launch your terminal is called a A program that runs automatically when your Terminal starts and interprets the commands you type to control your computer instead of pointing-and-clicking. FYI: the default shell on macOS is zsh not bash.shell. You can think of it like console in RStudio, but for running system commands like cd, pwd, ls, etc.\n\n\nAfter some long messages and setup you should have the brew A program that you interact with exclusively from your terminal; often abbreviated as CLIcommand-line-program (CLI) available.\n\n\n\n\n\n\nTip\n\n\n\nYou can check by typing which brew and seeing if you get any output\nIn general the which command will tell you where a CLI tool is installed if it‚Äôs installed; no output means it doesn‚Äôt exist!\n\n\nWe can use this to brew install/update/remove/list/search a variety of tools.\nLet‚Äôs use it to make sure you have a few other tools setup for the course:\n\n\nBuilding off of 201A, you‚Äôll use a scientific publishing tool that allows you to mix prose and code-cells to render executable documents in a variety of formats (website, PDF, etc)Quarto to author all your assignment submissions (labs, HWs, final project). While you can install it from the official website it‚Äôs easier to get from brew\n\n\n\n\n\n\nNote\n\n\n\nTry which quarto first to see if it‚Äôs already installed\nIf so, feel free to skip the next command\n\n\nbrew install --cask quarto\n\n\n\n\n\n\nNote\n\n\n\nThe --cask flag to brew is sometimes needed when installing specific libraries and applications. This makes it possible to installed full GUI applications in addition to CLI ones (e.g.¬†like VSCode) all from brew! But don‚Äôt worry about remembering whether you need to use it or not. brew will helpfully complain if you do.\n\n\n\n\n\nTo keep all our work reproducible and easy to collaborate, on we‚Äôll use A library and environment manager for Python making it easy to create/add/update additional Python libraries & tools in a reproducible and isolated way. using a pyproject.toml ‚Äúblueprints‚Äù fileuv to manage our Python Really just a hidden folder on your computer (typically .venv/) that contains an isolated Python installation with all additional libraries and tools. uv handles this all for us!environment. This makes it effortless to add/update/remove any additional Python libraries in an isolated, project-specific way.\nbrew install uv\n\n\n\nLastly we‚Äôll want to make sure we have the latest tools to interact with A CLI to version control LOCAL files and folders called repositories. See the git guide for more details and a command cheatsheet.Git and A online cloud-based service for synchronize local git repositories with REMOTE repositories on github.com. This faciliates collaborative works flows and open-source development. We‚Äôre using the Github Classroom feature built-up on this for our course.Github.\nbrew install git gh\nFirst we‚Äôll want to make sure the local git CLI program knows who we are. Run the following commands in your terminal (you won‚Äôt see any output):\ngit config --global user.name \"your name\"\ngit config --global user.email \"email associated with your github account\"\nWe‚Äôll also set a few other git options to avoid future headaches:\ngit config --global pull.rebase true\nAnd\ngit config --global rebase.autoStash true\nNow we can login to A online cloud-based service for synchronize local git repositories with REMOTE repositories on github.com. This faciliates collaborative works flows and open-source development. We‚Äôre using the Github Classroom feature built-up on this for our course.Github from our computers to ensure that all our future work is associated with the same account:\ngh auth login\nThen you can answer the prompts with the following answers:\nWhere do you use GitHub? GitHub.com\nWhat is your preferred protocol for Git operations on this host? HTTPS\nAuthenticate Git with your GitHub credentials? Yes\nHow would you like to authenticate GitHub CLI? Login with a web browser\nYou should see the following prompt with a unique code for you. Make sure to copy it and then press &lt;enter&gt;\nFirst copy your one-time code: 4722-D256\nPress Enter to open https://github.com/login/device in your browser...\nCopy and paste the code into the browser page and press the green button to approve. When you‚Äôre all set you‚Äôll get the following output with your Your user ‚Äúhandle‚Äù on github.com that identifies you when using git commands (local) and interacting with Github.com (remote). E.g. Eshin‚Äôs is @ejollyGithub userid:\n‚úì Authentication complete.\n- gh config set -h github.com git_protocol https\n‚úì Configured git protocol\n‚úì Logged in as ejolly\nYou can verify your local setup by running git config --list and looking at the output for some sanity checks like your Github username & email:\nuser.name=ejolly\nuser.email=eshin.jolly@gmail.com\npull.ff=false\npull.rebase=false\n\n\n\nFinally let‚Äôs setup our Integrated-development-environment; a program that includes a code-editor, terminal/console, and other helpful features to be a ‚Äúone-stop-shop‚Äù for most of your needsintegrated-development-environment (IDE)\n\nVSCode (recommended)RStudio (advanced)\n\n\nIf you‚Äôve already installed An extremely popular general purpose IDE that supports multiple language (e.g.¬†Python, R, Javascript) and makes use of extensions to add additional functionality (e.g.¬†quarto rendering, Python notebooks).Visual Studio Code before you can skip the next command. Otherwise, copy and paste the following into your terminal:\nbrew install --cask visual-studio-code\nYou can then launch VSCode like any other program on your computer. We‚Äôll orient to the interface in Lab 01.\n\n\nWhile we‚Äôve mostly built this course around VSCode to provide a consistent experience, you can continue using An IDE originally designed to work with R, but also works well with Quarto documents. Also includes a Terminal separate from the R console for running shell commands.RStudio if you‚Äôre an advanced user. When working with .qmd files, you can use familiar buttons to render Quarto documents and RStudio will handle running the Python code-chunks for you.\nHowever, you will not be able to run .py files that we can work with interactively using code-cells and markdown-cells (similar to quarto chunks). A much richer interface for interactively working with pieces of code one-at-a-timeinteractive Python notebooks (.py) files that we provide. Instead, you‚Äôll need to use the integrated Terminal (not the R console!) to run some commands to launch them (we‚Äôll cover this in the Lab 01 assignment later)\n\n\n\n\n\n\n\nNow that you‚Äôre setup with Github let‚Äôs go over the main workflow you‚Äôll regulary use when working on course materials\n\n\n\n\n\n\nTip\n\n\n\nThese steps are also available for quick future reference in the dedicated github classroom guide linked in the top navigation bar\n\n\n\n\n\nClick any course website link that starts with üìö.\nAccept the assignment in your browser. This will create a copy (fork) of the assignment under your own github account\nClick the URL to go the auto-created github repo. This will always be named assignment-name-your-githubid\nClone it to your local computer: git clone REPO-URL You can get the REPO-URL by clicking the green code button on github\nMove into the folder: cd folder-you-cloned\nSetup the Python environment: uv sync && uv run poe setup\nOpen the project in VSCode (or RStudio)\n\n\n\n\nSubmitting an assignment is as easy as pushing your changes to github. We‚Äôll automatically be able to see when you submit, run automatic checks, etc.\n\nCommit your changes locally. Using the VSCode UI or terminal commands git commit -am \"my message\"\nPush your changes to github: git push\n\nThere are no restrictions on how often or the final deadline to git commit and git push your assignments! For any deadlines we announce, you‚Äôll just want to make sure to make the final push you want us to review by the deadline. Later, once we review assignments together in class, you can continue using commit and pushto update your assignments with corrections, notes, etc for updated grading!\n\n\n\nOften we‚Äôll add new files (e.g.¬†solutions) or make corrections to an assignment and we‚Äôll ask you to update your repository after you‚Äôve already run git clone and maybe even git commit and git push. Here‚Äôs how you can do that:\n\nOpen the assignment repository on github.com You can either find the original üìö link OR cd into the folder and run git remote -v to print out the URL\nGo to the ‚ÄúPull Request‚Äù‚Äù page\nChoose the PR called ‚ÄúGitHub Classroom: Sync Assignment‚Äù\nClick the green ‚ÄúMerge pull request‚Äù button near the bottom\nBack on your computer verify you‚Äôve committed any work-in-progress. If you run git status and you don‚Äôt see ‚Äúnothing to commit, working tree clean‚Äù you‚Äôll need to run git commit -am \"my message\" first.\nSync the merged PR to your computer: git pull\n\nNow any files you had open in VSCode will automatically refresh to the latest versions and any new files we be available for editing!\n\n\n\n\n\n\nTip\n\n\n\nWe‚Äôll also use the ‚ÄúPull Requests‚Äù tab to create a ‚ÄúFeedback‚Äù PR.\nYou don‚Äôt need to merge this in. Instead, think of it as an on-going discussion between you and your instructors & peers where you reference specific files and lines of your project.\n\n\n\n\n\n\nNow that you have the basics configured, let‚Äôs try this out to get setup with the first lab:\n\n\n\n\n\n\nCautionüìö Lab 01\n\n\n\nGithub Classroom Assignment\n\n\nOnce you‚Äôve cloned the assignment to your computer you can start going through tutorials in this order:\n\nREADME.md - VSCode introduction and configuration\nindex.qmd - Quarto & Python intro\nnotebooks/python-quickstart.py - Python fundamentals\n\n\n\n\n\n\n\nWarning\n\n\n\nIf the link above does not work for you please send your Your user ‚Äúhandle‚Äù on github.com that identifies you when using git commands (local) and interacting with Github.com (remote). E.g. Eshin‚Äôs is @ejollygithub-userid to Eshin on Slack so he can add you to the Github classroom!",
    "crumbs": [
      "Schedule",
      "Week 1",
      "\"Pre-flight\" Lab Setup"
    ]
  },
  {
    "objectID": "weeks/01/lab/index.html#computer-setup",
    "href": "weeks/01/lab/index.html#computer-setup",
    "title": "Lab ‚ÄúPre-flight‚Äù Setup",
    "section": "",
    "text": "First we‚Äôll install the A command-line package manager for macOS that lets you install packages and applications using the brew commandHomebrew package manager for macOS. You can think of this as an ‚ÄúApp store‚Äù for programs we‚Äôll run from our macOS An application for controlling your computer via commands that you type in (e.g.¬†cd, ls) instead of point-and-clickTerminal.\nStart by copying and paste the following command into a new macOS An application for controlling your computer via commands that you type in (e.g.¬†cd, ls) instead of point-and-clickTerminal\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n\n\n\n\n\n\nNote\n\n\n\nThe ‚Äúprogram‚Äù that‚Äôs running when you first launch your terminal is called a A program that runs automatically when your Terminal starts and interprets the commands you type to control your computer instead of pointing-and-clicking. FYI: the default shell on macOS is zsh not bash.shell. You can think of it like console in RStudio, but for running system commands like cd, pwd, ls, etc.\n\n\nAfter some long messages and setup you should have the brew A program that you interact with exclusively from your terminal; often abbreviated as CLIcommand-line-program (CLI) available.\n\n\n\n\n\n\nTip\n\n\n\nYou can check by typing which brew and seeing if you get any output\nIn general the which command will tell you where a CLI tool is installed if it‚Äôs installed; no output means it doesn‚Äôt exist!\n\n\nWe can use this to brew install/update/remove/list/search a variety of tools.\nLet‚Äôs use it to make sure you have a few other tools setup for the course:\n\n\nBuilding off of 201A, you‚Äôll use a scientific publishing tool that allows you to mix prose and code-cells to render executable documents in a variety of formats (website, PDF, etc)Quarto to author all your assignment submissions (labs, HWs, final project). While you can install it from the official website it‚Äôs easier to get from brew\n\n\n\n\n\n\nNote\n\n\n\nTry which quarto first to see if it‚Äôs already installed\nIf so, feel free to skip the next command\n\n\nbrew install --cask quarto\n\n\n\n\n\n\nNote\n\n\n\nThe --cask flag to brew is sometimes needed when installing specific libraries and applications. This makes it possible to installed full GUI applications in addition to CLI ones (e.g.¬†like VSCode) all from brew! But don‚Äôt worry about remembering whether you need to use it or not. brew will helpfully complain if you do.\n\n\n\n\n\nTo keep all our work reproducible and easy to collaborate, on we‚Äôll use A library and environment manager for Python making it easy to create/add/update additional Python libraries & tools in a reproducible and isolated way. using a pyproject.toml ‚Äúblueprints‚Äù fileuv to manage our Python Really just a hidden folder on your computer (typically .venv/) that contains an isolated Python installation with all additional libraries and tools. uv handles this all for us!environment. This makes it effortless to add/update/remove any additional Python libraries in an isolated, project-specific way.\nbrew install uv\n\n\n\nLastly we‚Äôll want to make sure we have the latest tools to interact with A CLI to version control LOCAL files and folders called repositories. See the git guide for more details and a command cheatsheet.Git and A online cloud-based service for synchronize local git repositories with REMOTE repositories on github.com. This faciliates collaborative works flows and open-source development. We‚Äôre using the Github Classroom feature built-up on this for our course.Github.\nbrew install git gh\nFirst we‚Äôll want to make sure the local git CLI program knows who we are. Run the following commands in your terminal (you won‚Äôt see any output):\ngit config --global user.name \"your name\"\ngit config --global user.email \"email associated with your github account\"\nWe‚Äôll also set a few other git options to avoid future headaches:\ngit config --global pull.rebase true\nAnd\ngit config --global rebase.autoStash true\nNow we can login to A online cloud-based service for synchronize local git repositories with REMOTE repositories on github.com. This faciliates collaborative works flows and open-source development. We‚Äôre using the Github Classroom feature built-up on this for our course.Github from our computers to ensure that all our future work is associated with the same account:\ngh auth login\nThen you can answer the prompts with the following answers:\nWhere do you use GitHub? GitHub.com\nWhat is your preferred protocol for Git operations on this host? HTTPS\nAuthenticate Git with your GitHub credentials? Yes\nHow would you like to authenticate GitHub CLI? Login with a web browser\nYou should see the following prompt with a unique code for you. Make sure to copy it and then press &lt;enter&gt;\nFirst copy your one-time code: 4722-D256\nPress Enter to open https://github.com/login/device in your browser...\nCopy and paste the code into the browser page and press the green button to approve. When you‚Äôre all set you‚Äôll get the following output with your Your user ‚Äúhandle‚Äù on github.com that identifies you when using git commands (local) and interacting with Github.com (remote). E.g. Eshin‚Äôs is @ejollyGithub userid:\n‚úì Authentication complete.\n- gh config set -h github.com git_protocol https\n‚úì Configured git protocol\n‚úì Logged in as ejolly\nYou can verify your local setup by running git config --list and looking at the output for some sanity checks like your Github username & email:\nuser.name=ejolly\nuser.email=eshin.jolly@gmail.com\npull.ff=false\npull.rebase=false\n\n\n\nFinally let‚Äôs setup our Integrated-development-environment; a program that includes a code-editor, terminal/console, and other helpful features to be a ‚Äúone-stop-shop‚Äù for most of your needsintegrated-development-environment (IDE)\n\nVSCode (recommended)RStudio (advanced)\n\n\nIf you‚Äôve already installed An extremely popular general purpose IDE that supports multiple language (e.g.¬†Python, R, Javascript) and makes use of extensions to add additional functionality (e.g.¬†quarto rendering, Python notebooks).Visual Studio Code before you can skip the next command. Otherwise, copy and paste the following into your terminal:\nbrew install --cask visual-studio-code\nYou can then launch VSCode like any other program on your computer. We‚Äôll orient to the interface in Lab 01.\n\n\nWhile we‚Äôve mostly built this course around VSCode to provide a consistent experience, you can continue using An IDE originally designed to work with R, but also works well with Quarto documents. Also includes a Terminal separate from the R console for running shell commands.RStudio if you‚Äôre an advanced user. When working with .qmd files, you can use familiar buttons to render Quarto documents and RStudio will handle running the Python code-chunks for you.\nHowever, you will not be able to run .py files that we can work with interactively using code-cells and markdown-cells (similar to quarto chunks). A much richer interface for interactively working with pieces of code one-at-a-timeinteractive Python notebooks (.py) files that we provide. Instead, you‚Äôll need to use the integrated Terminal (not the R console!) to run some commands to launch them (we‚Äôll cover this in the Lab 01 assignment later)",
    "crumbs": [
      "Schedule",
      "Week 1",
      "\"Pre-flight\" Lab Setup"
    ]
  },
  {
    "objectID": "weeks/01/lab/index.html#github-classroom-workflows",
    "href": "weeks/01/lab/index.html#github-classroom-workflows",
    "title": "Lab ‚ÄúPre-flight‚Äù Setup",
    "section": "",
    "text": "Now that you‚Äôre setup with Github let‚Äôs go over the main workflow you‚Äôll regulary use when working on course materials\n\n\n\n\n\n\nTip\n\n\n\nThese steps are also available for quick future reference in the dedicated github classroom guide linked in the top navigation bar\n\n\n\n\n\nClick any course website link that starts with üìö.\nAccept the assignment in your browser. This will create a copy (fork) of the assignment under your own github account\nClick the URL to go the auto-created github repo. This will always be named assignment-name-your-githubid\nClone it to your local computer: git clone REPO-URL You can get the REPO-URL by clicking the green code button on github\nMove into the folder: cd folder-you-cloned\nSetup the Python environment: uv sync && uv run poe setup\nOpen the project in VSCode (or RStudio)\n\n\n\n\nSubmitting an assignment is as easy as pushing your changes to github. We‚Äôll automatically be able to see when you submit, run automatic checks, etc.\n\nCommit your changes locally. Using the VSCode UI or terminal commands git commit -am \"my message\"\nPush your changes to github: git push\n\nThere are no restrictions on how often or the final deadline to git commit and git push your assignments! For any deadlines we announce, you‚Äôll just want to make sure to make the final push you want us to review by the deadline. Later, once we review assignments together in class, you can continue using commit and pushto update your assignments with corrections, notes, etc for updated grading!\n\n\n\nOften we‚Äôll add new files (e.g.¬†solutions) or make corrections to an assignment and we‚Äôll ask you to update your repository after you‚Äôve already run git clone and maybe even git commit and git push. Here‚Äôs how you can do that:\n\nOpen the assignment repository on github.com You can either find the original üìö link OR cd into the folder and run git remote -v to print out the URL\nGo to the ‚ÄúPull Request‚Äù‚Äù page\nChoose the PR called ‚ÄúGitHub Classroom: Sync Assignment‚Äù\nClick the green ‚ÄúMerge pull request‚Äù button near the bottom\nBack on your computer verify you‚Äôve committed any work-in-progress. If you run git status and you don‚Äôt see ‚Äúnothing to commit, working tree clean‚Äù you‚Äôll need to run git commit -am \"my message\" first.\nSync the merged PR to your computer: git pull\n\nNow any files you had open in VSCode will automatically refresh to the latest versions and any new files we be available for editing!\n\n\n\n\n\n\nTip\n\n\n\nWe‚Äôll also use the ‚ÄúPull Requests‚Äù tab to create a ‚ÄúFeedback‚Äù PR.\nYou don‚Äôt need to merge this in. Instead, think of it as an on-going discussion between you and your instructors & peers where you reference specific files and lines of your project.",
    "crumbs": [
      "Schedule",
      "Week 1",
      "\"Pre-flight\" Lab Setup"
    ]
  },
  {
    "objectID": "weeks/01/lab/index.html#putting-it-all-together-get-lab-01",
    "href": "weeks/01/lab/index.html#putting-it-all-together-get-lab-01",
    "title": "Lab ‚ÄúPre-flight‚Äù Setup",
    "section": "",
    "text": "Now that you have the basics configured, let‚Äôs try this out to get setup with the first lab:\n\n\n\n\n\n\nCautionüìö Lab 01\n\n\n\nGithub Classroom Assignment\n\n\nOnce you‚Äôve cloned the assignment to your computer you can start going through tutorials in this order:\n\nREADME.md - VSCode introduction and configuration\nindex.qmd - Quarto & Python intro\nnotebooks/python-quickstart.py - Python fundamentals\n\n\n\n\n\n\n\nWarning\n\n\n\nIf the link above does not work for you please send your Your user ‚Äúhandle‚Äù on github.com that identifies you when using git commands (local) and interacting with Github.com (remote). E.g. Eshin‚Äôs is @ejollygithub-userid to Eshin on Slack so he can add you to the Github classroom!",
    "crumbs": [
      "Schedule",
      "Week 1",
      "\"Pre-flight\" Lab Setup"
    ]
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Note\n\n\n\nWe‚Äôve marked future assignments as TBD to allow us to adjust the pacing as needed.\nIn total, we‚Äôll aim to cover in total: ~4-5 HWs and ~6-7 labs\n\n\n\n\n\n\nWeek\nDate\nContent\nAssignment Due?\n\n\n\n\n1\nMon Jan 5\nCourse Intro\nN/A\n\n\n1\nTues Jan 6\nLaptop setup, Github Clasroom workflow, quarto, Python notebooks\npush lab 01 at least once\n\n\n1\nWed Jan 6\nPython essentials, dataframes, meet polars\npush updated lab 01 at least once\n\n\n2\nMon Jan 12\nTwo cultures of statistics, sampling theory\nskim readings for Wed Jan 14\n\n\n2\nTues Jan 13\nData visualization, meet seaborn & matplotlib\npush lab 02 at least once\n\n\n2\nWed Jan 14\nWhat is a model? Model-based thinking\npush HW 1 by Tues Jan 20\n\n\n3\nMon Jan 19\nNo class holiday\n-\n\n\n3\nTues Jan 20\nHW 1 review; Python Q&A\npush HW 1 by 1pm\n\n\n3\nWed Jan 21\nHypothesis testing as model comparison\n-\n\n\n\n\n\nTentative schedule\n\n\n\n\n\n\n\n\nWeek\nDate\nContent\nAssignment Due?\n\n\n\n\n4\nMon Jan 26\nStatistical Inference & Resampling\nN/A\n\n\n4\nTues Jan 27\nBootstrap, permutation, simulation, meet numpy & scipy\npush lab 03 at least once\n\n\n4\nWed Jan 28\nGLM I: Foundations\npush HW 2 by Midnight Mon Feb 1\n\n\n5\nMon Feb 2\nNo class, prospective interviews\nN/A\n\n\n5\nTues Feb 3\nHW 2 review; meet bossanova\npush lab 03 at least once\n\n\n5\nWed Feb 4\nGLM II\nTBD\n\n\n6\nMon Feb 9\nMarginal effects estimation\nTBD\n\n\n6\nTues Feb 10\nModels as tools: inference, prediction, exploration\npush lab 04 at least once\n\n\n6\nWed Feb 11\nFinal Project planning\nschedule meeting with instructors\n\n\n7\nMon Feb 16\nNo class holiday\n-\n\n\n7\nTues Feb 17\nLMMs I\npush lab 05 at least once\n\n\n7\nWed Feb 18\nLMMs II: Complete/partial/non-pooling\npush HW 3 by Midnight Mon Feb 23\n\n\n8\nMon Feb 23\nLMMs III: RFX syntax, ‚Äúrm-ANOVA‚Äù\nTBD\n\n\n8\nTues Feb 24\nHW 3 review; essence of linear algebra\nTBD\n\n\n8\nWed Feb 25\nUnsupervised Learning II: PCA and friends, meet sklearn\npush HW 4 by Midnight Mar 2\n\n\n9\nMon Mar 2\nTBD\nTBD\n\n\n9\nTues Mar 3\nHW 4 reivew; TBD\nTBD\n\n\n9\nWed Mar 4\nTBD\nTBD\n\n\n10\nMon Mar 9\nfinal project time\n-\n\n\n10\nTues Mar 10\nfinal project time\n-\n\n\n10\nWed Mar 11\nCourse Wrap-Up\n-\n\n\nFW\nMar 16-18\nwork on final project\npush Final Project by TBD"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistical Intuitions for Social Scientists",
    "section": "",
    "text": "Current version: Winter 2026\nPre-requisites: PSYC 201A or instructor approval\nRapid advances in computing have revolutionized modern statistical practice, offering approaches that transcend traditional training in psychological statistics (Efron et al 2016). And yet at the heart of these developments are just a handful of fundamental ideas (Gelman 2021). This course is designed to help you interactively develop your own statistical intuitions about these ideas using the Python programming language. At the core of the class is a deep understanding of the General-Linear-Model (GLM) and its extensions (e.g.¬†multi-level models), through which you‚Äôll learn how to adopt model-based thinking rather than classic statistical ritualization. We‚Äôll also explore the ‚ÄúTwo Cultures‚Äù of statistical modeling (explanation vs prediction) (Breiman, 2001), integrating ideas from both to build a robust foundation for you to pursue more advanced topics & coursework (e.g.¬†machine-learning, econometrics).\nThe ‚Äúliving‚Äù open-course materials are available at https://stat-intuitions.com and developed with the following goals in mind:\n\nServe as the primary resource for all course related materials (e.g.¬†slides, readings, notebooks, etc)\n\nBe openly accessible to all past, current, future, students and the general public (live lectures & grading currently only available for enrolled students)\nServe as a reference resource for members of the Psych Department at UCSD, continually updated each course year and between course offerings\n\n\n\n\nLearn to adopt model-based-thinking rather than statistical ritualization\nAcquire a deep understanding of the General-Linear-Model (GLM) and its extensions\nDevelop statistical & inferential intuitions from first principles using modern computational approaches (e.g.¬†simulation, resampling, permutation)\nDevelop cross-disciplinary technical & conceptual skills that lay the foundation for advanced coursework (e.g.¬†deep-learning, econometrics)\n\n\n\n\nLecturer: Eshin Jolly\nTA: Jane Yang\nOffice Hours: Slack or by appointment\n\n\n\nCommunication: Slack\nLocation: Mandler 3545 (Crick Conference Room)\nSchedule: M/T/W 2-3:50pm\n\n\n\n\n\n\nNote\n\n\n\nThe week-by-week schedule is available on the schedule page\n\n\n\n\n\n\n\nWe will be using Github Classroom to manage all course materials (labs, HWs, interactive lectures). Each week, we‚Äôll update the course website with a new assignment repository link that we‚Äôll keep updated that that week‚Äôs materials. At the start of class/lab, or when a HW problem-set is made available, you should accept assignments and git clone them to your local computer to work interactively.\nWhen you‚Äôre finished with an assignment or when you want to get feedback on work-in-progress, you should commit your changes to your local copy of the assignment, and then push them to Github. This will allow your instructors to review your work, provide Feedback, and/or have a private discussion with you while referencing questions/issues in your code directly. At the same time, you‚Äôll be building up a set of references (with feedback) that you can always check-out and refresh after this class is over!\n\n\n\nWhen in doubt, this course website should be the first place you look for any logistical information! We‚Äôll update it regularly and each week with a new sidebar section.\n\n\n\nAll course communications will occur over Slack in #w26-201b channel. Keep an eye out here for all announcements, additional links/resources, and logistics updates.\n\n\n\n\nWe ºre interested in grading you on your ability to achieve the skill sets that are taught in this course regardless of your starting experience with Python. For this reason, you can attempt any Github Classroom assignment (lab or HW) multiple times, especially if you think you could do better or if you want to incorporate instructor feedback. Practically, this just means making additional code changes and pushing another commit to your assignment. Your instructors will automatically be able to see your code changes and your latest submission. We ºll grade you based partially on your accurate completion of the assignment, but mostly on your ability to demonstrate: - You attempted the assignment in good-faith (lecture, lab, or HW notebooks) - You made effort to clearly document and explain your thought process, reasoning, code, and where/why you got stuck if you did - What attempts you made to fix issues you ran into, how you approached debugging, and what you learned from the process - Why you made a particular choice in your code/analysis, and/or what assumptions you made for a particular statistical inference\n\n\n\nComponent\nWeight\n\n\n\n\nLabs & Engagement\n30%\n\n\nHomeworks\n40%\n\n\nFinal Project\n30%\n\n\n\n\n\n\nAdapted from the UC San Diego & University of Waterloo Academic Integrity Offices\n\n\n\n\n\n\nWarningGenAI is known to fabricate sources/facts and can perpetuate biases/misunderstanding\n\n\n\nYou should also be aware that there are copyright and privacy concerns with these tools. You should exercise caution when using large portions of content from AI sources for these reasons. Also, you are accountable for the content and accuracy of all work you submit in this class, including any supported by generative AI.\n\n\nWe encourage the use of Generative artificial intelligence (GenAI) tools like OpenAI‚Äôs ChatGPT, Anthropic‚Äôs Claude, and/or Google‚Äôs Gemini to help you master concepts and skills in this class in accordance with the UCSD Academic Integrity Guidelines on GenAI and the following guidelines:\n\nIf you use GenAI for any submitted coursework, you must attach a link or text transcript to any assignments you submit. Many services offer a ‚Äúshare your chat‚Äù link-creation function or you can use a Google Chrome Browser Extension like ChatGPT Exporter or Claude Exporter. This will help us provide feedback on using LLM tools effectively (if desired) and make it transparent to us how you are completing assignments, while respecting the standards of academic integrity.\nDirectly prompting GenAI with course assignments, or copying/pasting GenAI output instead of performing the work yourself, will not earn you assignment credit and could result in an academic integrity violation.\n\nInstead you should aim to master GenAI as tools that supplement your programming and critical thinking skills, not as a substitute for them. They can be especially helpful for: debugging and troubleshooting unfamiliar code, reviewing Python fundamentals, reasoning about statistical concepts via analogy/example, or simply conversing in natural language about technical concepts.\n\n\n\nAll students are expected to adhere to standards of academic integrity. Cheating of any kind on any assignment will not be tolerated. It is disrespectful to your peers, the university, and to your instructors. If you are unsure what might constitute a violation of academic integrity, ask your instructors and/or the UCSD website on academic integrity: http://academicintegrity.ucsd.edu. Any evidence of academic misconduct will be reported to the Academic Integrity Office.\n\n\n\nFamily emergencies and illness are excused absences, as per UCSD policy. Please do not come to class if you have active symptoms (instead, please rest!). In general, absences will have a direct impact on your ability to learn the skills presented in this course as well as your participation grade.\nThat being said, life happens and we genuinely care about your well-being. Sometimes you simply can‚Äôt be in class or turn in an assignment on time. There may also be times when I‚Äôm unable to make it to class for a given reason, and I will ask for your grace and understanding then as well. Please, prioritize your well-being in graduate school and use this class as a way for you to learn skills that will be useful for your career (versus focusing on passing the requirements for a grade).\n\n\n\nAny student with a documented disability will be accommodated according to University policy. For details, please consult the Office of Students with Disabilities (OSD): http://disabilities.ucsd.edu. If you require accommodation for any component of the course, please provide the instructor with documentation from OSD as soon as possible. Please note that accommodations cannot be made retroactively under any circumstances.\n\nThis syllabus is subject to change. Check the course website (stat-intuitions.com) for the most up-to-date information."
  },
  {
    "objectID": "index.html#how-well-learn",
    "href": "index.html#how-well-learn",
    "title": "Statistical Intuitions for Social Scientists",
    "section": "",
    "text": "We will be using Github Classroom to manage all course materials (labs, HWs, interactive lectures). Each week, we‚Äôll update the course website with a new assignment repository link that we‚Äôll keep updated that that week‚Äôs materials. At the start of class/lab, or when a HW problem-set is made available, you should accept assignments and git clone them to your local computer to work interactively.\nWhen you‚Äôre finished with an assignment or when you want to get feedback on work-in-progress, you should commit your changes to your local copy of the assignment, and then push them to Github. This will allow your instructors to review your work, provide Feedback, and/or have a private discussion with you while referencing questions/issues in your code directly. At the same time, you‚Äôll be building up a set of references (with feedback) that you can always check-out and refresh after this class is over!\n\n\n\nWhen in doubt, this course website should be the first place you look for any logistical information! We‚Äôll update it regularly and each week with a new sidebar section.\n\n\n\nAll course communications will occur over Slack in #w26-201b channel. Keep an eye out here for all announcements, additional links/resources, and logistics updates."
  },
  {
    "objectID": "index.html#mastery-based-grading",
    "href": "index.html#mastery-based-grading",
    "title": "Statistical Intuitions for Social Scientists",
    "section": "",
    "text": "We ºre interested in grading you on your ability to achieve the skill sets that are taught in this course regardless of your starting experience with Python. For this reason, you can attempt any Github Classroom assignment (lab or HW) multiple times, especially if you think you could do better or if you want to incorporate instructor feedback. Practically, this just means making additional code changes and pushing another commit to your assignment. Your instructors will automatically be able to see your code changes and your latest submission. We ºll grade you based partially on your accurate completion of the assignment, but mostly on your ability to demonstrate: - You attempted the assignment in good-faith (lecture, lab, or HW notebooks) - You made effort to clearly document and explain your thought process, reasoning, code, and where/why you got stuck if you did - What attempts you made to fix issues you ran into, how you approached debugging, and what you learned from the process - Why you made a particular choice in your code/analysis, and/or what assumptions you made for a particular statistical inference\n\n\n\nComponent\nWeight\n\n\n\n\nLabs & Engagement\n30%\n\n\nHomeworks\n40%\n\n\nFinal Project\n30%"
  },
  {
    "objectID": "index.html#generative-ai-course-policy",
    "href": "index.html#generative-ai-course-policy",
    "title": "Statistical Intuitions for Social Scientists",
    "section": "",
    "text": "Adapted from the UC San Diego & University of Waterloo Academic Integrity Offices\n\n\n\n\n\n\nWarningGenAI is known to fabricate sources/facts and can perpetuate biases/misunderstanding\n\n\n\nYou should also be aware that there are copyright and privacy concerns with these tools. You should exercise caution when using large portions of content from AI sources for these reasons. Also, you are accountable for the content and accuracy of all work you submit in this class, including any supported by generative AI.\n\n\nWe encourage the use of Generative artificial intelligence (GenAI) tools like OpenAI‚Äôs ChatGPT, Anthropic‚Äôs Claude, and/or Google‚Äôs Gemini to help you master concepts and skills in this class in accordance with the UCSD Academic Integrity Guidelines on GenAI and the following guidelines:\n\nIf you use GenAI for any submitted coursework, you must attach a link or text transcript to any assignments you submit. Many services offer a ‚Äúshare your chat‚Äù link-creation function or you can use a Google Chrome Browser Extension like ChatGPT Exporter or Claude Exporter. This will help us provide feedback on using LLM tools effectively (if desired) and make it transparent to us how you are completing assignments, while respecting the standards of academic integrity.\nDirectly prompting GenAI with course assignments, or copying/pasting GenAI output instead of performing the work yourself, will not earn you assignment credit and could result in an academic integrity violation.\n\nInstead you should aim to master GenAI as tools that supplement your programming and critical thinking skills, not as a substitute for them. They can be especially helpful for: debugging and troubleshooting unfamiliar code, reviewing Python fundamentals, reasoning about statistical concepts via analogy/example, or simply conversing in natural language about technical concepts."
  },
  {
    "objectID": "index.html#academic-integrity",
    "href": "index.html#academic-integrity",
    "title": "Statistical Intuitions for Social Scientists",
    "section": "",
    "text": "All students are expected to adhere to standards of academic integrity. Cheating of any kind on any assignment will not be tolerated. It is disrespectful to your peers, the university, and to your instructors. If you are unsure what might constitute a violation of academic integrity, ask your instructors and/or the UCSD website on academic integrity: http://academicintegrity.ucsd.edu. Any evidence of academic misconduct will be reported to the Academic Integrity Office."
  },
  {
    "objectID": "index.html#absence-policy",
    "href": "index.html#absence-policy",
    "title": "Statistical Intuitions for Social Scientists",
    "section": "",
    "text": "Family emergencies and illness are excused absences, as per UCSD policy. Please do not come to class if you have active symptoms (instead, please rest!). In general, absences will have a direct impact on your ability to learn the skills presented in this course as well as your participation grade.\nThat being said, life happens and we genuinely care about your well-being. Sometimes you simply can‚Äôt be in class or turn in an assignment on time. There may also be times when I‚Äôm unable to make it to class for a given reason, and I will ask for your grace and understanding then as well. Please, prioritize your well-being in graduate school and use this class as a way for you to learn skills that will be useful for your career (versus focusing on passing the requirements for a grade)."
  },
  {
    "objectID": "index.html#osd-accommodations",
    "href": "index.html#osd-accommodations",
    "title": "Statistical Intuitions for Social Scientists",
    "section": "",
    "text": "Any student with a documented disability will be accommodated according to University policy. For details, please consult the Office of Students with Disabilities (OSD): http://disabilities.ucsd.edu. If you require accommodation for any component of the course, please provide the instructor with documentation from OSD as soon as possible. Please note that accommodations cannot be made retroactively under any circumstances.\n\nThis syllabus is subject to change. Check the course website (stat-intuitions.com) for the most up-to-date information."
  },
  {
    "objectID": "guides/python-resources.html",
    "href": "guides/python-resources.html",
    "title": "Python Resources",
    "section": "",
    "text": "Terminal commands cheatsheet\nPython basics cheatsheet\nPython interactive reference\nPython for R users"
  },
  {
    "objectID": "guides/python-resources.html#basics",
    "href": "guides/python-resources.html#basics",
    "title": "Python Resources",
    "section": "",
    "text": "Terminal commands cheatsheet\nPython basics cheatsheet\nPython interactive reference\nPython for R users"
  },
  {
    "objectID": "guides/python-resources.html#python-libraries-well-use",
    "href": "guides/python-resources.html#python-libraries-well-use",
    "title": "Python Resources",
    "section": "Python libraries we‚Äôll use",
    "text": "Python libraries we‚Äôll use\n\nWhen you‚Äôre working in Python it can be super helpful to regularly refer to these resources. Remember that you can always use any API reference link below to get a comprehensive list of all the functions and methods in a library - a bit nicer than only relying on ? in your notebook.\n\nThroughout the course we‚Äô‚Äôll make use of the following Python libraries in case you want quickly reference their documentation:\n\npolars\nseaborn\nmatplotlib\nscipy\nnumpy\nbossanova\nscikit-learn\n\n\npolars - DataFrames & tidy data analysis\n\nPolars user guide\nPolars API reference\nTidyverse and Polars side-by-side\nPolars Rgonomic patterns\nPandas - alternative DataFrame library we‚Äôre NOT using\n\n\n\nseaborn - high-level statistical visualizations\n\nSeaborn user guide\nSeaborn API\nSeaborn cheatsheet\n\n\n\nmatplotlib - lower-level plot customization\n\nMatplotlib user guide\nMatplotlib API reference\nMatplotlib tutorials\nMatplotlib cheatsheets\n\n\n\nscipy - scientific functions & basic stats\n\nSciPy user guide\nSciPy API reference\nSummary statistics\nResampling, i.e.¬†montecarlo, bootstrap, permutation\nHypothesis testing\n\n\n\nnumpy - arrays, matrices, and linear algebra\n\nNumpy tutorials\nNumpy API reference\nNumpy Cheatsheet\nNumpy for MATLAB users\n\n\n\nbossanova - intuitive formula-based statistical modeling\n\nDocumentation & tutorials\n\n\n\nscikit-learn - machine-learning\n\nScikit-learn cheatsheet\nSupervised learning\nDecomposition\nModel selection & evaluations"
  },
  {
    "objectID": "guides/classroom-guide.html",
    "href": "guides/classroom-guide.html",
    "title": "Github Classroom",
    "section": "",
    "text": "We‚Äôll be using Github Classroom to share all resources for class. This is the primary way you should be downloading and working with course materials. Each week, we‚Äôll create a new Github Classroom assignment link (prefixed with üìö). Clicking it will automatically create a github repository for you containing all the materials you need.\nYou‚Äôll then be able to clone this repository to your computer, working through files interactively, make edits/updates, and commit and submit your assignment for review. Each time you push your work to Github, your instructors will be able to provide review, feedback, and discussions that directly reference your code.\nYou‚Äôll always be able to access your assignment repositories, history, and instructor feedback after the course is over. So the more effort you put into assignments, the more you engage with instructors, the more you‚Äôll learn, and the higher quality resources you‚Äôll have for your own future reference!"
  },
  {
    "objectID": "guides/classroom-guide.html#getting-assignments",
    "href": "guides/classroom-guide.html#getting-assignments",
    "title": "Github Classroom",
    "section": "Getting Assignments",
    "text": "Getting Assignments\n\nOpen any course link that starts with üìö\nAccept the assignment in your browser\nClick the URL to go the auto-created github repo (this will always be named assignment-name-YOUR-GITHUB-USERNAME)\nClone it to your local computer using git clone\nOpen and work on any notebook files using VSCode\nCommit your changes locally using git add & git commit\nPush your changes to github using git push\nRespond to any feedback discussions under the ‚ÄúPull Requests‚Äù tab on the github repo"
  },
  {
    "objectID": "guides/classroom-guide.html#updating-assignments",
    "href": "guides/classroom-guide.html#updating-assignments",
    "title": "Github Classroom",
    "section": "Updating Assignments",
    "text": "Updating Assignments\nOccasionally, we‚Äôll update assignments that you‚Äôve already accepted and git clone-d to your local computer with additional files (e.g.¬†solutions). Here‚Äôs how you can git pull them to your local computer\n\nFollow the assignment link to go to the repository on github.com that you cloned to first start the assignment. It will be named MM-DD-YOURGITHUBID, e.g.¬†‚Äú01-21-ejolly‚Äù\nClick on ‚ÄúPull Requests‚Äù\nClick on ‚ÄúGithub Classroom: Sync Assignment‚Äù\nClick on green ‚ÄúMerge pull request‚Äù button\nAfter the button turns purple, indicating the ‚Äúmerge is complete‚Äù open up a terminal on your local computer and cd into the folder you cloned from this repository, e.g.¬†‚Äú01-21-ejolly‚Äù\nUse git status to check if you‚Äôve saved some changes, but haven‚Äôt yet git commit them. If you don‚Äôt see ‚Äúnothing to commit, working tree clean‚Äù, you‚Äôll need to git add and git commit your changed files\nRun git pull to download the latest changes from github\nIf you get any wonky error message, run git merge --no-ff, and then type the following commands to exit the window that opens: :, w, q, enter\n\nIf everything worked you should see some new files in your local folder, and you can hack on them as you normally would."
  },
  {
    "objectID": "glossary.html",
    "href": "glossary.html",
    "title": "Course Terminology",
    "section": "",
    "text": "Note\n\n\n\nWe‚Äôll keep this page updated as we come across new technical and statistical terms for easy reference\n\n\n\n\n Term  Definition \nestimatorA rule for aggregating data (e.g.¬†mean) under a set of assumptions defined by a loss-function (e.g.¬†SSE)\ncentral-limit-theoremThe sampling distribution of an estimator converges to a normal (gaussian) distribution even if the underlying samples are not normally distributed.\ngithubA online cloud-based service for synchronize local git repositories with REMOTE repositories on github.com. This faciliates collaborative works flows and open-source development. We‚Äôre using the Github Classroom feature built-up on this for our course.\ncommand-line-programA program that you interact with exclusively from your terminal; often abbreviated as CLI\nlaw-of-large-numbersAn estimator stabilizes to a true theoretical value (population statistic) as we aggregate more independent observations.\nloss-functionQuantifies the disparity between an estimator and observations (e.g squared vs absolue errors).\nshellA program that runs automatically when your Terminal starts and interprets the commands you type to control your computer instead of pointing-and-clicking. FYI: the default shell on macOS is zsh not bash.\ngithub-useridYour user ‚Äúhandle‚Äù on github.com that identifies you when using git commands (local) and interacting with Github.com (remote). E.g. Eshin‚Äôs is @ejolly\nenvironmentReally just a hidden folder on your computer (typically .venv/) that contains an isolated Python installation with all additional libraries and tools. uv handles this all for us!\ngitA CLI to version control LOCAL files and folders called repositories. See the git guide for more details and a command cheatsheet.\nrstudioAn IDE originally designed to work with R, but also works well with Quarto documents. Also includes a Terminal separate from the R console for running shell commands.\nuvA library and environment manager for Python making it easy to create/add/update additional Python libraries & tools in a reproducible and isolated way. using a pyproject.toml ‚Äúblueprints‚Äù file\nmarimoA program like Quarto that can render .py files as interactive Python notebooks with code cells. FYI: marimo is the modern alternative to Jupyter notebooks which you may have heard of/used in the past.\nquartoa scientific publishing tool that allows you to mix prose and code-cells to render executable documents in a variety of formats (website, PDF, etc)\nterminalAn application for controlling your computer via commands that you type in (e.g.¬†cd, ls) instead of point-and-click\nvscodeAn extremely popular general purpose IDE that supports multiple language (e.g.¬†Python, R, Javascript) and makes use of extensions to add additional functionality (e.g.¬†quarto rendering, Python notebooks).\npython-notebook.py files that we can work with interactively using code-cells and markdown-cells (similar to quarto chunks). A much richer interface for interactively working with pieces of code one-at-a-time\nideIntegrated-development-environment; a program that includes a code-editor, terminal/console, and other helpful features to be a ‚Äúone-stop-shop‚Äù for most of your needs\nhomebrewA command-line package manager for macOS that lets you install packages and applications using the brew command\nno-free-lunch-theoremAny estimator that performs well somewhere, must perform poorly elsewhere.\nquarto-doc.qmd files that contain a mix of prose (markdown) and code-chunks (Python/R) that you can preview and render with Quarto. By default quarto will always rerun ALL code-chunks in the file upon saving."
  },
  {
    "objectID": "guides/git-guide.html",
    "href": "guides/git-guide.html",
    "title": "Git & Github",
    "section": "",
    "text": "In this course we‚Äôll assume that you‚Äôre at least somewhat familiar with git and github. If not you can toggle the drop-down below for a slide-deck that provides a high-level conceptual overview of version control using the analogy of a ‚Äúsocial time-machine.‚Äù"
  },
  {
    "objectID": "guides/git-guide.html#most-common-commands",
    "href": "guides/git-guide.html#most-common-commands",
    "title": "Git & Github",
    "section": "Most common commands",
    "text": "Most common commands\nRather than spend time on nitty-gritty details of git, we‚Äôre providing a list of the most common commands you‚Äôll use in class (and in your day-to-day work!).\n\ngit status\nSee what files are ready to be made into a ‚Äúsnapshot‚Äù (committed) and which ones are not being kept track of\n\n\n\ngit add\nAdd one or more files to the list of files that should be made into a ‚Äúsnapshot‚Äù (committed) \n\n\ngit reset\nRemove one or more file from the list of files that should be made into a ‚Äúsnapshot‚Äù (committed).\nThis doesn‚Äôt remove/delete files. It just removes them from the file you plan to include in this commit.\n\n\ngit commit\nTake a ‚Äúsnapshot‚Äù of all currently tracked project files. Files need to be ‚Äúprepped‚Äù (staged) for commit using git add beforehand. You‚Äôll almost always use the -m 'some commit message' flag when running this command. These messages will then appear in the git log!\n\n\n\ngit log\nSee the full historical timeline of the project\n\n\n\ngit init\nCreate a new git repository for the first time (will not add any files)"
  },
  {
    "objectID": "guides/git-guide.html#commandsoperations-that-work-with-github",
    "href": "guides/git-guide.html#commandsoperations-that-work-with-github",
    "title": "Git & Github",
    "section": "Commands/operations that work with github",
    "text": "Commands/operations that work with github\nThe following commands communicate between your local computer‚Äôs git repository and a remote github repository.\n\ngit clone\nDuplicate a remote repository (e.g.¬†github) on your local computer\n\n\n\ngit push\nSend latest local changes to a remote location (e.g.¬†github). You‚Äôll run this command after you‚Äôve performed a git commit\n\n\n\ngit pull\nGet the latest changes from a remote location (e.g.¬†github)\n\n\n\nforking\nCopy a remote repository on github, to your own remote account on github. This isn‚Äôt a command per se, but a way to create a copy of another project on Github that you can then clone to your own computer. This is useful when you want to work on your own independent copy of another project, while still being able to suggest changes to the original project owner via a pull request.\n\n\n\npull request\nNotify a github (remote) repository owner you would like them to review+incorporate your commits. You can make a PR against a repository you own or one that someone else owns. PRs are the predominant way that you can collaborate and integrate changes between group members on github."
  },
  {
    "objectID": "guides/git-guide.html#more-advanced-git",
    "href": "guides/git-guide.html#more-advanced-git",
    "title": "Git & Github",
    "section": "More advanced git",
    "text": "More advanced git\nWe won‚Äôt necessarily be making much of use of the following commands in class, but they‚Äôre useful to know about for your own projects.\n\ngit branch\nCreate a new independent ‚Äútimeline‚Äù for the project. This is the ‚Äútrue power‚Äù of git, where you can create a totally independent copy of your project from any point in time (i.e.¬†any commit), without affecting the original project. Branches can be useful for working on different features/ideas/etc or even collaborating with other people.\n\n\n\ngit revert\nUndo changes by reversing any specific ‚Äúsnapshot‚Äù (commit). Think of this is a ‚Äúrollback‚Äù command that adds an entry to your project timeline. In other words, in addition to ‚Äúundoing‚Äù a previous commit, we also keep a record of this ‚Äúundo‚Äù using another commit."
  },
  {
    "objectID": "guides/terminology.html",
    "href": "guides/terminology.html",
    "title": "Course Terminology",
    "section": "",
    "text": "Note\n\n\n\nWe‚Äôll keep this page updated as we come across new technical and statistical terms for easy reference\n\n\n\n\n Term  Definition \nterminalAn application for controlling your computer via commands that you type in (e.g.¬†cd, ls) instead of point-and-click\ngithubA online cloud-based service for synchronize local git repositories with REMOTE repositories on github.com. This faciliates collaborative works flows and open-source development. We‚Äôre using the Github Classroom feature built-up on this for our course.\nloss-functionQuantifies the disparity between an estimator and observations (e.g squared vs absolue errors).\ngithub-useridYour user ‚Äúhandle‚Äù on github.com that identifies you when using git commands (local) and interacting with Github.com (remote). E.g. Eshin‚Äôs is @ejolly\nenvironmentReally just a hidden folder on your computer (typically .venv/) that contains an isolated Python installation with all additional libraries and tools. uv handles this all for us!\nrstudioAn IDE originally designed to work with R, but also works well with Quarto documents. Also includes a Terminal separate from the R console for running shell commands.\npython-notebook.py files that we can work with interactively using code-cells and markdown-cells (similar to quarto chunks). A much richer interface for interactively working with pieces of code one-at-a-time\nestimatorA rule for aggregating data (e.g.¬†mean) under a set of assumptions defined by a loss-function (e.g.¬†SSE)\nno-free-lunch-theoremAny estimator that performs well somewhere, must perform poorly elsewhere.\nvscodeAn extremely popular general purpose IDE that supports multiple language (e.g.¬†Python, R, Javascript) and makes use of extensions to add additional functionality (e.g.¬†quarto rendering, Python notebooks).\nideIntegrated-development-environment; a program that includes a code-editor, terminal/console, and other helpful features to be a ‚Äúone-stop-shop‚Äù for most of your needs\ncommand-line-programA program that you interact with exclusively from your terminal; often abbreviated as CLI\ngitA CLI to version control LOCAL files and folders called repositories. See the git guide for more details and a command cheatsheet.\nquarto-doc.qmd files that contain a mix of prose (markdown) and code-chunks (Python/R) that you can preview and render with Quarto. By default quarto will always rerun ALL code-chunks in the file upon saving.\nmarimoA program like Quarto that can render .py files as interactive Python notebooks with code cells. FYI: marimo is the modern alternative to Jupyter notebooks which you may have heard of/used in the past.\nuvA library and environment manager for Python making it easy to create/add/update additional Python libraries & tools in a reproducible and isolated way. using a pyproject.toml ‚Äúblueprints‚Äù file\nshellA program that runs automatically when your Terminal starts and interprets the commands you type to control your computer instead of pointing-and-clicking. FYI: the default shell on macOS is zsh not bash.\nhomebrewA command-line package manager for macOS that lets you install packages and applications using the brew command\nlaw-of-large-numbersAn estimator stabilizes to a true theoretical value (population statistic) as we aggregate more independent observations.\ncentral-limit-theoremThe sampling distribution of an estimator converges to a normal (gaussian) distribution even if the underlying samples are not normally distributed.\nquartoa scientific publishing tool that allows you to mix prose and code-cells to render executable documents in a variety of formats (website, PDF, etc)"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "References"
  },
  {
    "objectID": "weeks/01/index.html",
    "href": "weeks/01/index.html",
    "title": "Week 1",
    "section": "",
    "text": "This week we‚Äôll cover course logistics, introduce the two cultures of statistical modeling, and discuss some of the foundational concepts that set the stage for later weeks. We‚Äôll also take the time to make sure your computing environment is properly configured and take our first steps with Python.",
    "crumbs": [
      "Schedule",
      "Week 1",
      "Overview"
    ]
  },
  {
    "objectID": "weeks/01/index.html#overview",
    "href": "weeks/01/index.html#overview",
    "title": "Week 1",
    "section": "",
    "text": "This week we‚Äôll cover course logistics, introduce the two cultures of statistical modeling, and discuss some of the foundational concepts that set the stage for later weeks. We‚Äôll also take the time to make sure your computing environment is properly configured and take our first steps with Python.",
    "crumbs": [
      "Schedule",
      "Week 1",
      "Overview"
    ]
  },
  {
    "objectID": "weeks/01/index.html#slides",
    "href": "weeks/01/index.html#slides",
    "title": "Week 1",
    "section": "Slides",
    "text": "Slides\n\n\n\n\n\n\nTipüõù Mon Jan 5th - Course Intro",
    "crumbs": [
      "Schedule",
      "Week 1",
      "Overview"
    ]
  },
  {
    "objectID": "weeks/01/index.html#materials",
    "href": "weeks/01/index.html#materials",
    "title": "Week 1",
    "section": "Materials",
    "text": "Materials\n\n‚ÄúPre-Flight‚Äù Lab Setup (start here)\n\n\n\n\n\n\n\nCautionüìö Lab 01 - Python basics & polars (dataframes) intro\n\n\n\nGitHub Classroom Assignment\n\n\nRemember: There are additional Git/GitHub & Python resources available in the ‚ÄúGuides & Resources‚Äù section above!",
    "crumbs": [
      "Schedule",
      "Week 1",
      "Overview"
    ]
  },
  {
    "objectID": "weeks/01/index.html#mentioned-references",
    "href": "weeks/01/index.html#mentioned-references",
    "title": "Week 1",
    "section": "Mentioned References",
    "text": "Mentioned References\n\nStatistical Thinking\nProgramming as theory-building",
    "crumbs": [
      "Schedule",
      "Week 1",
      "Overview"
    ]
  },
  {
    "objectID": "weeks/01/lab/polars-crash-course.html",
    "href": "weeks/01/lab/polars-crash-course.html",
    "title": "A Crash Course on Python DataFrames",
    "section": "",
    "text": "In this tutorial we‚Äôll build on your basic Python skills and immediately start working with a new kind of object: DataFrame. We‚Äôll meet the first Python library we‚Äôll use throughout the course polars and walkthrough all the basics in this notebook.\nPolars is very user-friendly DataFrame library for working with structured data in Python, but also R and other languages.",
    "crumbs": [
      "Schedule",
      "Week 1",
      "Polars (dataframes) Crash Course"
    ]
  },
  {
    "objectID": "weeks/01/lab/polars-crash-course.html#how-to-use-this-notebook",
    "href": "weeks/01/lab/polars-crash-course.html#how-to-use-this-notebook",
    "title": "A Crash Course on Python DataFrames",
    "section": "How to use this notebook",
    "text": "How to use this notebook\nThis notebook is designed for you to work through at your own pace or use as a reference with other materials.\nAs you go through this notebook, you should regularly refer to the polars documentation to look things up and general help. Try experimenting by creating new code cells and playing around with the demonstrated functionality.\nRemember to use help() from within this notebook to look up how functionality works.",
    "crumbs": [
      "Schedule",
      "Week 1",
      "Polars (dataframes) Crash Course"
    ]
  },
  {
    "objectID": "weeks/01/lab/polars-crash-course.html#how-to-import-polars",
    "href": "weeks/01/lab/polars-crash-course.html#how-to-import-polars",
    "title": "A Crash Course on Python DataFrames",
    "section": "How to import Polars",
    "text": "How to import Polars\nWe can make polars available by using the import statement. It‚Äôs convention to import polars in the following way:\n\n\nCode\nimport polars as pl",
    "crumbs": [
      "Schedule",
      "Week 1",
      "Polars (dataframes) Crash Course"
    ]
  },
  {
    "objectID": "weeks/01/lab/polars-crash-course.html#why-use-polars",
    "href": "weeks/01/lab/polars-crash-course.html#why-use-polars",
    "title": "A Crash Course on Python DataFrames",
    "section": "Why use Polars?",
    "text": "Why use Polars?\nSo far we‚Äôve made most use of Python lists and NumPy arrays. But in practice you‚Äôre probably working with some kind of structured data, i.e.¬†spreadsheet-style data with columns and rows\n\nIn Polars we call this a DataFrame, a 2d table with rows and columns of different types of data (e.g.¬†strings, numbers, etc).\n\nEach column of a DataFrame contains the same type of data. Let‚Äôs look at an example by loading a file with the pl.read_csv() function.\nThis will return a DataFrame we can check out:\n\n\nCode\ndf = pl.read_csv('data/example.csv')\ndf\n\n\n\nshape: (3, 3)\n\n\n\nName\nAge\nSex\n\n\nstr\ni64\nstr\n\n\n\n\n\"Braund, Mr. Owen Harris\"\n22\n\"male\"\n\n\n\"Allen, Mr. William Henry\"\n35\n\"male\"\n\n\n\"Bonnell, Miss. Elizabeth\"\n58\n\"female\"\n\n\n\n\n\n\nNotice how Polars tells us the type of each column below it‚Äôs name.",
    "crumbs": [
      "Schedule",
      "Week 1",
      "Polars (dataframes) Crash Course"
    ]
  },
  {
    "objectID": "weeks/01/lab/polars-crash-course.html#dataframe-fundamentals",
    "href": "weeks/01/lab/polars-crash-course.html#dataframe-fundamentals",
    "title": "A Crash Course on Python DataFrames",
    "section": "DataFrame fundamentals",
    "text": "DataFrame fundamentals\nWe can get basic information about a DataFrame by accessing its attributes using . syntax without () at the end:\n\n\nCode\ndf.shape\n\n\n(3, 3)\n\n\n\n\nCode\ndf.height\n\n\n3\n\n\n\n\nCode\ndf.width\n\n\n3\n\n\n\n\nCode\ndf.columns\n\n\n['Name', 'Age', 'Sex']\n\n\nDataFrames have various methods that we can use with . syntax with a () at the end.\nRemember that methods in Python are just functions that ‚Äúbelong‚Äù to some object. In this case these methods ‚Äúbelong‚Äù to a DataFrame object and can take arguments that operate on it.\nSome might be familiar from R or other languages:\n.head() gives us the first few rows. Since we only have 3 rows, we can pass an argument to the method to as for the first 2:\n\n\nCode\ndf.head(2)\n\n\n\nshape: (2, 3)\n\n\n\nName\nAge\nSex\n\n\nstr\ni64\nstr\n\n\n\n\n\"Braund, Mr. Owen Harris\"\n22\n\"male\"\n\n\n\"Allen, Mr. William Henry\"\n35\n\"male\"\n\n\n\n\n\n\nAnd .tail() is the opposite:\n\n\nCode\ndf.tail(2) # last 2 rows\n\n\n\nshape: (2, 3)\n\n\n\nName\nAge\nSex\n\n\nstr\ni64\nstr\n\n\n\n\n\"Allen, Mr. William Henry\"\n35\n\"male\"\n\n\n\"Bonnell, Miss. Elizabeth\"\n58\n\"female\"\n\n\n\n\n\n\nWe can use .glimpse() to transpose a DataFrame. This can sometimes make it easier to see the column names listed as rows and the values listed as columns:\n\n\nCode\ndf.glimpse()\n\n\nRows: 3\nColumns: 3\n$ Name &lt;str&gt; 'Braund, Mr. Owen Harris', 'Allen, Mr. William Henry', 'Bonnell, Miss. Elizabeth'\n$ Age  &lt;i64&gt; 22, 35, 58\n$ Sex  &lt;str&gt; 'male', 'male', 'female'\n\n\n\nAnd .describe() to get some quick summary stats:\n\n\nCode\ndf.describe()\n\n\n\nshape: (9, 4)\n\n\n\nstatistic\nName\nAge\nSex\n\n\nstr\nstr\nf64\nstr\n\n\n\n\n\"count\"\n\"3\"\n3.0\n\"3\"\n\n\n\"null_count\"\n\"0\"\n0.0\n\"0\"\n\n\n\"mean\"\nnull\n38.333333\nnull\n\n\n\"std\"\nnull\n18.230012\nnull\n\n\n\"min\"\n\"Allen, Mr. William Henry\"\n22.0\n\"female\"\n\n\n\"25%\"\nnull\n35.0\nnull\n\n\n\"50%\"\nnull\n35.0\nnull\n\n\n\"75%\"\nnull\n58.0\nnull\n\n\n\"max\"\n\"Braund, Mr. Owen Harris\"\n58.0\n\"male\"\n\n\n\n\n\n\nThere are many additional methods to calculate statistics as well. But we‚Äôll revisit these later:\n\n\nCode\ndf.mean()\n\n\n\nshape: (1, 3)\n\n\n\nName\nAge\nSex\n\n\nstr\nf64\nstr\n\n\n\n\nnull\n38.333333\nnull\n\n\n\n\n\n\n\n\nCode\ndf.min()\n\n\n\nshape: (1, 3)\n\n\n\nName\nAge\nSex\n\n\nstr\ni64\nstr\n\n\n\n\n\"Allen, Mr. William Henry\"\n22\n\"female\"\n\n\n\n\n\n\nDataFrames also have a .sample() method that allows you resample rows from the DataFrame with or without replacement.\nYou can tell Polars to sample all rows without replacement, aka permuting:\n\n\nCode\ndf.sample(fraction=1, shuffle=True, with_replacement=False)\n\n\n\nshape: (3, 3)\n\n\n\nName\nAge\nSex\n\n\nstr\ni64\nstr\n\n\n\n\n\"Bonnell, Miss. Elizabeth\"\n58\n\"female\"\n\n\n\"Allen, Mr. William Henry\"\n35\n\"male\"\n\n\n\"Braund, Mr. Owen Harris\"\n22\n\"male\"\n\n\n\n\n\n\nOr resample with replacement, aka bootstrapping:\n\n\nCode\ndf.sample(fraction=1, shuffle=True, with_replacement=True)\n\n\n\nshape: (3, 3)\n\n\n\nName\nAge\nSex\n\n\nstr\ni64\nstr\n\n\n\n\n\"Bonnell, Miss. Elizabeth\"\n58\n\"female\"\n\n\n\"Allen, Mr. William Henry\"\n35\n\"male\"\n\n\n\"Braund, Mr. Owen Harris\"\n22\n\"male\"\n\n\n\n\n\n\nThese methods will be handy when we cover resampling statistics later in the course.",
    "crumbs": [
      "Schedule",
      "Week 1",
      "Polars (dataframes) Crash Course"
    ]
  },
  {
    "objectID": "weeks/01/lab/polars-crash-course.html#indexing-a-dataframe-for-simple-stuff-only",
    "href": "weeks/01/lab/polars-crash-course.html#indexing-a-dataframe-for-simple-stuff-only",
    "title": "A Crash Course on Python DataFrames",
    "section": "Indexing a DataFrame (for simple stuff only!)",
    "text": "Indexing a DataFrame (for simple stuff only!)\nBecause a DataFrame is a 2d table, we can use the same indexing and slicing syntax but in 2d for rows and columns.\nRemember these are 0-indexed: the first row/col is at position 0, not position 1\nIf this is our DataFrame:\n\n\nCode\ndf\n\n\n\nshape: (3, 3)\n\n\n\nName\nAge\nSex\n\n\nstr\ni64\nstr\n\n\n\n\n\"Braund, Mr. Owen Harris\"\n22\n\"male\"\n\n\n\"Allen, Mr. William Henry\"\n35\n\"male\"\n\n\n\"Bonnell, Miss. Elizabeth\"\n58\n\"female\"\n\n\n\n\n\n\nWe can slice it like this:\n\n\nCode\n# 0 row index = 1st row\n# 1 col index = 2nd col (age)\ndf[0, 1]\n\n\n22\n\n\nAnd of course we can slice using start:stop:step, which is always up-to the end value we slice to:\n\n\nCode\n# 0:2 slice = rows up to, but not including 2 - just 0, 1\n# 0 col index = 1st col (name)\ndf[0:2,0]\n\n\n\nshape: (2,)\n\n\n\nName\n\n\nstr\n\n\n\n\n\"Braund, Mr. Owen Harris\"\n\n\n\"Allen, Mr. William Henry\"\n\n\n\n\n\n\nWe can also using slicing syntax to quickly refer to columns by name:\n\n\nCode\n# All rows in column 'Name'\ndf['Name']\n\n\n\nshape: (3,)\n\n\n\nName\n\n\nstr\n\n\n\n\n\"Braund, Mr. Owen Harris\"\n\n\n\"Allen, Mr. William Henry\"\n\n\n\"Bonnell, Miss. Elizabeth\"\n\n\n\n\n\n\nWhich is equivalent to:\n\n\nCode\n# Explicitly slice 'all' rows\n# 'Name' = just the Name column\ndf[:, 'Name']\n\n\n\nshape: (3,)\n\n\n\nName\n\n\nstr\n\n\n\n\n\"Braund, Mr. Owen Harris\"\n\n\n\"Allen, Mr. William Henry\"\n\n\n\"Bonnell, Miss. Elizabeth\"\n\n\n\n\n\n\n\n\nCode\n# 0:2 slice = rows up to, but not including 2\n# 'Name' = just the Name column\ndf[0:2, 'Name']\n\n\n\nshape: (2,)\n\n\n\nName\n\n\nstr\n\n\n\n\n\"Braund, Mr. Owen Harris\"\n\n\n\"Allen, Mr. William Henry\"\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhile you can access values this way, what makes Polars powerful is that it offers a much richer ‚Äúlanguage‚Äù or set of ‚Äúpatterns‚Äù for working with DataFrames, like dplyr‚Äôs verbs in R‚Äôs Tidyverse.\nWhile it doesn‚Äôt map on one-to-one, Polars offers a consistent and intuitive way of working with DataFrames that we‚Äôll teach you in this notebook. Once the fundamentals ‚Äúclick‚Äù for you, you‚Äôll be a data manipulating ninja!",
    "crumbs": [
      "Schedule",
      "Week 1",
      "Polars (dataframes) Crash Course"
    ]
  },
  {
    "objectID": "weeks/01/lab/polars-crash-course.html#thinking-in-polars-contexts-expressions",
    "href": "weeks/01/lab/polars-crash-course.html#thinking-in-polars-contexts-expressions",
    "title": "A Crash Course on Python DataFrames",
    "section": "Thinking in Polars: Contexts & Expressions",
    "text": "Thinking in Polars: Contexts & Expressions\nTo understand how to ‚Äúthink‚Äù in polars, we need to understand 2 fundamental concepts: contexts and expressions\nA context in Polars is how you choose what data you want to operate on. There are only a few you‚Äôll use regularly:\n\nContexts\n\ndf.select() - to subset columns\n\n\n\ndf.with_columns() - to add new columns\n\n\n\ndf.filter() - to subset rows\n\n\n\ndf.group_by().agg() - to summarize by group\n\n\n\n\nExpressions\nAn expression is any computation we do inside of a context.\nTo build an expression we first use a selector to choose one or more columns.\nThe most common selector you‚Äôll use is pl.col() to select one or more columns by name.\nThen we used method-chaining, . syntax, to perform operations on the selected columns.\nLet‚Äôs see a simple example.",
    "crumbs": [
      "Schedule",
      "Week 1",
      "Polars (dataframes) Crash Course"
    ]
  },
  {
    "objectID": "weeks/01/lab/polars-crash-course.html#starting-simple",
    "href": "weeks/01/lab/polars-crash-course.html#starting-simple",
    "title": "A Crash Course on Python DataFrames",
    "section": "Starting simple",
    "text": "Starting simple\nLet‚Äôs say we have data from some experiment that contains 3 participants, each of whom made 5 judgments about some stimuli.\nWe can use the pl.read_csv() function to load a file and get back a polars DataFrame:\n\n\nCode\ndf_1 = pl.read_csv('data/example_2.csv')\ndf_1\n\n\n\nshape: (15, 3)\n\n\n\nparticipant\naccuracy\nrt\n\n\ni64\ni64\nf64\n\n\n\n\n1\n44\n261.28435\n\n\n1\n47\n728.208489\n\n\n1\n64\n801.889016\n\n\n1\n67\n713.555026\n\n\n1\n67\n362.682105\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n3\n70\n439.524973\n\n\n3\n88\n55.11928\n\n\n3\n88\n644.801272\n\n\n3\n12\n571.800553\n\n\n3\n58\n715.224208\n\n\n\n\n\n\nWe can verify there are 15 rows and 3 columns:\n\n\nCode\ndf_1.shape\n\n\n(15, 3)",
    "crumbs": [
      "Schedule",
      "Week 1",
      "Polars (dataframes) Crash Course"
    ]
  },
  {
    "objectID": "weeks/01/lab/polars-crash-course.html#selecting-columns-.select",
    "href": "weeks/01/lab/polars-crash-course.html#selecting-columns-.select",
    "title": "A Crash Course on Python DataFrames",
    "section": "Selecting columns: .select()",
    "text": "Selecting columns: .select()\nThe .select context is what you‚Äôll use most often. It lets you apply expressions only to the specific columns you select.\n\nHere‚Äôs a simple example. Let‚Äôs say we want to calculate the average of the accuracy column.\nHow would we start?\nFirst, we need to think about our context. Since we only want the ‚Äúaccuracy‚Äù column we can use .select().\ndf.select(                     # &lt;- this is our context\n\n)\nSecond, we need to create an expression that means: ‚Äúuse the accuracy column, and calculate it‚Äôs mean‚Äù.\nWe can create an expression by combining the selector pl.col() with the operation .mean() using . syntax, i.e.¬†method-chaining.\ndf.select(                     # &lt;- this is our context\n  pl.col('accuracy').mean()   # &lt;- this is an expression, inside this context\n)\nLet‚Äôs try this now:\n\n\nCode\n# start of context\ndf_1.select(\n    pl.col('accuracy').mean() # &lt;- this is our expression\n)\n# end of context\n\n\n\nshape: (1, 1)\n\n\n\naccuracy\n\n\nf64\n\n\n\n\n56.066667\n\n\n\n\n\n\n\n\n\n\n\n\nNoteIndentation within polars context does not matter\n\n\n\nIn this examples throughout this notebook you‚Äôll see that we split up expressions over multiple lines within a polars context. You do not have to do this as indentation does not matter between the () We‚Äôre just trying to keep things a bit more readable for you. But the following code is exactly the same as the cell above:\ndf_1.select(pl.col('accuracy').mean())\n\n\n\n\n\n\n\n\nTipYour Turn\n\n\n\nHow would you build expression to calculate the ‚Äúmedian‚Äù Reaction Time?\n\n\n\n\nCode\ndf_1.select()  # put your expression here!\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\n\nCode\ndf_1.select(pl.col('rt').median())\n\n\n\nshape: (1, 1)\n\n\n\nrt\n\n\nf64\n\n\n\n\n502.974663\n\n\n\n\n\n\n\n\n\nLet‚Äôs make our lives a bit easier and type less by using what we know about import from the previous tutorials:\n\n\nCode\n# now we can use col() instead of pl.col()\nfrom polars import col\n\n\nNow we can use col in place of pl.col\n\nExpressing multiple things\nWe can add as many expressions inside a context as we want. We just need to separate them with a ,.\nEach the result from each expression will be saved to a separate column.\nWe‚Äôll use the col() selector again to perform two different operations: n_unique() and mean() on two different columns:\n\n\nCode\n# start context\ndf_1.select(\n    col('participant').n_unique(), col('accuracy').mean() # &lt;- multiple expressions separate by ,\n)\n# end context\n\n\n\nshape: (1, 2)\n\n\n\nparticipant\naccuracy\n\n\nu32\nf64\n\n\n\n\n3\n56.066667\n\n\n\n\n\n\n\n\n\n\n\n\nTipYour Turn\n\n\n\nHow would you express the following statement in polars?\nSelect only the participant and accuracy columns\nFor each participant, calculate the number of values, i.e.¬†.count()\nFor accuracy, calculate its standard deviation (what method do you think it is?)\n\n\n\n\nCode\n# Your code here\n\n# 1) Use the .select() context\n# 2) Use col() to create 2 expressions separated by a comma\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\n\nCode\ndf_1.select(\n    col('participant').count(),\n    col('accuracy').std()\n)\n\n\n\nshape: (1, 2)\n\n\n\nparticipant\naccuracy\n\n\nu32\nf64\n\n\n\n\n15\n27.043528\n\n\n\n\n\n\n\n\n\n\n\nRepeating the same expression across many columns\nSometimes we might find ourselves repeating the same expression for different columns.\nOne way we can do that is simply by creating multiple expressions like by before, by using col() to select each column separately:\n\n\nCode\ndf_1.select(\n    col('accuracy').median(), col('rt').median()\n    )\n\n\n\nshape: (1, 2)\n\n\n\naccuracy\nrt\n\n\nf64\nf64\n\n\n\n\n64.0\n502.974663\n\n\n\n\n\n\nBut Polars makes this much easier for us - we can condense this down to a single expression by giving our selector - col() - additional column names:\n\n\nCode\ndf_1.select(col('accuracy', 'rt').median())  # &lt;- one expression repeated for both columns\n\n\n\nshape: (1, 2)\n\n\n\naccuracy\nrt\n\n\nf64\nf64\n\n\n\n\n64.0\n502.974663\n\n\n\n\n\n\nThese both do the same thing so if you find it helpful to start explicit, building up each expression one at a time, feel free to do that!\nLater on you might find it helpful to use a single condensed expression, when you find yourself getting annoyed by repeating yourself.\n\n\nRenaming expression outputs\nLet‚Äôs try creating two expressions that operate on the same column. In natural language:\n‚ÄúSelect only the accuracy column. For accuracy, calculate its median For accuracy, calculate its variance‚Äù\nLet‚Äôs try it:\n\n\nCode\ndf_1.select(col('accuracy').mean(), col('accuracy').std())\n\n\n\n---------------------------------------------------------------------------\nDuplicateError                            Traceback (most recent call last)\nCell In[30], line 1\n----&gt; 1 df_1.select(col('accuracy').mean(), col('accuracy').std())\n\nFile ~/Dropbox/docs/teaching/201b/w26/.venv/lib/python3.11/site-packages/polars/dataframe/frame.py:10150, in DataFrame.select(self, *exprs, **named_exprs)\n  10066 \"\"\"\n  10067 Select columns from this DataFrame.\n  10068 \n   (...)  10143 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n  10144 \"\"\"\n  10145 from polars.lazyframe.opt_flags import QueryOptFlags\n  10147 return (\n  10148     self.lazy()\n  10149     .select(*exprs, **named_exprs)\n&gt; 10150     .collect(optimizations=QueryOptFlags._eager())\n  10151 )\n\nFile ~/Dropbox/docs/teaching/201b/w26/.venv/lib/python3.11/site-packages/polars/_utils/deprecation.py:97, in deprecate_streaming_parameter.&lt;locals&gt;.decorate.&lt;locals&gt;.wrapper(*args, **kwargs)\n     93         kwargs[\"engine\"] = \"in-memory\"\n     95     del kwargs[\"streaming\"]\n---&gt; 97 return function(*args, **kwargs)\n\nFile ~/Dropbox/docs/teaching/201b/w26/.venv/lib/python3.11/site-packages/polars/lazyframe/opt_flags.py:328, in forward_old_opt_flags.&lt;locals&gt;.decorate.&lt;locals&gt;.wrapper(*args, **kwargs)\n    325         optflags = cb(optflags, kwargs.pop(key))  # type: ignore[no-untyped-call,unused-ignore]\n    327 kwargs[\"optimizations\"] = optflags\n--&gt; 328 return function(*args, **kwargs)\n\nFile ~/Dropbox/docs/teaching/201b/w26/.venv/lib/python3.11/site-packages/polars/lazyframe/frame.py:2429, in LazyFrame.collect(self, type_coercion, predicate_pushdown, projection_pushdown, simplify_expression, slice_pushdown, comm_subplan_elim, comm_subexpr_elim, cluster_with_columns, collapse_joins, no_optimization, engine, background, optimizations, **_kwargs)\n   2427 # Only for testing purposes\n   2428 callback = _kwargs.get(\"post_opt_callback\", callback)\n-&gt; 2429 return wrap_df(ldf.collect(engine, callback))\n\nDuplicateError: projections contained duplicate output name 'accuracy'. It's possible that multiple expressions are returning the same default column name. If this is the case, try renaming the columns with `.alias(\"new_name\")` to avoid duplicate column names.\n\n\n\n\n\n\n\n\n\nImportantpolars DuplicateError\n\n\n\nPolars automatically enforces the requirement that all column names are must be unique.\nBy default the results of an expression are saved using the same column name that you selected.\nIn this case we selected ‚Äúaccuracy‚Äù using col('accuracy') twice - once to calculate the mean and once to calculate the standard deviation. So Polars is trying to save both results into a column called accuracy causing a conflict!\nTo fix this, we can extend our expression with additional operations using method-chaining with the . syntax.\nThe operation we‚Äôre looking for is .alias() which you‚Äôll often put at the end of an expression in order to give it a new name\n\n\n\n\nCode\ndf_1.select(\n    col('accuracy').mean().alias('acc_mean'),\n    col('accuracy').std().alias('acc_std')\n    )\n\n\n\nshape: (1, 2)\n\n\n\nacc_mean\nacc_std\n\n\nf64\nf64\n\n\n\n\n56.066667\n27.043528\n\n\n\n\n\n\n\n\n\n\n\n\nNoteTwo styles of expressing yourself\n\n\n\nYou might find this style of ‚Äúmethod-chaining‚Äù the use of .alias() unintuitive at first. So Polars also lets your rename your expressions in a different ‚Äústyle‚Äù using = as in other language like R.\nIn English, we could rephrase our expressions as so:\nSelect the accuracy column Create a new column named ‚Äòacc_mean‚Äô, which is the mean of accuracy Create a new column named ‚Äòacc_std‚Äô, which is the standard-deviation of accuracy\nAnd in code like this:\ndf_1.select(\n    acc_mean = col('accuracy').mean(),\n    acc_std = col('accuracy').std()\n)\nYou can use which ever style of ‚Äúphrasing‚Äù an expression that feels more natural to you based on what you‚Äôre doing!\n\n\n\n\n\n\n\n\nTipYour turn\n\n\n\nRun the following code. Why are the values in the accuracy column being overwritten? Can you fix it?\n\n\n\n\nCode\ndf_1.select(col('participant'), col('accuracy').mean())\n\n\n\nshape: (15, 2)\n\n\n\nparticipant\naccuracy\n\n\ni64\nf64\n\n\n\n\n1\n56.066667\n\n\n1\n56.066667\n\n\n1\n56.066667\n\n\n1\n56.066667\n\n\n1\n56.066667\n\n\n‚Ä¶\n‚Ä¶\n\n\n3\n56.066667\n\n\n3\n56.066667\n\n\n3\n56.066667\n\n\n3\n56.066667\n\n\n3\n56.066667\n\n\n\n\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nThe mean of accuracy is being saved to a column named ‚Äúaccuracy‚Äù which overwrites the participant values being selected. Fix by renaming:\n\n\nCode\ndf_1.select(col('participant'), col('accuracy').mean().alias('acc_mean'))\n# Or equivalently:\n# df_1.select(col('participant'), acc_mean=col('accuracy').mean())\n\n\n\nshape: (15, 2)\n\n\n\nparticipant\nacc_mean\n\n\ni64\nf64\n\n\n\n\n1\n56.066667\n\n\n1\n56.066667\n\n\n1\n56.066667\n\n\n1\n56.066667\n\n\n1\n56.066667\n\n\n‚Ä¶\n‚Ä¶\n\n\n3\n56.066667\n\n\n3\n56.066667\n\n\n3\n56.066667\n\n\n3\n56.066667\n\n\n3\n56.066667",
    "crumbs": [
      "Schedule",
      "Week 1",
      "Polars (dataframes) Crash Course"
    ]
  },
  {
    "objectID": "weeks/01/lab/polars-crash-course.html#aggregating-columns-.group_by",
    "href": "weeks/01/lab/polars-crash-course.html#aggregating-columns-.group_by",
    "title": "A Crash Course on Python DataFrames",
    "section": "Aggregating columns: .group_by()",
    "text": "Aggregating columns: .group_by()\nThe .group_by('some_col') context is used for summarizing columns separately by 'some_col'.\n\nYou always follow up a .group_by() with .agg(), and place our expressions inside to tell Polars what should be calculated per group.\nUsing .group_by() will always give you a smaller DataFrame than the original. Specifically you will get back a DataFrame whose rows = number of groups\n\n\nCode\n# start of .agg context\ndf_1.group_by('participant').agg(\n    col('rt').mean(), col('accuracy').mean() # &lt;- expressions like before\n)\n\n\n\nshape: (3, 3)\n\n\n\nparticipant\nrt\naccuracy\n\n\ni64\nf64\nf64\n\n\n\n\n1\n573.523797\n57.8\n\n\n2\n496.969382\n47.2\n\n\n3\n485.294057\n63.2\n\n\n\n\n\n\n\nMaintaining group order\nUnfortunately, by default Polars doesn‚Äôt preserve the order of groups as they exist in the original DataFrame. But we can easily fix this by giving .group_by() and additional argument maintain_order=True:\n\n\nCode\ndf_1.group_by('participant', maintain_order=True).agg(\n    col('rt').mean(), col('accuracy').mean()\n    )\n\n\n\nshape: (3, 3)\n\n\n\nparticipant\nrt\naccuracy\n\n\ni64\nf64\nf64\n\n\n\n\n1\n573.523797\n57.8\n\n\n2\n496.969382\n47.2\n\n\n3\n485.294057\n63.2\n\n\n\n\n\n\n\n\n\n\n\n\nTipYour Turn\n\n\n\nCalculate each participant‚Äôs average reaction time divided by their average accuracy. Remember since there are just 3 unique participants, i.e.¬†3 ‚Äúgroups‚Äù, our result should have 3 rows; one for each participant.\nHint: you can divide 2 columns using the method-chaining style with .truediv() or simply using /\n\n\n\n\nCode\n# Your code here\n\n# Hint: use group_by on 'participant' and then create an expression\n# that divides the average 'rt' by average 'accuracy' and name it 'rt_acc_avg'\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\n\nCode\ndf_1.group_by('participant', maintain_order=True).agg(\n    rt_acc_avg = col('rt').mean() / col('accuracy').mean()\n)\n\n\n\nshape: (3, 2)\n\n\n\nparticipant\nrt_acc_avg\n\n\ni64\nf64\n\n\n\n\n1\n9.922557\n\n\n2\n10.529012\n\n\n3\n7.678703",
    "crumbs": [
      "Schedule",
      "Week 1",
      "Polars (dataframes) Crash Course"
    ]
  },
  {
    "objectID": "weeks/01/lab/polars-crash-course.html#creating-columns-.with_columns",
    "href": "weeks/01/lab/polars-crash-course.html#creating-columns-.with_columns",
    "title": "A Crash Course on Python DataFrames",
    "section": "Creating columns: .with_columns()",
    "text": "Creating columns: .with_columns()\nWhenever we want to return the original DataFrame along with some new columns we can use the .with_columns context instead of .select.\n\nIt will always output the original DataFrame and the outputs of your expressions.\nIf your expression returns just a single value (e.g.¬†the mean of a column), Polars is smart enough to automatically repeat that value over all rows to make sure it fits inside the DataFrame.\n\n\nCode\n# start with_columns context\ndf_1.with_columns(\n    acc_mean=col('accuracy').mean() # &lt;- expression like before\n)\n\n\n\nshape: (15, 4)\n\n\n\nparticipant\naccuracy\nrt\nacc_mean\n\n\ni64\ni64\nf64\nf64\n\n\n\n\n1\n44\n261.28435\n56.066667\n\n\n1\n47\n728.208489\n56.066667\n\n\n1\n64\n801.889016\n56.066667\n\n\n1\n67\n713.555026\n56.066667\n\n\n1\n67\n362.682105\n56.066667\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n3\n70\n439.524973\n56.066667\n\n\n3\n88\n55.11928\n56.066667\n\n\n3\n88\n644.801272\n56.066667\n\n\n3\n12\n571.800553\n56.066667\n\n\n3\n58\n715.224208\n56.066667\n\n\n\n\n\n\nContrast this with the .select context which will only return the mean of accuracy:\n\n\nCode\ndf_1.select(\n    acc_mean=col('accuracy').mean()\n)\n\n\n\nshape: (1, 1)\n\n\n\nacc_mean\n\n\nf64\n\n\n\n\n56.066667\n\n\n\n\n\n\nAs before we can create multiple new columns by including multiple expressions:\n\n\nCode\ndf_1.with_columns(\n    acc_mean=col('accuracy').mean(),\n    rt_scaled=col('rt') / 100\n    )\n\n\n\nshape: (15, 5)\n\n\n\nparticipant\naccuracy\nrt\nacc_mean\nrt_scaled\n\n\ni64\ni64\nf64\nf64\nf64\n\n\n\n\n1\n44\n261.28435\n56.066667\n2.612843\n\n\n1\n47\n728.208489\n56.066667\n7.282085\n\n\n1\n64\n801.889016\n56.066667\n8.01889\n\n\n1\n67\n713.555026\n56.066667\n7.13555\n\n\n1\n67\n362.682105\n56.066667\n3.626821\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n3\n70\n439.524973\n56.066667\n4.39525\n\n\n3\n88\n55.11928\n56.066667\n0.551193\n\n\n3\n88\n644.801272\n56.066667\n6.448013\n\n\n3\n12\n571.800553\n56.066667\n5.718006\n\n\n3\n58\n715.224208\n56.066667\n7.152242\n\n\n\n\n\n\n\n\n\n\n\n\nTipUsing .over() to perform Tidy group-by operations\n\n\n\nA very handy use for .with_columns is to combine it with the .over() operation.\nThis allows us to calculate an expression separately by group, but then save the results into a DataFrame the same size as the original.\nFor example, Polars will keep the tidy-format of the data and correctly repeat the values across rows.\n\n\n\n\nCode\ndf_1.with_columns(\n    acc_mean=col('accuracy').mean().over('participant') # &lt;- chaining .over() handles grouping!\n)\n\n\n\nshape: (15, 4)\n\n\n\nparticipant\naccuracy\nrt\nacc_mean\n\n\ni64\ni64\nf64\nf64\n\n\n\n\n1\n44\n261.28435\n57.8\n\n\n1\n47\n728.208489\n57.8\n\n\n1\n64\n801.889016\n57.8\n\n\n1\n67\n713.555026\n57.8\n\n\n1\n67\n362.682105\n57.8\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n3\n70\n439.524973\n63.2\n\n\n3\n88\n55.11928\n63.2\n\n\n3\n88\n644.801272\n63.2\n\n\n3\n12\n571.800553\n63.2\n\n\n3\n58\n715.224208\n63.2\n\n\n\n\n\n\nChaining .over('some_col') to any expression is like using .group_by but preserving the shape of the original DataFrame:\n\n\nCode\ndf_1.with_columns(\n    acc_mean=col('accuracy').mean().over('participant'),\n    rt_mean=col('rt').mean().over('participant')\n)\n\n\n\nshape: (15, 5)\n\n\n\nparticipant\naccuracy\nrt\nacc_mean\nrt_mean\n\n\ni64\ni64\nf64\nf64\nf64\n\n\n\n\n1\n44\n261.28435\n57.8\n573.523797\n\n\n1\n47\n728.208489\n57.8\n573.523797\n\n\n1\n64\n801.889016\n57.8\n573.523797\n\n\n1\n67\n713.555026\n57.8\n573.523797\n\n\n1\n67\n362.682105\n57.8\n573.523797\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n3\n70\n439.524973\n63.2\n485.294057\n\n\n3\n88\n55.11928\n63.2\n485.294057\n\n\n3\n88\n644.801272\n63.2\n485.294057\n\n\n3\n12\n571.800553\n63.2\n485.294057\n\n\n3\n58\n715.224208\n63.2\n485.294057\n\n\n\n\n\n\nRemember that the .group_by() context will always return a smaller aggregated DataFrame:\n\n\nCode\ndf_1.group_by('participant', maintain_order=True).agg(\n    acc_mean=col('accuracy').mean(),\n    rt_mean=col('rt').mean()\n)\n\n\n\nshape: (3, 3)\n\n\n\nparticipant\nacc_mean\nrt_mean\n\n\ni64\nf64\nf64\n\n\n\n\n1\n57.8\n573.523797\n\n\n2\n47.2\n496.969382\n\n\n3\n63.2\n485.294057\n\n\n\n\n\n\nIn Polars you should only rely on .group_by if you know for sure that you want your output to be smaller than your original DataFrame - and by smaller we mean rows = number of groups.\n\n\n\n\n\n\nTipYour Turn\n\n\n\nCreate a DataFrame that adds 3 new columns:\n\nAccuracy on a 0-1 scale\nRT / Accuracy\nRT / max RT, separately using each participant‚Äôs max RT\n\n\n\n\n\nCode\n# Your code here\n\n\n# Hint: you can wrap an entire expression in () and use .over()\n# on the entire wrapped expression to do things like\n# add, subtract columns multiple columns by \"participant\"\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\n\nCode\ndf_1.with_columns(\n    acc_scaled = col('accuracy') / 100,\n    rt_acc = col('rt') / col('accuracy'),\n    rt_max_scaled = (col('rt') / col('rt').max()).over('participant')\n)\n\n\n\nshape: (15, 6)\n\n\n\nparticipant\naccuracy\nrt\nacc_scaled\nrt_acc\nrt_max_scaled\n\n\ni64\ni64\nf64\nf64\nf64\nf64\n\n\n\n\n1\n44\n261.28435\n0.44\n5.938281\n0.325836\n\n\n1\n47\n728.208489\n0.47\n15.493798\n0.908116\n\n\n1\n64\n801.889016\n0.64\n12.529516\n1.0\n\n\n1\n67\n713.555026\n0.67\n10.650075\n0.889843\n\n\n1\n67\n362.682105\n0.67\n5.413166\n0.452285\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n3\n70\n439.524973\n0.7\n6.278928\n0.614528\n\n\n3\n88\n55.11928\n0.88\n0.626355\n0.077066\n\n\n3\n88\n644.801272\n0.88\n7.327287\n0.901537\n\n\n3\n12\n571.800553\n0.12\n47.650046\n0.79947\n\n\n3\n58\n715.224208\n0.58\n12.331452\n1.0",
    "crumbs": [
      "Schedule",
      "Week 1",
      "Polars (dataframes) Crash Course"
    ]
  },
  {
    "objectID": "weeks/01/lab/polars-crash-course.html#selecting-rows-.filter",
    "href": "weeks/01/lab/polars-crash-course.html#selecting-rows-.filter",
    "title": "A Crash Course on Python DataFrames",
    "section": "Selecting rows: .filter()",
    "text": "Selecting rows: .filter()\nThe .filter context is used for sub-setting rows using a logical expression.\n\nInstead of returning one or more values like other expressions, a logical expression returns True/False values that we can use to filter rows that mean those criteria:\n\n\nCode\n# start filter context\ndf_1.filter(\n    col('participant') == 1 # &lt;- expression like before\n)\n\n\n\nshape: (5, 3)\n\n\n\nparticipant\naccuracy\nrt\n\n\ni64\ni64\nf64\n\n\n\n\n1\n44\n261.28435\n\n\n1\n47\n728.208489\n\n\n1\n64\n801.889016\n\n\n1\n67\n713.555026\n\n\n1\n67\n362.682105\n\n\n\n\n\n\nOr in Polars methods-style using .eq():\n\n\nCode\ndf_1.filter(\n    col('participant').eq(1)\n)\n\n\n\nshape: (5, 3)\n\n\n\nparticipant\naccuracy\nrt\n\n\ni64\ni64\nf64\n\n\n\n\n1\n44\n261.28435\n\n\n1\n47\n728.208489\n\n\n1\n64\n801.889016\n\n\n1\n67\n713.555026\n\n\n1\n67\n362.682105\n\n\n\n\n\n\nOr even the opposite: we can negate or invert any logical expression by putting a ~ in front of it.\nThis is like using not in regular Python or ! in some other languages.\n\n\nCode\ndf_1.filter(\n    ~col('participant').eq(1)\n)\n\n\n\nshape: (10, 3)\n\n\n\nparticipant\naccuracy\nrt\n\n\ni64\ni64\nf64\n\n\n\n\n2\n9\n502.974663\n\n\n2\n83\n424.866821\n\n\n2\n21\n492.355273\n\n\n2\n36\n573.594895\n\n\n2\n87\n491.05526\n\n\n3\n70\n439.524973\n\n\n3\n88\n55.11928\n\n\n3\n88\n644.801272\n\n\n3\n12\n571.800553\n\n\n3\n58\n715.224208\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nBut be careful. If you‚Äôre not using the method-chaining style then you need to wrap you expression in () before using ~:\n\n\n\n\nCode\ndf_1.filter(\n    ~(col('participant') == 1)   # &lt;- notice extra () around expression\n)\n\n\n\nshape: (10, 3)\n\n\n\nparticipant\naccuracy\nrt\n\n\ni64\ni64\nf64\n\n\n\n\n2\n9\n502.974663\n\n\n2\n83\n424.866821\n\n\n2\n21\n492.355273\n\n\n2\n36\n573.594895\n\n\n2\n87\n491.05526\n\n\n3\n70\n439.524973\n\n\n3\n88\n55.11928\n\n\n3\n88\n644.801272\n\n\n3\n12\n571.800553\n\n\n3\n58\n715.224208\n\n\n\n\n\n\nJust like in with other contexts (i.e.¬†.select, .with_columns, .group_by) we can using multiple logical expressions to refine our filtering criteria.\nIf we use , Polars treats them logically as an and statement. For example, we use 2 logical expressions to filter where: participant is 1 AND accuracy is 67:\n\n\nCode\ndf_1.filter(\n    col('participant').eq(1),\n    col('accuracy').eq(67)\n)\n\n\n\nshape: (2, 3)\n\n\n\nparticipant\naccuracy\nrt\n\n\ni64\ni64\nf64\n\n\n\n\n1\n67\n713.555026\n\n\n1\n67\n362.682105\n\n\n\n\n\n\nBut you might find it clearer to use & for and expressions:\n\n\nCode\ndf_1.filter(\n    col('participant').eq(1) & col('accuracy').eq(67)\n)\n\n\n\nshape: (2, 3)\n\n\n\nparticipant\naccuracy\nrt\n\n\ni64\ni64\nf64\n\n\n\n\n1\n67\n713.555026\n\n\n1\n67\n362.682105\n\n\n\n\n\n\nThe | operator can be used for or expressions:\n\n\nCode\ndf_1.filter(\n    col('participant').eq(1) | col('participant').eq(3)\n)\n\n\n\nshape: (10, 3)\n\n\n\nparticipant\naccuracy\nrt\n\n\ni64\ni64\nf64\n\n\n\n\n1\n44\n261.28435\n\n\n1\n47\n728.208489\n\n\n1\n64\n801.889016\n\n\n1\n67\n713.555026\n\n\n1\n67\n362.682105\n\n\n3\n70\n439.524973\n\n\n3\n88\n55.11928\n\n\n3\n88\n644.801272\n\n\n3\n12\n571.800553\n\n\n3\n58\n715.224208\n\n\n\n\n\n\nTo combine more complicated logical expressions, you can wrap them in ().\nBelow we get rows where participant 1‚Äôs accuracy is 67 OR any of participant 2‚Äôs rows:\n\n\nCode\ndf_1.filter(\n    col('participant').eq(1) & col('accuracy').eq(67) | col('participant').eq(2)\n)\n\n\n\nshape: (7, 3)\n\n\n\nparticipant\naccuracy\nrt\n\n\ni64\ni64\nf64\n\n\n\n\n1\n67\n713.555026\n\n\n1\n67\n362.682105\n\n\n2\n9\n502.974663\n\n\n2\n83\n424.866821\n\n\n2\n21\n492.355273\n\n\n2\n36\n573.594895\n\n\n2\n87\n491.05526\n\n\n\n\n\n\n\n\n\n\n\n\nNoteTwo styles of logical expressions\n\n\n\nLike renaming the outputs of an expression, Polars gives us 2 styles we can use to combine logical expressions.\nWe‚Äôve seen the first one using & and |.\nThe second one uses the method-chaining style with the . syntax. Here Polars provides a .and_() and a .or_() method.\ndf_1.filter(\n    col('participant').eq(1).and_(\n        col('accuracy').eq(67)).or_(\n            col('participant').eq(2)\n        )\n)\nFeel free to use which every style you find more intuitive and readable:",
    "crumbs": [
      "Schedule",
      "Week 1",
      "Polars (dataframes) Crash Course"
    ]
  },
  {
    "objectID": "weeks/01/lab/polars-crash-course.html#missing-data-null-vs-nan",
    "href": "weeks/01/lab/polars-crash-course.html#missing-data-null-vs-nan",
    "title": "A Crash Course on Python DataFrames",
    "section": "Missing Data: null vs NaN",
    "text": "Missing Data: null vs NaN\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ ‚îÇ null ‚îÇ NaN ‚îÇ ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§ ‚îÇ Meaning ‚îÇ Missing/unknown value ‚îÇ ‚ÄúNot a Number‚Äù (undefined math result) ‚îÇ ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§ ‚îÇ Dtype ‚îÇ Any column type ‚îÇ Float columns only ‚îÇ ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§ ‚îÇ Origin ‚îÇ Empty cells, missing data ‚îÇ Math like 0/0, ‚àû - ‚àû, sqrt(-1) ‚îÇ ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§ ‚îÇ Check ‚îÇ .is_null() ‚îÇ .is_nan() ‚îÇ ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§ ‚îÇ Fill ‚îÇ .fill_null(val) ‚îÇ .fill_nan(val) ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n  import polars as pl                                                              \n                                                                                   \n  df = pl.DataFrame({                                                              \n      \"a\": [1.0, None, 3.0],      # None ‚Üí null                                    \n      \"b\": [1.0, float('nan'), 3.0]  # NaN from Python                             \n  })                                                                               \n                                                                                   \n  df.select(                                                                       \n      pl.col(\"a\").fill_null(0),   # fills the null                                 \n      pl.col(\"b\").fill_nan(0),    # fills the NaN                                  \n  )                                                                                \nCommon gotcha: CSV empty cells become null, but division by zero creates NaN.\nThey require different handling:\n                                                                                   \ndf.with_columns(                                                                 \n  (pl.col(\"x\") / pl.col(\"y\"))  # might produce NaN if y=0                      \n  .fill_nan(None)              # convert NaN ‚Üí null for consistency            \n  .fill_null(0)                # then fill all missing                         \n  .alias(\"result\")                                                             \n)",
    "crumbs": [
      "Schedule",
      "Week 1",
      "Polars (dataframes) Crash Course"
    ]
  },
  {
    "objectID": "weeks/01/lab/polars-crash-course.html#expressions-are-for-performing-operations",
    "href": "weeks/01/lab/polars-crash-course.html#expressions-are-for-performing-operations",
    "title": "A Crash Course on Python DataFrames",
    "section": "Expressions are for performing operations",
    "text": "Expressions are for performing operations\nSo far we‚Äôve see how to build up an expression that computes some value, e.g.¬†.mean() or performs some logic, e.g.¬†.eq().\nPolars calls these computations operations and include tons of them (accessible via . syntax). Some of the notable ones include:\nArithmetic, e.g.¬†.add, .sub, .mul\nBoolean, e.g.¬†.all, .any, .is_null, .is_not_null\nSummary (aggregate) stats, e.g.¬†.mean, .median, .std, .count\nComparison, e.g.¬†.gt, .lt, .gte, .lte, .eq, .ne.\n\n\n\n\n\n\nTipYour Turn\n\n\n\nUse the linked documentation and contexts you learned about above to complete the following exercises:\n\n\n1. Select the accuracy and RT columns from df_1 and multiply them by 10\n\n\nCode\n# Your code here\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\n\nCode\ndf_1.select(col('accuracy', 'rt') * 10)\n\n\n\nshape: (15, 2)\n\n\n\naccuracy\nrt\n\n\ni64\nf64\n\n\n\n\n440\n2612.843496\n\n\n470\n7282.084892\n\n\n640\n8018.890165\n\n\n670\n7135.550259\n\n\n670\n3626.821046\n\n\n‚Ä¶\n‚Ä¶\n\n\n700\n4395.249735\n\n\n880\n551.192796\n\n\n880\n6448.012718\n\n\n120\n5718.005529\n\n\n580\n7152.242075\n\n\n\n\n\n\n\n\n\n2. Add 2 new columns to the DataFrame: rt_acc and acc_max_scaled\nFor rt_acc divide reaction time by accuracy.\nFor acc_max_scaled divide accuracy by maximum accuracy, separately by participant.\n\n\nCode\n# Your code here\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\n\nCode\ndf_1.with_columns(\n    rt_acc = col('rt') / col('accuracy'),\n    acc_max_scaled = (col('accuracy') / col('accuracy').max()).over('participant')\n)\n\n\n\nshape: (15, 5)\n\n\n\nparticipant\naccuracy\nrt\nrt_acc\nacc_max_scaled\n\n\ni64\ni64\nf64\nf64\nf64\n\n\n\n\n1\n44\n261.28435\n5.938281\n0.656716\n\n\n1\n47\n728.208489\n15.493798\n0.701493\n\n\n1\n64\n801.889016\n12.529516\n0.955224\n\n\n1\n67\n713.555026\n10.650075\n1.0\n\n\n1\n67\n362.682105\n5.413166\n1.0\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n3\n70\n439.524973\n6.278928\n0.795455\n\n\n3\n88\n55.11928\n0.626355\n1.0\n\n\n3\n88\n644.801272\n7.327287\n1.0\n\n\n3\n12\n571.800553\n47.650046\n0.136364\n\n\n3\n58\n715.224208\n12.331452\n0.659091\n\n\n\n\n\n\n\n\n\n3. Filter rows where reaction time is &gt; 100ms and &lt; 725ms\n\n\nCode\n# Your code here\n\n# Hint: You should write a logical expression\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\n\nCode\ndf_1.filter(\n    (col('rt') &gt; 100) & (col('rt') &lt; 725)\n)\n\n\n\nshape: (12, 3)\n\n\n\nparticipant\naccuracy\nrt\n\n\ni64\ni64\nf64\n\n\n\n\n1\n44\n261.28435\n\n\n1\n67\n713.555026\n\n\n1\n67\n362.682105\n\n\n2\n9\n502.974663\n\n\n2\n83\n424.866821\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n2\n87\n491.05526\n\n\n3\n70\n439.524973\n\n\n3\n88\n644.801272\n\n\n3\n12\n571.800553\n\n\n3\n58\n715.224208",
    "crumbs": [
      "Schedule",
      "Week 1",
      "Polars (dataframes) Crash Course"
    ]
  },
  {
    "objectID": "weeks/01/lab/polars-crash-course.html#saving-re-usable-expressions",
    "href": "weeks/01/lab/polars-crash-course.html#saving-re-usable-expressions",
    "title": "A Crash Course on Python DataFrames",
    "section": "Saving re-usable expressions",
    "text": "Saving re-usable expressions\nWhen you find yourself creating complex expressions that you want to re-use later on, perhaps across other DataFrames, you can save them as re-usable functions!\nFor example, Polars doesn‚Äôt include an operation to calculate a z-score by default. But we know how to do that manually. So let‚Äôs create a function called scale that defines an expression we can reuse.\n\n\nCode\ndef scale(column_name):\n    \"\"\"Reminder:\n        z-score = (x - x.mean() / x.std())\n    \"\"\"\n    return (col(column_name) - col(column_name).mean()) / col(column_name).std()\n\n\nThis is a function that accepts a single argument column_name, and then uses the col selector from Polars to select a column and calculate its z-score.\nWe can now use this expression in any context saves us a ton of typing and typos!\nFor example just across all accuracy scores:\n\n\nCode\ndf_1.select(\n    acc_z=scale('accuracy')\n)\n\n\n\nshape: (15, 1)\n\n\n\nacc_z\n\n\nf64\n\n\n\n\n-0.446194\n\n\n-0.335262\n\n\n0.293354\n\n\n0.404287\n\n\n0.404287\n\n\n‚Ä¶\n\n\n0.515219\n\n\n1.180812\n\n\n1.180812\n\n\n-1.629472\n\n\n0.07149\n\n\n\n\n\n\nOr as a more realistic example: z-score separately by participant\nThis is a great example of where .with_columns + .over() can come in super-handy.\nBecause our function returns an expression we can call operations on it just like any other expression:\n\n\nCode\n# .over() works with our scale() function\n# because it return a Polars expression!\ndf_1.with_columns(\n    acc_z=scale('accuracy').over('participant'),\n    rt_z=scale('rt').over('participant')\n)\n\n\n\nshape: (15, 5)\n\n\n\nparticipant\naccuracy\nrt\nacc_z\nrt_z\n\n\ni64\ni64\nf64\nf64\nf64\n\n\n\n\n1\n44\n261.28435\n-1.216438\n-1.281041\n\n\n1\n47\n728.208489\n-0.951995\n0.634633\n\n\n1\n64\n801.889016\n0.546515\n0.936926\n\n\n1\n67\n713.555026\n0.810958\n0.574513\n\n\n1\n67\n362.682105\n0.810958\n-0.865031\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n3\n70\n439.524973\n0.217085\n-0.175214\n\n\n3\n88\n55.11928\n0.791722\n-1.646805\n\n\n3\n88\n644.801272\n0.791722\n0.610629\n\n\n3\n12\n571.800553\n-1.634524\n0.331166\n\n\n3\n58\n715.224208\n-0.166006\n0.880224\n\n\n\n\n\n\nThis is entirely equivalent to the following code, but so much easier to read and so much less potential for errors when typing:\n\n\nCode\ndf_1.with_columns(\n    acc_z=((col('accuracy') - col('accuracy').mean()) / col('accuracy').std()).over('participant'), rt_z=((col('rt') - col('rt').mean()) / col('rt').std()).over('participant')\n)\n\n\n\nshape: (15, 5)\n\n\n\nparticipant\naccuracy\nrt\nacc_z\nrt_z\n\n\ni64\ni64\nf64\nf64\nf64\n\n\n\n\n1\n44\n261.28435\n-1.216438\n-1.281041\n\n\n1\n47\n728.208489\n-0.951995\n0.634633\n\n\n1\n64\n801.889016\n0.546515\n0.936926\n\n\n1\n67\n713.555026\n0.810958\n0.574513\n\n\n1\n67\n362.682105\n0.810958\n-0.865031\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n3\n70\n439.524973\n0.217085\n-0.175214\n\n\n3\n88\n55.11928\n0.791722\n-1.646805\n\n\n3\n88\n644.801272\n0.791722\n0.610629\n\n\n3\n12\n571.800553\n-1.634524\n0.331166\n\n\n3\n58\n715.224208\n-0.166006\n0.880224\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nAs you‚Äôre thinking about how to manipulate data, think about saving an expression you find yourself using a lot as function! In fact Python as a short-hand alternative to def for creating simple one-line functions: lambda\nmyfunc = lambda param1: print(param1)\nWe can rewrite the function above as a lambda expression like this:\nscale = lambda column_name: (col(column_name) - col(column_name).mean()) / col(column_name).std()\nYou‚Äôll often see this in Python code when people are defining and using functions within some other code.\n\n\n\n\n\n\n\n\nTipYour Turn\n\n\n\nCreate a Polars expression that mean-centers a column. You can use def or lambda whatever feels more comfortable right now\n\n\n\n\nCode\n# Your code here\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\n\nCode\ndef mean_center(column_name):\n    return col(column_name) - col(column_name).mean()\n\n# Or with lambda:\n# mean_center = lambda column_name: col(column_name) - col(column_name).mean()\n\n\n\n\n\nAdd 2 new columns to the df_1 DataFrame that include mean-centered accuracy, and mean-centered RT\n\n\nCode\n# Your code here\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\n\nCode\ndf_1.with_columns(\n    acc_centered = mean_center('accuracy'),\n    rt_centered = mean_center('rt')\n)\n\n\n\nshape: (15, 5)\n\n\n\nparticipant\naccuracy\nrt\nacc_centered\nrt_centered\n\n\ni64\ni64\nf64\nf64\nf64\n\n\n\n\n1\n44\n261.28435\n-12.066667\n-257.311396\n\n\n1\n47\n728.208489\n-9.066667\n209.612744\n\n\n1\n64\n801.889016\n7.933333\n283.293271\n\n\n1\n67\n713.555026\n10.933333\n194.95928\n\n\n1\n67\n362.682105\n10.933333\n-155.913641\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n3\n70\n439.524973\n13.933333\n-79.070772\n\n\n3\n88\n55.11928\n31.933333\n-463.476466\n\n\n3\n88\n644.801272\n31.933333\n126.205526\n\n\n3\n12\n571.800553\n-44.066667\n53.204807\n\n\n3\n58\n715.224208\n1.933333\n196.628462",
    "crumbs": [
      "Schedule",
      "Week 1",
      "Polars (dataframes) Crash Course"
    ]
  },
  {
    "objectID": "weeks/01/lab/polars-crash-course.html#more-complex-expression-with-functions-when-and-lit",
    "href": "weeks/01/lab/polars-crash-course.html#more-complex-expression-with-functions-when-and-lit",
    "title": "A Crash Course on Python DataFrames",
    "section": "More complex expression with functions: when and lit",
    "text": "More complex expression with functions: when and lit\nPolars also offers a few other operations as functions you can use inside of a context for building expressions.\nThese are called as pl.something() but we can also directly import them.\nYou should check out the documentation to see what‚Äôs possible, but two common ones you‚Äôre likely to use are pl.when and pl.lit\n\n\nCode\n# Directly import them to make life easier\nfrom polars import when, lit\n\n\nwhen lets you run an if-else statement as an expression, which is particularly useful for creating new columns based on the values in another column.\nlit works in conjunction with when to tell Polars to use the literal value of something rather than try to find a corresponding column name:\nLet‚Äôs use them together to create a new column that splits participant responses that were faster and slower than 300ms:\nWe‚Äôll use the .with_columns context, because we want the result of our expression (the new column) and the original DataFrame:\n\n\nCode\n# Create a new column rt_split that contains the result of the following if/else statement:\n# If RT &gt;= 300, set the value to the lit(eral) string 'slow'\n# Otherwise, set the value to the lit(eral) string 'fast'\n\n# Start with_columns context\nddf = df_1.with_columns(\n    rt_split=when(\n            col('rt') &gt;= 300).then(lit('slow')).otherwise(lit('fast') # expression inside function\n        )\n    )\n# We saved the output to a new variable called ddf\nddf\n\n\n\nshape: (15, 4)\n\n\n\nparticipant\naccuracy\nrt\nrt_split\n\n\ni64\ni64\nf64\nstr\n\n\n\n\n1\n44\n261.28435\n\"fast\"\n\n\n1\n47\n728.208489\n\"slow\"\n\n\n1\n64\n801.889016\n\"slow\"\n\n\n1\n67\n713.555026\n\"slow\"\n\n\n1\n67\n362.682105\n\"slow\"\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n3\n70\n439.524973\n\"slow\"\n\n\n3\n88\n55.11928\n\"fast\"\n\n\n3\n88\n644.801272\n\"slow\"\n\n\n3\n12\n571.800553\n\"slow\"\n\n\n3\n58\n715.224208\n\"slow\"\n\n\n\n\n\n\n\n\n\n\n\n\nTipYour Turn\n\n\n\nUse when and lit to add a column to the DataFrame called performance.\nIt should contain the string ‚Äòsuccess‚Äô if accuracy &gt;= 50, or ‚Äòfail‚Äô if it was &lt; 50.\nSave the result to a new dataframe called df_new and print the first 10 rows:\n\n\n\n\nCode\n# Your code here\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\n\nCode\ndf_new = df_1.with_columns(\n    performance = when(col('accuracy') &gt;= 50).then(lit('success')).otherwise(lit('fail'))\n)\ndf_new.head(10)\n\n\n\nshape: (10, 4)\n\n\n\nparticipant\naccuracy\nrt\nperformance\n\n\ni64\ni64\nf64\nstr\n\n\n\n\n1\n44\n261.28435\n\"fail\"\n\n\n1\n47\n728.208489\n\"fail\"\n\n\n1\n64\n801.889016\n\"success\"\n\n\n1\n67\n713.555026\n\"success\"\n\n\n1\n67\n362.682105\n\"success\"\n\n\n2\n9\n502.974663\n\"fail\"\n\n\n2\n83\n424.866821\n\"success\"\n\n\n2\n21\n492.355273\n\"fail\"\n\n\n2\n36\n573.594895\n\"fail\"\n\n\n2\n87\n491.05526\n\"success\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipYour Turn\n\n\n\nUsing the previous DataFrame (df_new), summarize how many successes and failures each participant had.\nYour result should have 6 rows: 2 for each participant\n\n\n\n\nCode\n# Your code here\n\n\n# Hint: you can group_by multiple columns by passing a list of column names, e.g.\n\n# df_new.group_by(['col_1', 'col_2']).agg(...)\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\n\nCode\n# First create df_new if it wasn't created above\ndf_new = df_1.with_columns(\n    performance = when(col('accuracy') &gt;= 50).then(lit('success')).otherwise(lit('fail'))\n)\n\ndf_new.group_by(['participant', 'performance'], maintain_order=True).agg(\n    count = col('accuracy').count()\n)\n\n\n\nshape: (6, 3)\n\n\n\nparticipant\nperformance\ncount\n\n\ni64\nstr\nu32\n\n\n\n\n1\n\"fail\"\n2\n\n\n1\n\"success\"\n3\n\n\n2\n\"fail\"\n3\n\n\n2\n\"success\"\n2\n\n\n3\n\"success\"\n4\n\n\n3\n\"fail\"\n1",
    "crumbs": [
      "Schedule",
      "Week 1",
      "Polars (dataframes) Crash Course"
    ]
  },
  {
    "objectID": "weeks/01/lab/polars-crash-course.html#more-complex-expressions-with-attribute-type-operations",
    "href": "weeks/01/lab/polars-crash-course.html#more-complex-expressions-with-attribute-type-operations",
    "title": "A Crash Course on Python DataFrames",
    "section": "More complex expressions with attribute (type) operations",
    "text": "More complex expressions with attribute (type) operations\nIn addition to importing functions to build more complicated expressions, Polars also allows you to perform specific operations based upon the type of data in a column.\nYou don‚Äôt need to import anything to use these. Instead, you can use . syntax to ‚Äúnarrow down‚Äù to the type of data attribute you want, and then select the operations you would like.\nFor example, we‚Äôll use the DataFrame we created in the previous section, ddf:\n\n\nCode\nddf.head()\n\n\n\nshape: (5, 4)\n\n\n\nparticipant\naccuracy\nrt\nrt_split\n\n\ni64\ni64\nf64\nstr\n\n\n\n\n1\n44\n261.28435\n\"fast\"\n\n\n1\n47\n728.208489\n\"slow\"\n\n\n1\n64\n801.889016\n\"slow\"\n\n\n1\n67\n713.555026\n\"slow\"\n\n\n1\n67\n362.682105\n\"slow\"\n\n\n\n\n\n\nTo create an expression that converts each value in the new ‚Äúrt_split‚Äù column to uppercase.\nWe can do this by selecting with col() as usual, but then before calling an operation with . like before, we first access the .str attribute, and then call operations that specifically operate on strings!\n\n\nCode\nddf.with_columns(\n    col('rt_split').str.to_uppercase() # .uppercase() is only available to str data!\n)\n\n\n\nshape: (15, 4)\n\n\n\nparticipant\naccuracy\nrt\nrt_split\n\n\ni64\ni64\nf64\nstr\n\n\n\n\n1\n44\n261.28435\n\"FAST\"\n\n\n1\n47\n728.208489\n\"SLOW\"\n\n\n1\n64\n801.889016\n\"SLOW\"\n\n\n1\n67\n713.555026\n\"SLOW\"\n\n\n1\n67\n362.682105\n\"SLOW\"\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n3\n70\n439.524973\n\"SLOW\"\n\n\n3\n88\n55.11928\n\"FAST\"\n\n\n3\n88\n644.801272\n\"SLOW\"\n\n\n3\n12\n571.800553\n\"SLOW\"\n\n\n3\n58\n715.224208\n\"SLOW\"\n\n\n\n\n\n\nWithout .str to ‚Äúnarrow-in‚Äù to the string attribute Polars will complain about an AttributeError, because only str types have a .to_uppercase() operation!\n\n\nCode\nddf.with_columns(\n    col('rt_split').to_uppercase() # no .str\n)\n\n\n\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nCell In[67], line 2\n      1 ddf.with_columns(\n----&gt; 2     col('rt_split').to_uppercase() # no .str\n      3 )\n\nAttributeError: 'Expr' object has no attribute 'to_uppercase'\n\n\n\nPolars includes many attribute operations. The most common ones you‚Äôll use are for working with:\n.str: if your data are strings\n.name: which allows you to quickly change the names of columns from within a more complicated expression.\n.list: if your columns contain Python lists\nFor example, below we using a single expression inside the with_columns context below to calculate the mean of the accuracy and rt columns.\n\n\nCode\ndf_1.with_columns(col('accuracy', 'rt').mean())\n\n\n\nshape: (15, 3)\n\n\n\nparticipant\naccuracy\nrt\n\n\ni64\nf64\nf64\n\n\n\n\n1\n56.066667\n518.595745\n\n\n1\n56.066667\n518.595745\n\n\n1\n56.066667\n518.595745\n\n\n1\n56.066667\n518.595745\n\n\n1\n56.066667\n518.595745\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n3\n56.066667\n518.595745\n\n\n3\n56.066667\n518.595745\n\n\n3\n56.066667\n518.595745\n\n\n3\n56.066667\n518.595745\n\n\n3\n56.066667\n518.595745\n\n\n\n\n\n\nBecause we‚Äôre using .with_columns, the output of our expression is overwriting the original values in the accuracy and rt columns.\nWe saw how to rename output when we had separate col('accuracy').mean() and col('rt').mean() expressions: using .alias() at the end or = at the beginning.\nBut how do we change the names of both columns at the same time?\nAccessing the .name attribute gives us access to additional operations that help us out. In this case we use the .suffix() operation to add a suffix to the output name(s).\n\n\nCode\ndf_1.with_columns(\n    col('accuracy', 'rt').mean().name.suffix('_mean')\n)\n\n\n\nshape: (15, 5)\n\n\n\nparticipant\naccuracy\nrt\naccuracy_mean\nrt_mean\n\n\ni64\ni64\nf64\nf64\nf64\n\n\n\n\n1\n44\n261.28435\n56.066667\n518.595745\n\n\n1\n47\n728.208489\n56.066667\n518.595745\n\n\n1\n64\n801.889016\n56.066667\n518.595745\n\n\n1\n67\n713.555026\n56.066667\n518.595745\n\n\n1\n67\n362.682105\n56.066667\n518.595745\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n3\n70\n439.524973\n56.066667\n518.595745\n\n\n3\n88\n55.11928\n56.066667\n518.595745\n\n\n3\n88\n644.801272\n56.066667\n518.595745\n\n\n3\n12\n571.800553\n56.066667\n518.595745\n\n\n3\n58\n715.224208\n56.066667\n518.595745\n\n\n\n\n\n\nNow we have the original accuracy and rt columns and the newly named ones we created!",
    "crumbs": [
      "Schedule",
      "Week 1",
      "Polars (dataframes) Crash Course"
    ]
  },
  {
    "objectID": "weeks/01/lab/polars-crash-course.html#building-expressions-from-additional-selectors",
    "href": "weeks/01/lab/polars-crash-course.html#building-expressions-from-additional-selectors",
    "title": "A Crash Course on Python DataFrames",
    "section": "Building expressions from additional selectors",
    "text": "Building expressions from additional selectors\nSo far we‚Äôve seen how to use col() to select 1 or more columns we want to create an expression about.\nBut sometimes you need to select things in more complicated ways. Fortunately, Polars has additional selectors that we can use to express ourselves. A common pattern is to import these together using as:\nfrom polars import selectors as cs\nThen we can refer to these using cs.some_selector(). Some of these include:\n\ncs.all()\ncs.exclude()\ncs.starts_with()\ncs.string()\n\nLet‚Äôs see some of these in action using a dataset that include a column of reaction times:\n\n\nCode\nimport polars.selectors as cs\n\ndf_1.select(cs.all().count())  # &lt;- get all cols and calc count()\n\n\n\nshape: (1, 3)\n\n\n\nparticipant\naccuracy\nrt\n\n\nu32\nu32\nu32\n\n\n\n\n15\n15\n15\n\n\n\n\n\n\nThis is as the same as the following code, but many fewer lines!\n\n\nCode\ndf_1.select(\n    col('participant').count(),\n    col('accuracy').count(), col('rt').count()\n)\n\n\n\nshape: (1, 3)\n\n\n\nparticipant\naccuracy\nrt\n\n\nu32\nu32\nu32\n\n\n\n\n15\n15\n15\n\n\n\n\n\n\nAnd cs.exclude is the opposite of cs.all()\n\n\nCode\ndf_1.select(cs.exclude('participant').mean())  # &lt;- all cols except participant\n\n\n\nshape: (1, 2)\n\n\n\naccuracy\nrt\n\n\nf64\nf64\n\n\n\n\n56.066667\n518.595745\n\n\n\n\n\n\nWe can select all columns that start with certain characters:\n\n\nCode\ndf_1.select(cs.starts_with('pa').n_unique())\n\n\n\nshape: (1, 1)\n\n\n\nparticipant\n\n\nu32\n\n\n\n\n3\n\n\n\n\n\n\nOr even select columns based on the type of data they contain. In this case all the columns with Integer data:\n\n\nCode\ndf_1.select(cs.integer())\n\n\n\nshape: (15, 2)\n\n\n\nparticipant\naccuracy\n\n\ni64\ni64\n\n\n\n\n1\n44\n\n\n1\n47\n\n\n1\n64\n\n\n1\n67\n\n\n1\n67\n\n\n‚Ä¶\n‚Ä¶\n\n\n3\n70\n\n\n3\n88\n\n\n3\n88\n\n\n3\n12\n\n\n3\n58\n\n\n\n\n\n\nThere are a many more useful selectors. So check out the selector documentation page when you‚Äôre trying the challenge exercises later on in this notebook",
    "crumbs": [
      "Schedule",
      "Week 1",
      "Polars (dataframes) Crash Course"
    ]
  },
  {
    "objectID": "weeks/01/lab/polars-crash-course.html#reshaping-dataframes",
    "href": "weeks/01/lab/polars-crash-course.html#reshaping-dataframes",
    "title": "A Crash Course on Python DataFrames",
    "section": "Reshaping DataFrames",
    "text": "Reshaping DataFrames\nSometimes you‚Äôll find yourself working ‚Äúnon-tidy‚Äù DataFrames or ‚Äúwide‚Äù format data.\nWhat‚Äôs tidy-data again?\n\nEach variable must have its own column.\nEach observation must have its own row.\nEach value must have its own cell.\n\nIn polars we can achieve this using:\n.pivot(): long -&gt; wide, similar to pivot_wider() in R .unpivot(): wide -&gt; long, similar to pivot_longer() in R pl.concat(): combine 2 or more DataFrames/columns/rows into a larger DataFrame\nHere, I‚Äôve generated data from two participants with three observations. This data frame is not tidy since each row contains more than a single observation.\n\n\nCode\ndf_2 = pl.DataFrame(\n        {'participant': [1, 2],\n        'observation_1': [10, 25],\n        'observation_2': [100, 63],\n        'observation_3': [24, 45]\n        }\n    )\ndf_2\n\n\n\nshape: (2, 4)\n\n\n\nparticipant\nobservation_1\nobservation_2\nobservation_3\n\n\ni64\ni64\ni64\ni64\n\n\n\n\n1\n10\n100\n24\n\n\n2\n25\n63\n45\n\n\n\n\n\n\nWe can make it tidy by using the .unpivot method on the DataFrame, which takes 4 arguments:\non: the column(s) that contain values for each row index: the column(s) to use as the identifier across rows variable_name: name of the column that contains the original column names value_name: name of the column that contains the values that were previous spread across columns\n\n\nCode\n# Just breaking up over lines to keep it readable!\ndf_long = df_2.unpivot(\n    on=cs.starts_with('observation'),\n    index='participant',\n    variable_name='trial',\n    value_name='rating'\n    )\ndf_long\n\n\n\nshape: (6, 3)\n\n\n\nparticipant\ntrial\nrating\n\n\ni64\nstr\ni64\n\n\n\n\n1\n\"observation_1\"\n10\n\n\n2\n\"observation_1\"\n25\n\n\n1\n\"observation_2\"\n100\n\n\n2\n\"observation_2\"\n63\n\n\n1\n\"observation_3\"\n24\n\n\n2\n\"observation_3\"\n45\n\n\n\n\n\n\nThe .pivot method is the counter-part of .unpivot. We can use it to turn tidydata (long) to wide format. It takes 4 arguments as well:\non: the column(s) whose values will be turned into new columns index: the column(s) that are unique rows in the new DataFrame values: the values that will be moved into new columns with each row aggregate_function: how to aggregate multiple rows within each index, e.g.¬†None, mean, first, sum, etc\n\n\nCode\ndf_long.pivot(\n    on='trial',\n    index='participant',\n    values='rating',\n    aggregate_function=None\n    )\n\n\n\nshape: (2, 4)\n\n\n\nparticipant\nobservation_1\nobservation_2\nobservation_3\n\n\ni64\ni64\ni64\ni64\n\n\n\n\n1\n10\n100\n24\n\n\n2\n25\n63\n45\n\n\n\n\n\n\nYou can safely set aggregate_function = None if you don‚Äôt have repeated observations within each unique combination of index and on. In this case each participant only has a single ‚Äúobservation_1‚Äù, ‚Äúobservation_2‚Äù, and ‚Äúobservation_3‚Äù.\nBut if they had multiple, Polars will raise an error and ask you to specify how to aggregate them",
    "crumbs": [
      "Schedule",
      "Week 1",
      "Polars (dataframes) Crash Course"
    ]
  },
  {
    "objectID": "weeks/01/lab/polars-crash-course.html#splitting-1-column-into-many",
    "href": "weeks/01/lab/polars-crash-course.html#splitting-1-column-into-many",
    "title": "A Crash Course on Python DataFrames",
    "section": "Splitting 1 column into many",
    "text": "Splitting 1 column into many\nSometimes you‚Äôll need to split one column into multiple columns. Let‚Äôs say we wanted to split the ‚Äúyear_month‚Äù column into 2 separate columns of ‚Äúyear‚Äù and ‚Äúmonth‚Äù:\n\n\nCode\ndf_3 = pl.DataFrame({'id': [1, 2, 3], 'year_month': ['2021-01', '2021-02', '2021-03']})\ndf_3\n\n\n\nshape: (3, 2)\n\n\n\nid\nyear_month\n\n\ni64\nstr\n\n\n\n\n1\n\"2021-01\"\n\n\n2\n\"2021-02\"\n\n\n3\n\"2021-03\"\n\n\n\n\n\n\nYou can use attribute operations for .str to do this!\nSpecifically we use can .split_exact to split a str into a n+1 parts.\n\n\nCode\ndf_split = df_3.with_columns(\n    col('year_month').str.split_exact('-', 1)\n)\ndf_split  # string attribute method, to split by delimiter \"-\" into 2 parts\n\n\n\nshape: (3, 2)\n\n\n\nid\nyear_month\n\n\ni64\nstruct[2]\n\n\n\n\n1\n{\"2021\",\"01\"}\n\n\n2\n{\"2021\",\"02\"}\n\n\n3\n{\"2021\",\"03\"}\n\n\n\n\n\n\nPolars stores these parts in a struct which is just a Python dictionary:\n\n\nCode\n# First row, second column value\ndf_split[0, 1]\n\n\n{'field_0': '2021', 'field_1': '01'}\n\n\nPolars provides additional attribute operations on the .struct to create new columns.\nFirst we‚Äôll call .rename_fields to rename the fields of the struct (equivalent to renaming the keys of a Python dictionary).\n\n\nCode\n# string attribute method, to split by delimiter \"-\" into 2 parts\n# struct attribute method to rename fields\ndf_split_1 = df_3.with_columns(\n    col('year_month').str.split_exact('-', 1).struct.rename_fields(['year', 'month'])\n    )\ndf_split_1\n\n\n\nshape: (3, 2)\n\n\n\nid\nyear_month\n\n\ni64\nstruct[2]\n\n\n\n\n1\n{\"2021\",\"01\"}\n\n\n2\n{\"2021\",\"02\"}\n\n\n3\n{\"2021\",\"03\"}\n\n\n\n\n\n\n\n\nCode\n# First row, second column value\ndf_split_1[0, 1]\n\n\n{'year': '2021', 'month': '01'}\n\n\nThen we‚Äôll call struct.unnest() to create new columns, 1 per field\n\n\nCode\n# string attribute method, to split by delimiter \"-\" into 2 parts\n# struct attribute method to rename fields  # struct attribute method to create 1 column per field\ndf_split_2 = df_3.with_columns(\n    col('year_month').str.split_exact('-', 1).struct.rename_fields(['year', 'month']).struct.unnest()\n    )\ndf_split_2\n\n\n\nshape: (3, 4)\n\n\n\nid\nyear_month\nyear\nmonth\n\n\ni64\nstr\nstr\nstr\n\n\n\n\n1\n\"2021-01\"\n\"2021\"\n\"01\"\n\n\n2\n\"2021-02\"\n\"2021\"\n\"02\"\n\n\n3\n\"2021-03\"\n\"2021\"\n\"03\"\n\n\n\n\n\n\nWe can also split up values in a column over rows .explode('column_name') method on the DataFrame itself:\n\n\nCode\ndf_4 = pl.DataFrame({'letters': ['a', 'a', 'b', 'c'], 'numbers': [[1], [2, 3], [4, 5], [6, 7, 8]]})\ndf_4\n\n\n\nshape: (4, 2)\n\n\n\nletters\nnumbers\n\n\nstr\nlist[i64]\n\n\n\n\n\"a\"\n[1]\n\n\n\"a\"\n[2, 3]\n\n\n\"b\"\n[4, 5]\n\n\n\"c\"\n[6, 7, 8]\n\n\n\n\n\n\n\n\nCode\ndf_4.explode('numbers')\n\n\n\nshape: (8, 2)\n\n\n\nletters\nnumbers\n\n\nstr\ni64\n\n\n\n\n\"a\"\n1\n\n\n\"a\"\n2\n\n\n\"a\"\n3\n\n\n\"b\"\n4\n\n\n\"b\"\n5\n\n\n\"c\"\n6\n\n\n\"c\"\n7\n\n\n\"c\"\n8",
    "crumbs": [
      "Schedule",
      "Week 1",
      "Polars (dataframes) Crash Course"
    ]
  },
  {
    "objectID": "weeks/01/lab/polars-crash-course.html#combining-many-columns-into-1",
    "href": "weeks/01/lab/polars-crash-course.html#combining-many-columns-into-1",
    "title": "A Crash Course on Python DataFrames",
    "section": "Combining many columns into 1",
    "text": "Combining many columns into 1\nWe can combine columns into a single column using additional functions in an expression like pl.concat_list() and pl.concat_str(), which take column names as input:\n\n\nCode\ndf_split_2.with_columns(\n    month_year=pl.concat_str('month', 'year', separator='-')\n)\n\n\n\nshape: (3, 5)\n\n\n\nid\nyear_month\nyear\nmonth\nmonth_year\n\n\ni64\nstr\nstr\nstr\nstr\n\n\n\n\n1\n\"2021-01\"\n\"2021\"\n\"01\"\n\"01-2021\"\n\n\n2\n\"2021-02\"\n\"2021\"\n\"02\"\n\"02-2021\"\n\n\n3\n\"2021-03\"\n\"2021\"\n\"03\"\n\"03-2021\"\n\n\n\n\n\n\nPolars also includes various functions that end with _horizontal.\nLike the suffix implies, these functions are design to operate horizontally across columns within each row separately.\nLet‚Äôs say our DataFrame had these 3 numeric columns:\n\n\nCode\nimport numpy as np  # we haven't met this library yet, just using it to generate data\ndf_5 = df_4.with_columns(\n    a=np.random.normal(size=df_4.height),\n    b=np.random.normal(size=df_4.height),\n    c=np.random.normal(size=df_4.height)\n)\ndf_5\n\n\n\nshape: (4, 5)\n\n\n\nletters\nnumbers\na\nb\nc\n\n\nstr\nlist[i64]\nf64\nf64\nf64\n\n\n\n\n\"a\"\n[1]\n1.38304\n1.517516\n-0.485741\n\n\n\"a\"\n[2, 3]\n1.255348\n-1.143134\n2.937848\n\n\n\"b\"\n[4, 5]\n-1.109448\n0.366913\n-1.339077\n\n\n\"c\"\n[6, 7, 8]\n1.188979\n1.125729\n0.025955\n\n\n\n\n\n\nAnd we want to create a new column that is the average of these 3 columns within each row. We can easily to that using a horizontal function like pl.mean_horizontal\n\n\nCode\ndf_5.with_columns(abc_mean=pl.mean_horizontal('a', 'b', 'c'))\n\n\n\nshape: (4, 6)\n\n\n\nletters\nnumbers\na\nb\nc\nabc_mean\n\n\nstr\nlist[i64]\nf64\nf64\nf64\nf64\n\n\n\n\n\"a\"\n[1]\n1.38304\n1.517516\n-0.485741\n0.804938\n\n\n\"a\"\n[2, 3]\n1.255348\n-1.143134\n2.937848\n1.016687\n\n\n\"b\"\n[4, 5]\n-1.109448\n0.366913\n-1.339077\n-0.693871\n\n\n\"c\"\n[6, 7, 8]\n1.188979\n1.125729\n0.025955\n0.780221",
    "crumbs": [
      "Schedule",
      "Week 1",
      "Polars (dataframes) Crash Course"
    ]
  },
  {
    "objectID": "weeks/01/lab/polars-crash-course.html#your-turn",
    "href": "weeks/01/lab/polars-crash-course.html#your-turn",
    "title": "A Crash Course on Python DataFrames",
    "section": "Your Turn",
    "text": "Your Turn\n\n\n\n\n\n\nTipYour Turn\n\n\n\nMake the following DataFrame ‚Äútidy‚Äù, i.e.¬†long-format with 4 columns:\n\nparticipant: integer of participant ID\norder: integer stimulus and observation order (from column names)\nstimulus: string of stimulus name\nobservation: float of numeric rating each participant gave\n\n\n\n\n\nCode\nreshape = pl.DataFrame({\n    'participant': [1., 2.],\n    'stimulus_1': ['flower', 'car'],\n    'observation_1': [10., 25.,],\n    'stimulus_2': ['house', 'flower'],\n    'observation_2': [100., 63.,],\n    'stimulus_3': ['car', 'house'],\n    'observation_3': [24., 45.,]\n})\nreshape\n\n\n\nshape: (2, 7)\n\n\n\nparticipant\nstimulus_1\nobservation_1\nstimulus_2\nobservation_2\nstimulus_3\nobservation_3\n\n\nf64\nstr\nf64\nstr\nf64\nstr\nf64\n\n\n\n\n1.0\n\"flower\"\n10.0\n\"house\"\n100.0\n\"car\"\n24.0\n\n\n2.0\n\"car\"\n25.0\n\"flower\"\n63.0\n\"house\"\n45.0\n\n\n\n\n\n\nHints\nThink about this as a sequence of 4 steps. We‚Äôve created 4 code cells below with an image above each of the expected result:\n1. unpivot wide -&gt; long\n\n\n\nCode\n# Your code here\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\n\nCode\nstep1 = reshape.unpivot(\n    on=cs.exclude('participant'),\n    index='participant',\n    variable_name='trial',\n    value_name='rating'\n)\nstep1\n\n\n\nshape: (12, 3)\n\n\n\nparticipant\ntrial\nrating\n\n\nf64\nstr\nstr\n\n\n\n\n1.0\n\"stimulus_1\"\n\"flower\"\n\n\n2.0\n\"stimulus_1\"\n\"car\"\n\n\n1.0\n\"observation_1\"\n\"10.0\"\n\n\n2.0\n\"observation_1\"\n\"25.0\"\n\n\n1.0\n\"stimulus_2\"\n\"house\"\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n2.0\n\"observation_2\"\n\"63.0\"\n\n\n1.0\n\"stimulus_3\"\n\"car\"\n\n\n2.0\n\"stimulus_3\"\n\"house\"\n\n\n1.0\n\"observation_3\"\n\"24.0\"\n\n\n2.0\n\"observation_3\"\n\"45.0\"\n\n\n\n\n\n\n\n\n\n2. split the variable_name column from the previous step (I called it trial) into 2 new columns by _ (which I called index and order)\n\n\n\nCode\n# Your code here\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\n\nCode\nstep2 = step1.with_columns(\n    col('trial').str.split_exact('_', 1).struct.rename_fields(['index', 'order']).struct.unnest()\n)\nstep2\n\n\n\nshape: (12, 5)\n\n\n\nparticipant\ntrial\nrating\nindex\norder\n\n\nf64\nstr\nstr\nstr\nstr\n\n\n\n\n1.0\n\"stimulus_1\"\n\"flower\"\n\"stimulus\"\n\"1\"\n\n\n2.0\n\"stimulus_1\"\n\"car\"\n\"stimulus\"\n\"1\"\n\n\n1.0\n\"observation_1\"\n\"10.0\"\n\"observation\"\n\"1\"\n\n\n2.0\n\"observation_1\"\n\"25.0\"\n\"observation\"\n\"1\"\n\n\n1.0\n\"stimulus_2\"\n\"house\"\n\"stimulus\"\n\"2\"\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n2.0\n\"observation_2\"\n\"63.0\"\n\"observation\"\n\"2\"\n\n\n1.0\n\"stimulus_3\"\n\"car\"\n\"stimulus\"\n\"3\"\n\n\n2.0\n\"stimulus_3\"\n\"house\"\n\"stimulus\"\n\"3\"\n\n\n1.0\n\"observation_3\"\n\"24.0\"\n\"observation\"\n\"3\"\n\n\n2.0\n\"observation_3\"\n\"45.0\"\n\"observation\"\n\"3\"\n\n\n\n\n\n\n\n\n\n3. select only the columns: participant, 2 you created (I called mine index and order), and the value_name column from the first step (I called it rating)\n\n\n\nCode\n# Your code here\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\n\nCode\nstep3 = step2.select(col('participant', 'index', 'order', 'rating'))\nstep3\n\n\n\nshape: (12, 4)\n\n\n\nparticipant\nindex\norder\nrating\n\n\nf64\nstr\nstr\nstr\n\n\n\n\n1.0\n\"stimulus\"\n\"1\"\n\"flower\"\n\n\n2.0\n\"stimulus\"\n\"1\"\n\"car\"\n\n\n1.0\n\"observation\"\n\"1\"\n\"10.0\"\n\n\n2.0\n\"observation\"\n\"1\"\n\"25.0\"\n\n\n1.0\n\"stimulus\"\n\"2\"\n\"house\"\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n2.0\n\"observation\"\n\"2\"\n\"63.0\"\n\n\n1.0\n\"stimulus\"\n\"3\"\n\"car\"\n\n\n2.0\n\"stimulus\"\n\"3\"\n\"house\"\n\n\n1.0\n\"observation\"\n\"3\"\n\"24.0\"\n\n\n2.0\n\"observation\"\n\"3\"\n\"45.0\"\n\n\n\n\n\n\n\n\n\n4. pivot long -&gt; wide to break-out the value_name column (I called it rating) into multiple columns\n\n\n\nCode\n# Your code here\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\n\nCode\nstep4 = step3.pivot(\n    on='index',\n    index=['participant', 'order'],\n    values='rating',\n    aggregate_function=None\n)\nstep4\n\n\n\nshape: (6, 4)\n\n\n\nparticipant\norder\nstimulus\nobservation\n\n\nf64\nstr\nstr\nstr\n\n\n\n\n1.0\n\"1\"\n\"flower\"\n\"10.0\"\n\n\n2.0\n\"1\"\n\"car\"\n\"25.0\"\n\n\n1.0\n\"2\"\n\"house\"\n\"100.0\"\n\n\n2.0\n\"2\"\n\"flower\"\n\"63.0\"\n\n\n1.0\n\"3\"\n\"car\"\n\"24.0\"\n\n\n2.0\n\"3\"\n\"house\"\n\"45.0\"",
    "crumbs": [
      "Schedule",
      "Week 1",
      "Polars (dataframes) Crash Course"
    ]
  },
  {
    "objectID": "weeks/01/lab/polars-crash-course.html#additional-resources",
    "href": "weeks/01/lab/polars-crash-course.html#additional-resources",
    "title": "A Crash Course on Python DataFrames",
    "section": "Additional Resources",
    "text": "Additional Resources\nHere a few additional resources that might be helpful on your journey:\n\nPolars official intro tutorial\nMore Comprehensive intro to Polars\nTidyData analysis in Polars\nPolars patterns vs R‚Äôs dplyr",
    "crumbs": [
      "Schedule",
      "Week 1",
      "Polars (dataframes) Crash Course"
    ]
  },
  {
    "objectID": "weeks/02/index.html",
    "href": "weeks/02/index.html",
    "title": "Week 2",
    "section": "",
    "text": "Cautionüìö HW1 (Deadline 1pm Tues Jan 20)\n\n\n\nGithub Classroom Assignment\n\n\n\n\n\nDeveloping statistical intuitions\nBridging the two cultures of statistics\nGetting familiar with plotting & EDA in Python",
    "crumbs": [
      "Schedule",
      "Week 2",
      "Overview"
    ]
  },
  {
    "objectID": "weeks/02/index.html#overview",
    "href": "weeks/02/index.html#overview",
    "title": "Week 2",
    "section": "",
    "text": "Cautionüìö HW1 (Deadline 1pm Tues Jan 20)\n\n\n\nGithub Classroom Assignment\n\n\n\n\n\nDeveloping statistical intuitions\nBridging the two cultures of statistics\nGetting familiar with plotting & EDA in Python",
    "crumbs": [
      "Schedule",
      "Week 2",
      "Overview"
    ]
  },
  {
    "objectID": "weeks/02/index.html#slides",
    "href": "weeks/02/index.html#slides",
    "title": "Week 2",
    "section": "Slides",
    "text": "Slides\n\n\n\n\n\n\nTipSlides Mon Jan 12th\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipSlides Wed Jan 14th",
    "crumbs": [
      "Schedule",
      "Week 2",
      "Overview"
    ]
  },
  {
    "objectID": "weeks/02/index.html#materials",
    "href": "weeks/02/index.html#materials",
    "title": "Week 2",
    "section": "Materials",
    "text": "Materials\n\n\n\n\n\n\nCautionüìö Lab 02 - Data visualization & EDA intro\n\n\n\nGithub Classroom Assignment",
    "crumbs": [
      "Schedule",
      "Week 2",
      "Overview"
    ]
  },
  {
    "objectID": "weeks/02/index.html#mentioned-references",
    "href": "weeks/02/index.html#mentioned-references",
    "title": "Week 2",
    "section": "Mentioned References",
    "text": "Mentioned References\n\n\n\n\n\n\nNoteStatistical Thinking (Chap 1) - What is statistical thinking?\n\n\n\n\n\n   \n    Unable to display PDF file. Download instead.\n  \n  \n\n\n\n\n\n\n\n\n\nNoteYarkoni & Westfall (2017) - Prediction vs Explanation\n\n\n\n\n\n   \n    Unable to display PDF file. Download instead.\n  \n  \n\n\n\n\n\n\n\n\n\nNoteBreiman (2001) - Two Cultures\n\n\n\n\n\n   \n    Unable to display PDF file. Download instead.\n  \n  \n\n\n\n\n\n\n\n\n\nNoteGelman & Vehtari (2021) - Most important statistical ideas\n\n\n\n\n\n   \n    Unable to display PDF file. Download instead.\n  \n  \n\n\n\n\n\n\n\n\n\nNoteBzdok (2017) - Classical Statistics & Statistical Learning\n\n\n\n\n\n   \n    Unable to display PDF file. Download instead.\n  \n  \n\n\n\n\n\n\n\n\n\nNoteBzdok & Ioannidis (2019)\n\n\n\n\n\n   \n    Unable to display PDF file. Download instead.\n  \n  \n\n\n\n\n\n\n\n\n\nNoteJolly & Chang (2019)\n\n\n\n\n\n   \n    Unable to display PDF file. Download instead.",
    "crumbs": [
      "Schedule",
      "Week 2",
      "Overview"
    ]
  },
  {
    "objectID": "weeks/02/lab/seaborn-crash-course.html",
    "href": "weeks/02/lab/seaborn-crash-course.html",
    "title": "A Crash Course on Python Dataviz",
    "section": "",
    "text": "In this tutorial we‚Äôll learn how to create statistical visualizations in Python using Seaborn. Just like polars gave us a consistent, intuitive way to work with DataFrames, seaborn gives us a consistent, intuitive way to visualize data.\nSeaborn is built on top of matplotlib (another plotting library), but provides a much higher-level interface designed specifically for exploring and creating statistical visualizations.",
    "crumbs": [
      "Schedule",
      "Week 2",
      "Seaborn (plotting) Crash Course"
    ]
  },
  {
    "objectID": "weeks/02/lab/seaborn-crash-course.html#how-to-use-this-notebook",
    "href": "weeks/02/lab/seaborn-crash-course.html#how-to-use-this-notebook",
    "title": "A Crash Course on Python Dataviz",
    "section": "How to use this notebook",
    "text": "How to use this notebook\nThis notebook is designed for you to work through at your own pace and use as a reference later.\nAs you go through this notebook, regularly consult the seaborn documentation.* Learning to quickly scan and read read API docs is a critical skill - the docs show you every parameter and option available.\n*Note: we‚Äôre NOT using seaborn‚Äôs ‚ÄúObjects interface‚Äù which is even more like ggplot, but still has a several rough edges that we wanted to avoid for the purposes of this class.\nYou can also always use the help() function from within this notebook:\nimport seaborn as sns\n\nhelp(sns.relplot)",
    "crumbs": [
      "Schedule",
      "Week 2",
      "Seaborn (plotting) Crash Course"
    ]
  },
  {
    "objectID": "weeks/02/lab/seaborn-crash-course.html#getting-started",
    "href": "weeks/02/lab/seaborn-crash-course.html#getting-started",
    "title": "A Crash Course on Python Dataviz",
    "section": "Getting Started",
    "text": "Getting Started\nWe import seaborn with the conventional alias sns, along with polars for data manipulation:\n\n\nCode\nimport seaborn as sns\nimport polars as pl\nfrom polars import col\n\n\nSeaborn includes several built-in datasets we can use to learn. Let‚Äôs load the famous penguins dataset - body measurements for three penguin species from three islands in Antarctica:\n\n\nCode\n# Load dataset and convert to polars DataFrame\npenguins = pl.DataFrame(sns.load_dataset(\"penguins\"))\npenguins.head()\n\n\n\nshape: (5, 7)\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\n\n\nstr\nstr\nf64\nf64\nf64\nf64\nstr\n\n\n\n\n\"Adelie\"\n\"Torgersen\"\n39.1\n18.7\n181.0\n3750.0\n\"Male\"\n\n\n\"Adelie\"\n\"Torgersen\"\n39.5\n17.4\n186.0\n3800.0\n\"Female\"\n\n\n\"Adelie\"\n\"Torgersen\"\n40.3\n18.0\n195.0\n3250.0\n\"Female\"\n\n\n\"Adelie\"\n\"Torgersen\"\nnull\nnull\nnull\nnull\nnull\n\n\n\"Adelie\"\n\"Torgersen\"\n36.7\n19.3\n193.0\n3450.0\n\"Female\"\n\n\n\n\n\n\n\n\n\n\n\n\nWarningPainpoint: seaborn really prefers pandas not polars\n\n\n\nMost seaborn functions are supposed to work with polars (our preferred) DataFrame library out-of-the-box. However, there are still a few rough-edges here and there.\nSo we highly recommend using a DataFrame‚Äôs .to_pandas() method, when you are calling a seaborn function instead of using the DataFrame directly. All the examples throughout this notebook use that pattern to remind you.\nIt‚Äôs also easy to convert the otherway by just passing a pandas DataFrame to pl.DataFrame():\n# polars -&gt; pandas for seaborn plotting\nmydf.to_pandas()\n\n# pandas -&gt; polars, if ever needed, e.g. loading built-in seaborn datasets\nmydf = pl.DataFrame(pandas_df)",
    "crumbs": [
      "Schedule",
      "Week 2",
      "Seaborn (plotting) Crash Course"
    ]
  },
  {
    "objectID": "weeks/02/lab/seaborn-crash-course.html#the-seaborn-mental-model",
    "href": "weeks/02/lab/seaborn-crash-course.html#the-seaborn-mental-model",
    "title": "A Crash Course on Python Dataviz",
    "section": "The seaborn mental model",
    "text": "The seaborn mental model\nSimilar to ggplot in R, you build figures by mapping column names to aesthetic properties\n\nAesthetic Mappings\n\n\n\nMapping\nWhat it controls\n\n\n\n\nx\nPosition on x-axis\n\n\ny\nPosition on y-axis\n\n\nhue\nColor of points/bars/lines\n\n\ncol\nCreate columns of subplots\n\n\nrow\nCreate rows of subplots\n\n\nstyle\nMarker style (scatter) or line style\n\n\nsize\nSize of points\n\n\n\n\n\nMain functions\nYou an mix and match how you use seaborn between two kinds of approaches:\n\n(recommended) Use one of the 3 main figure-level functions with the kind argument to pick a type\nUse any of the axis-level functions directly if you don‚Äôt need faceting (but you‚Äôll probably need to interact with matplotlib)\n\n\n\n\n\n\n\n\n\n\nFunction\nPurpose\nPlot Types\n\n\n\n\nsns.relplot()\nRelationships between numeric variables\nsns.scatter, sns.line\n\n\nsns.displot()\nDistributions of variables\nsns.hist, sns.kde, sns.ecdf\n\n\nsns.catplot()\nCategorical comparisons\nsns.strip, sns.box, sns.violin, sns.bar, sns.point\n\n\nsns.lmplot() (not pictured above)\nRegression models\nsns.regplot (not pictured above)\n\n\n\nEach function accepts the same mappings (along with some data) using and returns a FacetGrid - a container that can hold one or more subplots.",
    "crumbs": [
      "Schedule",
      "Week 2",
      "Seaborn (plotting) Crash Course"
    ]
  },
  {
    "objectID": "weeks/02/lab/seaborn-crash-course.html#visualizing-relationships-relplot",
    "href": "weeks/02/lab/seaborn-crash-course.html#visualizing-relationships-relplot",
    "title": "A Crash Course on Python Dataviz",
    "section": "Visualizing Relationships: relplot()",
    "text": "Visualizing Relationships: relplot()\nUse sns.relplot() when you want to see how numeric variables relate to each other. Let‚Äôs explore the relationship between flipper length and body mass in penguins:\n\n\n\n\n\n\nNote\n\n\n\nRemember Python cares about white-space and indentation except between (). So like the previous notebook, we‚Äôre using new lines to separate inputs to each seaborn function just for clarity. You can put them on a single line and it works the same\n\n\n\n\nCode\nsns.relplot(\n    data=penguins.to_pandas(), # &lt;- remember .to_pandas() to convert on-the-fly, no need for another variable\n    x=\"flipper_length_mm\",\n    y=\"body_mass_g\"\n)\n\n\n\n\n\n\n\n\n\nThere‚Äôs a clear positive relationship - penguins with longer flippers tend to be heavier.\nBut wait - are there differences by species? Let‚Äôs map hue to the species column:\n\n\nCode\nsns.relplot(\n    data=penguins.to_pandas(),\n    x=\"flipper_length_mm\",\n    y=\"body_mass_g\",\n    hue=\"species\"\n)\n\n\n\n\n\n\n\n\n\nNow we can see that Gentoo penguins (green) are generally larger than Adelie (blue) and Chinstrap (orange).\nWhat if we want separate plots for each species? Map col to species:\n\n\nCode\nsns.relplot(\n    data=penguins.to_pandas(),\n    x=\"flipper_length_mm\",\n    y=\"body_mass_g\",\n    hue=\"species\",\n    col=\"species\"\n)\n\n\n\n\n\n\n\n\n\n\nControlling Layout\nYou can control the size and arrangement of subplots: - height: Height of each subplot (in inches) - aspect: Width-to-height ratio - col_wrap: Maximum number of columns before wrapping\n\n\nCode\nsns.relplot(\n    data=penguins.to_pandas(),\n    x=\"flipper_length_mm\",\n    y=\"body_mass_g\",\n    col=\"species\",\n    # we left out hue so species is only mapped to column\n    height=3,\n    aspect=1.2\n)\n\n\n\n\n\n\n\n\n\n\n\nAdding Regression Lines with lmplot()\nWhen you want to see the trend in a relationship, use sns.lmplot(). It works just like relplot() but adds a regression line with an optional confidence band:\n\n\nCode\nsns.lmplot(\n    data=penguins.to_pandas(),\n    x=\"flipper_length_mm\",\n    y=\"body_mass_g\",\n    col=\"species\"\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteConfidence bands/bars\n\n\n\nOne of the strengths of seaborn (and over ggplot) is that it calculates confidence bands/intervals using bootstrap resampling by default. We‚Äôll cover cover statistical uncertainty and bootstrapping in more depth later in the course, but this is really nice for quick visuals without having to calculate additional statistics yourself, even if when working with small samples.\nYou can control this using the numerous additional arguments (inputs) that many seaborn functions support. Here are some common ones and the lmplot help page:\n\nn_boot (1000 by default)\nunits (None by default - great when you have repeated observations per cluster/participant/etc)\nerrrorbar/ ci (lmplot only) (95 by default)\n\n\n\n\n\n\n\n\n\nTipYour Turn\n\n\n\nCreate a scatter plot showing the relationship between bill_length_mm and bill_depth_mm, colored by species.\nWhat pattern do you notice? Does the overall trend match the within-species trends?\n\n\n\n\nCode\n# Your code here",
    "crumbs": [
      "Schedule",
      "Week 2",
      "Seaborn (plotting) Crash Course"
    ]
  },
  {
    "objectID": "weeks/02/lab/seaborn-crash-course.html#visualizing-distributions-displot",
    "href": "weeks/02/lab/seaborn-crash-course.html#visualizing-distributions-displot",
    "title": "A Crash Course on Python Dataviz",
    "section": "Visualizing Distributions: displot()",
    "text": "Visualizing Distributions: displot()\nUse sns.displot() when you want to understand the distribution of a variable - how values are spread out, where they cluster, whether there are outliers.\nThe kind parameter controls the type of distribution plot: - \"hist\": Histogram (default) - \"kde\": Kernel density estimate (smoothed histogram) - \"ecdf\": Empirical cumulative distribution function\n\nHistograms\nLet‚Äôs look at the distribution of body mass:\n\n\nCode\nsns.displot(\n    data=penguins.to_pandas(),\n    x=\"body_mass_g\",\n    kind=\"hist\"\n)\n\n\n\n\n\n\n\n\n\nHmm, this distribution doesn‚Äôt look unimodal - there might me more than one peak. Let‚Äôs try increasing the granularity by increasing the number of automatically calculated bins\n\n\nCode\nsns.displot(\n    data=penguins.to_pandas(),\n    x=\"body_mass_g\",\n    kind=\"hist\",\n    bins=30\n)\n\n\n\n\n\n\n\n\n\nAh we can see it a bit better now, but it‚Äôs probably driven the fact that we‚Äôre ignoring difference species\n\n\nCode\nsns.displot(\n    data=penguins.to_pandas(),\n    x=\"body_mass_g\",\n    kind=\"hist\",\n    hue=\"species\",\n    bins=30\n)\n\n\n\n\n\n\n\n\n\nThe bimodality comes from Gentoo penguins being much heavier than the other two species.\nWhen histograms overlap, you can use multiple=\"stack\" to stack them:\n\n\nCode\nsns.displot(\n    data=penguins.to_pandas(),\n    x=\"body_mass_g\",\n    kind=\"hist\",\n    hue=\"species\",\n    bins=30,\n    multiple=\"stack\"\n)\n\n\n\n\n\n\n\n\n\n\n\nKDE Plots\nKernel Density Estimation (KDE) creates a smoothed version of a histogram. It‚Äôs often easier to compare shapes:\n\n\nCode\nsns.displot(\n    data=penguins.to_pandas(),\n    x=\"body_mass_g\",\n    kind=\"kde\",\n    hue=\"species\",\n    fill=True  # Fill under the curves\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteChoosing bins and smoothing\n\n\n\nHistograms have bins (bins or binwidth parameters), KDE has smoothing (bw_adjust parameter).\nToo few bins / too much smoothing can hide patterns. Too many bins / too little smoothing creates noise.\nAlways try a few values to make sure you‚Äôre not missing something important.\n\n\n\n\nBivariate Distributions\nYou can visualize the joint distribution of two variables by specifying both x and y:\n\n\nCode\nsns.displot(\n    data=penguins.to_pandas(),\n    x=\"flipper_length_mm\",\n    y=\"body_mass_g\",\n    kind=\"kde\",\n    hue=\"species\"\n)\n\n\n\n\n\n\n\n\n\nThe contour lines show regions of equal density - like a topographic map of where the data concentrates.\n\n\n\n\n\n\nTipYour Turn\n\n\n\nCreate a histogram of bill_length_mm split by sex using col=\"sex\".\nWhat do you notice about the distributions?\n\n\n\n\nCode\n# Your code here",
    "crumbs": [
      "Schedule",
      "Week 2",
      "Seaborn (plotting) Crash Course"
    ]
  },
  {
    "objectID": "weeks/02/lab/seaborn-crash-course.html#visualizing-categories-catplot",
    "href": "weeks/02/lab/seaborn-crash-course.html#visualizing-categories-catplot",
    "title": "A Crash Course on Python Dataviz",
    "section": "Visualizing Categories: catplot()",
    "text": "Visualizing Categories: catplot()\nUse sns.catplot() when one of your variables is categorical (like species, sex, or experimental condition).\nThe kind parameter offers several options:\nShow individual observations: - \"strip\": Jittered points - \"swarm\": Points arranged to avoid overlap\nShow distributions: - \"box\": Box plots (median, quartiles, outliers) - \"violin\": Violin plots (KDE + box plot hybrid)\nShow summaries: - \"bar\": Mean with error bars - \"point\": Mean with error bars as points/lines\n\nShowing Raw Data: Strip and Swarm Plots\nStrip plots show every data point, jittered horizontally to reduce overlap:\n\n\nCode\nsns.catplot(\n    data=penguins.to_pandas(),\n    x=\"species\",\n    y=\"body_mass_g\",\n    kind=\"strip\"\n)\n\n\n\n\n\n\n\n\n\nSwarm plots arrange points so they don‚Äôt overlap, making the distribution shape visible:\n\n\nCode\nsns.catplot(\n    data=penguins.to_pandas(),\n    x=\"species\",\n    y=\"body_mass_g\",\n    kind=\"swarm\"\n)\n\n\n\n\n\n\n\n\n\nAdd hue to compare subgroups within each category:\n\n\nCode\nsns.catplot(\n    data=penguins.to_pandas(),\n    x=\"species\",\n    y=\"body_mass_g\",\n    hue=\"sex\",\n    kind=\"swarm\"\n)\n\n\n\n\n\n\n\n\n\n\n\nShowing Distributions: Box and Violin Plots\nBox plots summarize the distribution with quartiles:\n\n\nCode\nsns.catplot(\n    data=penguins.to_pandas(),\n    x=\"species\",\n    y=\"body_mass_g\",\n    hue=\"sex\",\n    kind=\"box\"\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteReading a box plot\n\n\n\nBox: Middle 50% of data (25th to 75th percentile) Line in box: Median (50th percentile) Whiskers: Extend to 1.5x the box width Diamonds: Outliers beyond the whiskers\n\n\nViolin plots combine a box plot with a KDE, showing the full distribution shape:\n\n\nCode\nsns.catplot(\n    data=penguins.to_pandas(),\n    x=\"species\",\n    y=\"body_mass_g\",\n    hue=\"sex\",\n    kind=\"violin\"\n)\n\n\n\n\n\n\n\n\n\n\n\nShowing Summaries: Bar and Point Plots\nBy default, bar plots show the mean of each group with error bars (95% CI by default):\n\n\nCode\nsns.catplot(\n    data=penguins.to_pandas(),\n    x=\"species\",\n    y=\"body_mass_g\",\n    hue=\"sex\",\n    kind=\"bar\"\n)\n\n\n\n\n\n\n\n\n\nWe can change this by passing in a different estimator either as a string that seaborn understands, or a custom function\n\n\nCode\nsns.catplot(\n    data=penguins.to_pandas(),\n    x=\"species\",\n    y=\"body_mass_g\",\n    hue=\"sex\",\n    kind=\"bar\",\n    estimator=\"median\" # &lt;- using the median\n)\n\n\n\n\n\n\n\n\n\nPoint plots are similar but use points and lines, which can be better for comparing groups within categories:\n\n\nCode\nsns.catplot(\n    data=penguins.to_pandas(),\n    x=\"sex\",\n    y=\"body_mass_g\",\n    hue=\"species\",\n    kind=\"point\",\n    estimator=\"min\", # &lt;- using the minimum\n    errorbar=None,  # &lt;- no uncertainty calculations\n)\n\n\n\n\n\n\n\n\n\n\n\nLayering Plots\n\n\n\n\n\n\nImportantShow all data when you can\n\n\n\nRemember our first statistical principle: aggregation Bar and point plots show only the summary statistic and uncertainty. They don‚Äôt tell you much about the actual range and shape (distribution) of the data.\n\n\nTo help layer multiple plots onto the same FacetGrid you can use the .map_dataframe() method that the FacetGrid has:\n\n\nCode\ngrid = sns.catplot(\n    data=penguins.to_pandas(),\n    x=\"species\",\n    y=\"body_mass_g\",\n    hue=\"sex\",\n    kind=\"bar\",\n)\n\n# Saving the output of catplot (FacetGrid) to a variable\n# makes it easy to get help on its *method*\nhelp(grid.map_dataframe)\n\n\nHelp on method map_dataframe in module seaborn.axisgrid:\n\nmap_dataframe(func, *args, **kwargs) method of seaborn.axisgrid.FacetGrid instance\n    Like ``.map`` but passes args as strings and inserts data in kwargs.\n    \n    This method is suitable for plotting with functions that accept a\n    long-form DataFrame as a `data` keyword argument and access the\n    data in that DataFrame using string variable names.\n    \n    Parameters\n    ----------\n    func : callable\n        A plotting function that takes data and keyword arguments. Unlike\n        the `map` method, a function used here must \"understand\" Pandas\n        objects. It also must plot to the currently active matplotlib Axes\n        and take a `color` keyword argument. If faceting on the `hue`\n        dimension, it must also take a `label` keyword argument.\n    args : strings\n        Column names in self.data that identify variables with data to\n        plot. The data for each variable is passed to `func` in the\n        order the variables are specified in the call.\n    kwargs : keyword arguments\n        All keyword arguments are passed to the plotting function.\n    \n    Returns\n    -------\n    self : object\n        Returns self.\n\n\n\n\n\n\n\n\n\n\nLet‚Äôs use the method. We can pass in sns.striplot as the function and then provide our mappings:\n\n\nCode\ngrid.map_dataframe(\n    sns.stripplot,\n    x=\"species\",\n    y=\"body_mass_g\",\n    hue=\"sex\",\n    dodge=True,\n    alpha=0.5\n)\n\n\n\n\n\n\n\n\nTipYour Turn\n\n\n\nCreate a box plot of flipper_length_mm by island, with separate subplots (col) for each species.\nAre there island differences within species?\nTry layering the data on top and trying to understand how it works\n\n\n\n\nCode\n# Your code here",
    "crumbs": [
      "Schedule",
      "Week 2",
      "Seaborn (plotting) Crash Course"
    ]
  },
  {
    "objectID": "weeks/02/lab/seaborn-crash-course.html#multi-panel-views",
    "href": "weeks/02/lab/seaborn-crash-course.html#multi-panel-views",
    "title": "A Crash Course on Python Dataviz",
    "section": "Multi-Panel Views",
    "text": "Multi-Panel Views\nSeaborn has two special functions for getting a quick overview of your data:\n\nsns.jointplot(): One relationship with marginal distributions\nsns.pairplot(): All pairwise relationships at once\n\n\nJoint Plots\njointplot() shows the relationship between two variables AND their individual distributions:\n\n\nCode\nsns.jointplot(\n    data=penguins.to_pandas(),\n    x=\"flipper_length_mm\",\n    y=\"body_mass_g\",\n    hue=\"species\"\n)\n\n\n\n\n\n\n\n\n\n\n\nPair Plots\npairplot() creates a matrix of scatter plots for all pairs of numeric variables:\n\n\nCode\n# pairplot needs pandas, so we convert\nsns.pairplot(\n    data=penguins.to_pandas(),\n    hue=\"species\"\n)\n\n\n\n\n\n\n\n\n\nThe diagonal shows distributions for each variable. Off-diagonal shows scatter plots between pairs.\nThis is a great way to quickly explore relationships in a new dataset!",
    "crumbs": [
      "Schedule",
      "Week 2",
      "Seaborn (plotting) Crash Course"
    ]
  },
  {
    "objectID": "weeks/02/lab/seaborn-crash-course.html#customizing-your-plots",
    "href": "weeks/02/lab/seaborn-crash-course.html#customizing-your-plots",
    "title": "A Crash Course on Python Dataviz",
    "section": "Customizing Your Plots",
    "text": "Customizing Your Plots\nSeaborn provides several ways to customize plots without diving into matplotlib.\n\nThemes and Contexts\nThemes control the overall style (background, grid lines, etc.): - \"darkgrid\" (default), \"whitegrid\", \"dark\", \"white\", \"ticks\"\nContexts control the scale (good for different output sizes): - \"paper\", \"notebook\" (default), \"talk\", \"poster\"\nUse sns.set_theme() to change both for all subsequent plots:\n\n\nCode\n# Set a clean theme with larger text\nsns.set_theme(style=\"whitegrid\", context=\"talk\")\n\nsns.relplot(\n    data=penguins.to_pandas(),\n    x=\"flipper_length_mm\",\n    y=\"body_mass_g\",\n    hue=\"species\"\n)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Reset to defaults\nsns.set_theme(style=\"darkgrid\", context=\"notebook\")\n\n\n\n\nColor Palettes\nSeaborn has many built-in color palettes. Pass a palette name to the palette parameter:\n\n\nCode\nsns.catplot(\n    data=penguins.to_pandas(),\n    x=\"species\",\n    y=\"body_mass_g\",\n    hue=\"species\",\n    kind=\"box\",\n    palette=\"Set2\"\n)\n\n\n\n\n\n\n\n\n\nCommon palettes: - Categorical: \"Set1\", \"Set2\", \"Paired\", \"tab10\" - Sequential: \"Blues\", \"Greens\", \"viridis\", \"rocket\" - Diverging: \"coolwarm\", \"RdBu\", \"vlag\"\n\n\nFacetGrid Methods\nThe FacetGrid returned by figure-level functions has methods for customization:\n\n\n\nMethod\nPurpose\n\n\n\n\n.set_axis_labels(x, y)\nChange axis labels\n\n\n.set_titles(template)\nChange subplot titles\n\n\n.set(xlim=, ylim=)\nSet axis limits\n\n\n.tight_layout()\nAdjust spacing\n\n\n.figure.suptitle()\nAdd overall title\n\n\n\n\n\nCode\ng = sns.relplot(\n    data=penguins.to_pandas(),\n    x=\"flipper_length_mm\",\n    y=\"body_mass_g\",\n    col=\"species\",\n    height=3\n)\n\n# Customize using FacetGrid methods\ng.set_axis_labels(\"Flipper Length (mm)\", \"Body Mass (g)\")\ng.set_titles(\"{col_name}\")\ng.figure.suptitle(\"Penguin Body Measurements\", y=1.02)\n\n\nText(0.5, 1.02, 'Penguin Body Measurements')\n\n\n\n\n\n\n\n\n\n\n\nControlling Order\nBy default, seaborn orders categorical variables as they appear in the data. Use these parameters to control order:\n\norder: Order for the x-axis variable\nhue_order: Order for hue categories\ncol_order, row_order: Order for subplot columns/rows\n\n\n\nCode\nsns.catplot(\n    data=penguins.to_pandas(),\n    x=\"species\",\n    y=\"body_mass_g\",\n    kind=\"bar\",\n    order=[\"Chinstrap\", \"Adelie\", \"Gentoo\"]  # Custom order\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipYour Turn\n\n\n\nCreate a violin plot of bill_length_mm by species, with: - A ‚Äúwhitegrid‚Äù theme - The ‚Äúpastel‚Äù color palette - Species ordered alphabetically - Axis labels ‚ÄúSpecies‚Äù and ‚ÄúBill Length (mm)‚Äù\n\n\n\n\nCode\n# Your code here",
    "crumbs": [
      "Schedule",
      "Week 2",
      "Seaborn (plotting) Crash Course"
    ]
  },
  {
    "objectID": "weeks/02/lab/seaborn-crash-course.html#pro-tips",
    "href": "weeks/02/lab/seaborn-crash-course.html#pro-tips",
    "title": "A Crash Course on Python Dataviz",
    "section": "Pro Tips",
    "text": "Pro Tips\nThese are common customization tasks that aren‚Äôt obvious from the basic API. Bookmark this section for future reference!\n\nMoving or Removing Legends\nSeaborn places legends automatically, but you often want them elsewhere. Use sns.move_legend():\n\n\nCode\nmyplot = sns.relplot(\n    data=penguins.to_pandas(),\n    x=\"flipper_length_mm\",\n    y=\"body_mass_g\",\n    hue=\"species\",\n    height=4\n)\n\n# Move legend outside the plot\nsns.move_legend(myplot, \"upper left\", bbox_to_anchor=(1, 1))\n\n# Since we saved the plot to a variable we need to type the variable name to see it\nmyplot\n\n\n\n\n\n\n\n\n\nTo remove a legend entirely, set legend=False in the plotting function:\n\n\nCode\nsns.relplot(\n    data=penguins.to_pandas(),\n    x=\"flipper_length_mm\",\n    y=\"body_mass_g\",\n    hue=\"species\",\n    legend=False,  # No legend\n    height=4\n)\n\n\n\n\n\n\n\n\n\n\n\nRotating Tick Labels\nLong category names often overlap. Use .set_xticklabels() with rotation:\n\n\nCode\nrotated = sns.catplot(\n    data=penguins.to_pandas(),\n    x=\"island\",\n    y=\"body_mass_g\",\n    hue=\"species\",\n    kind=\"bar\"\n)\n\n# Rotate x-axis labels\nrotated.set_xticklabels(rotation=45, ha=\"right\")  # ha = horizontal alignment\n\n# Show it\nrotated\n\n\n\n\n\n\n\n\n\n\n\nCleaner Figures with despine()\nRemove the top and right borders (‚Äúspines‚Äù) for a cleaner look:\n\n\nCode\nnospine = sns.relplot(\n    data=penguins,\n    x=\"flipper_length_mm\",\n    y=\"body_mass_g\",\n    height=4\n)\n\n# Remove top and right spines\nsns.despine()\n\n# Show it\nnospine\n\n\n\n\n\n\n\n\n\n\n\nSaving Publication-Quality Figures\nUse .savefig() with these key parameters: - dpi=300 (or higher) for print quality - bbox_inches='tight' to prevent labels being cut off - Vector formats (PDF, SVG) for publications\n\n\nCode\nsaveme = sns.relplot(\n    data=penguins,\n    x=\"flipper_length_mm\",\n    y=\"body_mass_g\",\n    hue=\"species\",\n    height=4\n)\n# Update axis labels\nsaveme.set_axis_labels(\"Flipper Length (mm)\", \"Body Mass (g)\")\n# Despine\nsns.despine()\n\n# Save with high quality settings\nsaveme.savefig(\"my_figure.png\", dpi=300, bbox_inches=\"tight\")\nsaveme.savefig(\"my_figure.pdf\", bbox_inches=\"tight\")  # Vector format\n\n# You should see them in the file explorer to the left!\n\n\n\n\n\n\n\n\nTipPro Tip: Publication Template\n\n\n\nFor consistent publication-quality figures, set your theme once at the start of your script:\nsns.set_theme(\n    style=\"ticks\",\n    context=\"paper\",\n    font_scale=1.0,\n    rc={\"savefig.dpi\": 300}\n)\nThen use sns.despine() after each plot and save with bbox_inches=\"tight\".",
    "crumbs": [
      "Schedule",
      "Week 2",
      "Seaborn (plotting) Crash Course"
    ]
  },
  {
    "objectID": "weeks/02/lab/seaborn-crash-course.html#putting-it-together",
    "href": "weeks/02/lab/seaborn-crash-course.html#putting-it-together",
    "title": "A Crash Course on Python Dataviz",
    "section": "Putting It Together",
    "text": "Putting It Together\nLet‚Äôs recreate a classic visualization of Anscombe‚Äôs Quartet - four datasets with identical summary statistics but very different distributions:\n\nFirst we‚Äôll load the data and take a quick look\n\n\nCode\n# Load Anscombe's quartet\nanscombe = pl.DataFrame(sns.load_dataset(\"anscombe\"))\nanscombe.head()\n\n\n\nshape: (5, 3)\n\n\n\ndataset\nx\ny\n\n\nstr\nf64\nf64\n\n\n\n\n\"I\"\n10.0\n8.04\n\n\n\"I\"\n8.0\n6.95\n\n\n\"I\"\n13.0\n7.58\n\n\n\"I\"\n9.0\n8.81\n\n\n\"I\"\n11.0\n8.33\n\n\n\n\n\n\nNow we aggregate by dataset and confirm they have the same means and correlations:\n\n\nCode\nanscombe.group_by(\"dataset\").agg(\n    x_mean=col(\"x\").mean(),\n    y_mean=col(\"y\").mean().round(2),\n    correlation=pl.corr(\"x\", \"y\").round(2) # handy to get correlations btwn cols\n)\n\n\n\nshape: (4, 4)\n\n\n\ndataset\nx_mean\ny_mean\ncorrelation\n\n\nstr\nf64\nf64\nf64\n\n\n\n\n\"II\"\n9.0\n7.5\n0.82\n\n\n\"IV\"\n9.0\n7.5\n0.82\n\n\n\"III\"\n9.0\n7.5\n0.82\n\n\n\"I\"\n9.0\n7.5\n0.82\n\n\n\n\n\n\nNow let‚Äôs create a nice figure trying to replicate the styles above as closely as possible. We‚Äôll:\n\nSet the theme and create the base plot with lmplot()\nDefine custom tick ranges using list() and range() with slicing\nUse FacetGrid methods to customize limits, labels, titles, and spacing\n\n\n\nCode\n# Set the theme and font\nsns.set_theme(style=\"white\", font=\"Avenir\")\n\n# Make the plot\nansplot = sns.lmplot(\n    data=anscombe,\n    x=\"x\",\n    y=\"y\",\n    col=\"dataset\",\n    col_wrap=2,\n    height=2,\n    aspect=1.2,\n    truncate=False, # dont restrict regression line to data range\n    ci=None, # lmplot uses ci, but everything else uses errorbar\n    line_kws={\"color\": \"steelblue\"}, # regression line\n    scatter_kws={\"edgecolors\": \"sienna\", \"color\": \"darkorange\"} # points\n)\n\n# Define custom tick ranges (skip 0)\nxticks = list(range(0, 25, 5))[1:]\nyticks = list(range(0, 16, 4))[1:]\n\n# Set the limits and ticks\nansplot.set(\n    xlim=(0, 25),\n    ylim=(0, 16),\n    xticks=xticks,\n    yticks=yticks,\n)\n\n# Hide ticks and adjust label font-size\nansplot.tick_params(length=0, labelsize=8)\n\n# Remove individual sub-plot labels\nansplot.set_axis_labels(\"\",\"\")\n\n# Adjust subplot titles\nansplot.set_titles(\"{col_name}\", size=10)\n\n# Add an overall title\nansplot.figure.suptitle(\"Anscombe's Quartet\", fontsize=12)\n\n# Auto-adjust title & label spacing to not overlap\nansplot.figure.tight_layout()\n\n# Show it\nansplot\n\n\nText(0.5, 0.98, \"Anscombe's Quartet\")",
    "crumbs": [
      "Schedule",
      "Week 2",
      "Seaborn (plotting) Crash Course"
    ]
  },
  {
    "objectID": "weeks/02/lab/seaborn-crash-course.html#appendix",
    "href": "weeks/02/lab/seaborn-crash-course.html#appendix",
    "title": "A Crash Course on Python Dataviz",
    "section": "Appendix",
    "text": "Appendix\n\nQuick Reference\nFigure-level functions:\nsns.relplot(data, x, y, hue, col, row, kind=\"scatter\"|\"line\")\nsns.displot(data, x, y, hue, col, row, kind=\"hist\"|\"kde\"|\"ecdf\")\nsns.catplot(data, x, y, hue, col, row, kind=\"strip\"|\"swarm\"|\"box\"|\"violin\"|\"bar\"|\"point\")\nsns.lmplot(data, x, y, hue, col, row)  # scatter + regression\nQuick overview functions:\nsns.jointplot(data, x, y, hue)   # one relationship + marginals\nsns.pairplot(data, hue)          # all pairwise relationships\nCustomization:\nsns.set_theme(style=\"...\", context=\"...\", palette=\"...\")\ng.set_axis_labels(\"x label\", \"y label\")\ng.set_titles(\"{col_name}\")\ng.set(xlim=(a, b), ylim=(c, d))\ng.figure.suptitle(\"Overall title\")\n\n\nFor ggplot experts\nIf you‚Äôre coming from R‚Äôs ggplot2, this section maps familiar concepts to their seaborn equivalents. Use the tabs to toggle between the two syntaxes.\n\nAesthetic Mappings\n\nggplotseaborn\n\n\nggplot(df, aes(x = var1, y = var2, color = group))\n\n\n\nMapping\naes() parameter\n\n\n\n\nPosition\nx, y\n\n\nColor\ncolor / colour\n\n\nFill\nfill\n\n\nShape\nshape\n\n\nSize\nsize\n\n\nTransparency\nalpha\n\n\n\n\n\nsns.scatterplot(data=df, x=\"var1\", y=\"var2\", hue=\"group\")\n\n\n\nMapping\nParameter\n\n\n\n\nPosition\nx=, y=\n\n\nColor\nhue=\n\n\nFill\nhue= (context-dependent)\n\n\nShape\nstyle=\n\n\nSize\nsize=\n\n\nTransparency\nalpha=\n\n\n\n\n\n\n\n\nPlot Types\n\nggplotseaborn\n\n\n\n\n\nTask\nFunction\n\n\n\n\nScatter\ngeom_point()\n\n\nLine\ngeom_line()\n\n\nBar (counts)\ngeom_bar()\n\n\nBar (values)\ngeom_col()\n\n\nHistogram\ngeom_histogram()\n\n\nDensity\ngeom_density()\n\n\nBoxplot\ngeom_boxplot()\n\n\nViolin\ngeom_violin()\n\n\nRegression\ngeom_smooth()\n\n\n\n\n\n\n\n\nTask\nFunction\n\n\n\n\nScatter\nsns.scatterplot() or relplot(kind=\"scatter\")\n\n\nLine\nsns.lineplot() or relplot(kind=\"line\")\n\n\nBar (counts)\nsns.countplot()\n\n\nBar (values)\nsns.barplot()\n\n\nHistogram\nsns.histplot() or displot(kind=\"hist\")\n\n\nDensity\nsns.kdeplot() or displot(kind=\"kde\")\n\n\nBoxplot\nsns.boxplot() or catplot(kind=\"box\")\n\n\nViolin\nsns.violinplot() or catplot(kind=\"violin\")\n\n\nRegression\nsns.lmplot() or sns.regplot()\n\n\n\n\n\n\n\n\nFaceting\n\nggplotseaborn\n\n\nggplot(df, aes(x, y)) +\n  geom_point() +\n  facet_wrap(~group)\n\n\n\nFaceting\nSyntax\n\n\n\n\nWrap by one variable\nfacet_wrap(~var)\n\n\nWrap with column limit\nfacet_wrap(~var, ncol=2)\n\n\nGrid by two variables\nfacet_grid(row ~ col)\n\n\n\n\n\nsns.relplot(data=df, x=\"x\", y=\"y\", col=\"group\")\n\n\n\nFaceting\nSyntax\n\n\n\n\nWrap by one variable\ncol=\"var\"\n\n\nWrap with column limit\ncol=\"var\", col_wrap=2\n\n\nGrid by two variables\nrow=\"row\", col=\"col\"\n\n\n\n\n\n\n\n\nTheming\n\nggplotseaborn\n\n\nggplot(df, aes(x, y)) +\n  geom_point() +\n  theme_minimal()\n\n\n\nTheme\nDescription\n\n\n\n\ntheme_gray()\nGray background with grid\n\n\ntheme_bw()\nWhite background with grid\n\n\ntheme_minimal()\nMinimal, clean\n\n\ntheme_classic()\nClassic with axis lines\n\n\n\nOutput scaling: Set in ggsave(width=, height=, dpi=)\n\n\nsns.set_theme(style=\"whitegrid\")\nsns.relplot(data=df, x=\"x\", y=\"y\")\n\n\n\nStyle\nDescription\n\n\n\n\n\"darkgrid\"\nGray background with grid\n\n\n\"whitegrid\"\nWhite background with grid\n\n\n\"white\"\nMinimal, clean\n\n\n\"ticks\"\nClassic with axis ticks\n\n\n\nOutput scaling: Use context= parameter\n\n\"paper\" ‚Äî smallest\n\"notebook\" ‚Äî default\n\"talk\" ‚Äî larger for presentations\n\"poster\" ‚Äî largest\n\n\n\n\n\n\nLabels & Titles\n\nggplotseaborn\n\n\nggplot(df, aes(x, y)) +\n  geom_point() +\n  labs(\n    title = \"My Title\",\n    x = \"X Label\",\n    y = \"Y Label\"\n  )\n\n\nFigure-level functions (relplot, displot, catplot):\ng = sns.relplot(data=df, x=\"x\", y=\"y\")\ng.set_axis_labels(\"X Label\", \"Y Label\")\ng.figure.suptitle(\"My Title\", y=1.02)\nAxes-level functions (scatterplot, histplot, etc.):\nax = sns.scatterplot(data=df, x=\"x\", y=\"y\")\nax.set(xlabel=\"X Label\", ylabel=\"Y Label\", title=\"My Title\")",
    "crumbs": [
      "Schedule",
      "Week 2",
      "Seaborn (plotting) Crash Course"
    ]
  },
  {
    "objectID": "weeks/02/lab/seaborn-crash-course.html#additional-resources",
    "href": "weeks/02/lab/seaborn-crash-course.html#additional-resources",
    "title": "A Crash Course on Python Dataviz",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nSeaborn Tutorial - Official comprehensive guide\nSeaborn Example Gallery - Visual examples with code\nSeaborn API Reference - Complete function documentation\nColor Palette Guide - Choosing colors",
    "crumbs": [
      "Schedule",
      "Week 2",
      "Seaborn (plotting) Crash Course"
    ]
  },
  {
    "objectID": "weeks/04/index.html",
    "href": "weeks/04/index.html",
    "title": "Week 4",
    "section": "",
    "text": "Understand the mechanics of the sampling distribution\nUnderstand what statistical inference means in practice",
    "crumbs": [
      "Schedule",
      "Week 4",
      "Overview"
    ]
  },
  {
    "objectID": "weeks/04/index.html#overview",
    "href": "weeks/04/index.html#overview",
    "title": "Week 4",
    "section": "",
    "text": "Understand the mechanics of the sampling distribution\nUnderstand what statistical inference means in practice",
    "crumbs": [
      "Schedule",
      "Week 4",
      "Overview"
    ]
  },
  {
    "objectID": "weeks/04/index.html#slides",
    "href": "weeks/04/index.html#slides",
    "title": "Week 4",
    "section": "Slides",
    "text": "Slides\n\n\n\n\n\n\nTipSlides Wed Jan 26",
    "crumbs": [
      "Schedule",
      "Week 4",
      "Overview"
    ]
  },
  {
    "objectID": "weeks/04/index.html#materials",
    "href": "weeks/04/index.html#materials",
    "title": "Week 4",
    "section": "Materials",
    "text": "Materials\n\n\n\n\n\n\nCautionüìö Lab\n\n\n\nGithub Classroom Assignment",
    "crumbs": [
      "Schedule",
      "Week 4",
      "Overview"
    ]
  },
  {
    "objectID": "weeks/04/index.html#mentioned-references",
    "href": "weeks/04/index.html#mentioned-references",
    "title": "Week 4",
    "section": "Mentioned References",
    "text": "Mentioned References\n\nStatistics for Hackers\n\nDownload\nYoutube",
    "crumbs": [
      "Schedule",
      "Week 4",
      "Overview"
    ]
  }
]